#!/usr/bin/env python3
#
# This code is was produced by an alpha version of Brainome Daimensions(tm) and is 
# licensed under GNU GPL v2.0 or higher. For details, please see: 
# https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
#
#
# Output of Brainome Daimensions(tm) 0.91 Table Compiler v0.922.
# Invocation: btc https://www.openml.org/data/get_csv/1718/BayesianNetworkGenerator_kr-vs-kp_small.arff -o Predictors/BNG-kr-vs-kp_NN.py -target class -stopat 95.9 -f NN -e 20 --yes -server brain.brainome.ai -port 8090
# Total compiler execution time: 15:06:49.50. Finished on: Apr-15-2020 16:24:23.
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                52.18%
Model accuracy:                     95.36% (953626/999999 correct)
Improvement over best guess:        43.18% (of possible 47.82%)
Model capacity (MEC):               281 bits
Generalization ratio:               3393.68 bits/bit
Model efficiency:                   0.15%/parameter
System behavior
True Negatives:                     46.48% (464769/999999)
True Positives:                     48.89% (488857/999999)
False Negatives:                    3.30% (33017/999999)
False Positives:                    1.34% (13356/999999)
True Pos. Rate/Sensitivity/Recall:  0.94
True Neg. Rate/Specificity:         0.97
Precision:                          0.97
F-1 Measure:                        0.95
False Negative Rate/Miss Rate:      0.06
Critical Success Index:             0.91

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "BayesianNetworkGenerator_kr-vs-kp_small.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 36
n_classes = 2

mappings = [{1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {30677878.0: 0, 2517025534.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {476252946.0: 0, 1908338681.0: 1, 2013832146.0: 2}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2013832146.0: 1, 2238339752.0: 2}]
list_of_cols_to_normalize = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values()))+1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize,mappings):
            if i>=data_arr.shape[1]:
                break
            col = data_arr[:,i]
            normcol = column_norm(col,mapping)
            data_arr[:,i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([0.11196622393244786, 0.07043414086828173, 0.03811007622015244, 0.10191020382040764, 0.33519467038934075, 0.45678691357382717, 0.3690227380454761, 0.22338644677289354, 0.383012766025532, 0.30608661217322436, 0.4271288542577085, 0.11798623597247195, 0.688013376026752, 0.005598011196022392, 1.6316112632225264, 0.05279210558421117, 0.034504069008138016, 0.6840253680507361, 0.009768019536039072, 0.16332032664065327, 0.18984237968475937, 0.2026984053968108, 0.06546013092026184, 0.3798747597495195, 0.003962007924015848, 0.6950513901027802, 0.06923613847227694, 0.000614001228002456, 0.02525205050410101, 0.049832099664199325, 0.1883763767527535, 0.06839813679627359, 0.39587079174158346, 0.6445892891785784, 0.7172134344268688, 1.2471604943209886])
        components = np.array([array([ 9.95253851e-02,  2.52306692e-02,  6.06364703e-03,  8.70256647e-05,
       -7.50364400e-02, -2.28896580e-02,  1.07017596e-02, -1.39950373e-02,
       -1.80087185e-03, -1.76286997e-02,  4.16228343e-01, -1.42881775e-02,
        1.79384796e-01,  4.23685496e-03, -6.58245947e-01,  3.46636920e-03,
        1.55394138e-02, -9.60719336e-02,  1.13938068e-02,  2.05560494e-01,
        3.21793630e-02,  7.02871868e-03,  1.43339774e-02,  1.50853008e-02,
        2.63462869e-03,  3.08317364e-01, -1.48478877e-02, -2.35431709e-05,
        2.28578016e-02,  9.38381895e-03,  2.36819187e-01,  3.14350255e-02,
       -6.93901753e-02, -6.43078405e-02, -2.50985974e-01, -2.55750074e-01]), array([ 5.67329231e-02, -7.71273228e-02,  2.25785336e-02,  3.21820763e-02,
        3.29758285e-01,  4.32400057e-02,  5.09424291e-01,  4.63304951e-01,
        5.41025887e-01,  8.30486803e-02,  4.75069765e-02, -5.93048976e-02,
       -8.34675510e-02,  6.10055572e-04, -3.80694272e-02, -1.37535159e-02,
       -9.32989358e-03,  1.34560903e-01, -3.57267650e-03, -7.50947261e-03,
       -1.48437706e-01,  1.73654504e-01, -8.30533682e-03, -6.58017124e-02,
        1.07292395e-03,  5.31068746e-02, -1.89205888e-03,  1.02809817e-04,
        4.82670001e-03, -1.82991398e-02, -9.53099854e-02,  1.16695711e-02,
       -8.35413291e-03,  9.64234802e-03, -3.60601656e-02, -4.25312618e-02]), array([-2.76036721e-02,  8.68120596e-02,  3.55215825e-02,  1.11307549e-01,
        1.50375675e-01,  1.75718258e-01,  8.42422907e-02,  8.34632321e-02,
        7.06631654e-02, -1.03628314e-01, -1.28937401e-01, -2.96522035e-02,
        3.35539122e-01,  1.03851110e-03,  2.71475346e-03,  5.02624402e-02,
       -1.38441096e-02, -5.76037778e-01,  1.42814233e-02,  2.75810516e-02,
        1.25106436e-01, -1.77925035e-02, -3.37754097e-02,  1.12223249e-01,
       -1.17787013e-03, -1.34759956e-01,  4.15011155e-02,  5.40894678e-05,
       -5.39086898e-03,  4.72593680e-02,  2.07133152e-02, -2.76263544e-02,
        7.91703506e-02, -5.89880941e-01,  7.90490918e-02,  1.25086327e-01]), array([-5.40024929e-02, -5.11208538e-02, -1.58317960e-02,  1.36332715e-02,
        5.54723619e-02,  2.60317584e-01, -1.25768216e-02, -4.12666299e-02,
       -2.67401629e-02,  2.46661966e-02, -2.39312912e-01, -8.18033894e-03,
       -4.57206008e-02, -6.00818136e-03, -6.56726336e-01, -4.23647338e-02,
       -9.05057340e-03,  1.05501379e-01, -6.40940135e-03, -1.11580249e-01,
        4.86756189e-02,  1.39247835e-02, -2.64430735e-02, -3.05662341e-02,
       -4.12869016e-03, -2.85795490e-01, -5.58717126e-03, -1.66479168e-04,
        2.45496494e-02, -1.65665455e-02, -1.38920566e-01, -5.85059370e-02,
       -2.50010742e-01,  1.23546256e-01,  3.94053586e-01,  2.47355019e-01]), array([-6.51346514e-02,  6.67563646e-02, -5.45862774e-02, -3.72091271e-02,
       -1.31911081e-01, -1.92439797e-01, -3.09926434e-01, -1.00982251e-01,
        4.47938968e-01, -3.42566477e-01, -8.60863216e-02,  6.99602950e-02,
        1.19901350e-01, -2.03801971e-03, -1.36003489e-02,  1.20383133e-02,
        8.70599741e-03, -3.76525999e-02, -6.26850465e-04,  5.62453469e-02,
        1.35911297e-01,  6.01866313e-01,  1.22900939e-02, -2.12361244e-01,
       -1.80633029e-03, -1.01209287e-01,  2.64370035e-02, -2.33821576e-04,
        4.90239563e-03,  1.81078020e-02,  1.34419326e-01,  5.59383592e-03,
        1.66303227e-02,  1.10533900e-01, -1.61475413e-02,  9.19868191e-02]), array([-7.87962939e-02,  3.92614492e-02, -1.05686904e-02, -1.43332794e-02,
       -1.01135035e-01, -1.11466485e-01, -6.54371888e-02,  1.39744115e-01,
        3.68410241e-02,  5.76888936e-01, -4.32045404e-02,  2.61579216e-02,
        9.22799653e-02, -1.37514697e-03, -1.29191345e-01,  2.45418133e-02,
        8.98659051e-03, -6.20252777e-03, -5.26285595e-03,  4.05864044e-02,
        2.78324623e-01, -8.06958126e-02,  2.93696170e-02, -4.10015470e-01,
        2.29284035e-03, -6.28729447e-02,  6.60728919e-02,  1.58968268e-04,
        9.05111384e-03,  4.12093791e-03,  3.36048114e-02, -1.69711434e-02,
        5.54707569e-01,  3.19003199e-02,  3.37444412e-02,  9.22338383e-02]), array([-3.94148116e-02, -3.47198489e-02, -5.47819159e-03,  1.31048002e-01,
       -4.14178571e-02,  8.08099605e-01, -7.03198525e-02, -1.12440177e-01,
        3.81162312e-02, -1.13001927e-01,  1.81583923e-01, -1.12823545e-01,
       -1.23739598e-01, -6.71358817e-03,  9.92841113e-02,  2.49255724e-02,
       -9.52671975e-03,  1.32123207e-01, -5.81988772e-04,  1.92912633e-02,
        8.99401803e-02,  1.59047171e-01, -5.31267288e-02,  1.71607720e-02,
        1.94043836e-03,  1.16553972e-01,  2.77435131e-02, -1.50642628e-04,
       -3.78905996e-02, -3.33569855e-02,  7.30838860e-02, -1.20453253e-01,
        3.53472778e-01,  5.83024176e-02, -3.90276408e-03, -4.91291145e-02]), array([ 5.67523965e-02, -2.57487579e-02,  5.04615610e-02,  2.74845937e-02,
        1.38718152e-01, -2.57407084e-01,  1.88369422e-02, -5.31354590e-02,
        2.00153713e-02, -1.20222958e-01, -7.50691565e-02, -5.85304638e-03,
       -1.44903725e-01,  2.91995667e-03, -2.13033004e-01,  1.51430758e-02,
       -2.07048059e-02,  7.97477992e-02,  7.29034614e-03, -3.50673974e-02,
        9.83934003e-02,  5.42062970e-02, -4.69229457e-02,  6.78433210e-01,
        1.48024989e-03, -7.88855080e-02,  1.24076519e-01,  2.64320194e-04,
        1.95322016e-02,  5.43288097e-02, -4.36916844e-02,  2.84758895e-02,
        5.16213319e-01,  9.79150093e-02, -1.06736714e-01,  1.36746543e-01]), array([-1.88426097e-02,  5.74008002e-02,  7.72084929e-02, -4.16273863e-02,
       -6.06609599e-01,  1.08982553e-01, -4.70337169e-02,  2.01297268e-01,
        2.59125654e-01,  3.75724062e-01,  2.40185629e-02,  1.33582320e-01,
        7.19633328e-02,  1.54181412e-03,  8.56671585e-02,  3.30886729e-03,
        3.19361044e-02, -5.74564027e-03,  7.45158251e-03, -7.06058901e-04,
        3.78595935e-02,  9.39409663e-02,  6.03092924e-02,  4.85619566e-01,
       -3.87110100e-05,  1.93529994e-03, -6.46907410e-02,  2.74510982e-05,
       -7.12844589e-03, -3.07979093e-03, -2.84439171e-03, -2.89999483e-02,
       -1.95404870e-01, -2.35797083e-02,  1.64645006e-01, -7.09111292e-02]), array([ 1.78174276e-01,  2.37361370e-02,  3.06529366e-02,  3.96289630e-02,
       -3.67691429e-01, -1.94925877e-02, -6.44603854e-03,  4.50855506e-02,
        3.03387780e-02, -1.79386593e-01, -4.86249413e-02,  1.68822028e-01,
       -5.49079349e-01,  1.45790928e-02, -1.68365366e-01,  5.48396413e-02,
        4.55645791e-02, -7.00500581e-02, -6.76930036e-03, -2.32576726e-01,
       -1.01115174e-01, -3.99169880e-04,  1.07092597e-01, -2.12241865e-01,
        5.45381273e-04,  5.22556548e-02,  4.67116487e-02,  5.77118089e-05,
        5.64612580e-02,  7.60695753e-04, -2.99041767e-01,  9.68786802e-02,
        1.25249992e-01, -4.03732265e-01, -1.26171171e-01, -3.36000628e-02]), array([ 1.39783282e-02, -1.28718154e-02,  2.15988791e-03, -9.50672505e-02,
       -3.24410740e-01,  1.49075252e-01,  2.65780170e-01,  5.24625009e-02,
       -7.00625427e-02, -2.37074492e-01, -1.79561666e-01,  1.93187621e-01,
        4.44555191e-01,  4.03737226e-03, -4.52987332e-02,  3.66334314e-02,
        4.54620836e-02,  1.23563134e-01, -1.06463621e-02,  1.42701373e-01,
       -4.25239926e-01, -1.29851584e-01,  1.25556231e-01, -7.02165160e-02,
        4.75401188e-04, -1.24161314e-01,  3.93696517e-02,  2.14412414e-04,
        7.27424297e-02, -2.68321150e-02, -2.35965710e-02,  1.37493957e-01,
        2.11759020e-01,  1.49907053e-01, -2.13090790e-01,  2.16021231e-01]), array([ 9.34080076e-02,  5.06548821e-02,  2.42535011e-02,  1.56664187e-01,
        1.80956697e-01,  2.26862702e-01, -1.44965917e-01, -1.55144666e-02,
        1.67315238e-02,  2.39589027e-01, -3.77052892e-01,  1.53040115e-01,
       -1.01523289e-01,  1.37742612e-02,  4.88364946e-03,  2.74417248e-02,
        8.87953836e-03,  4.96454802e-02, -3.99241657e-03,  1.97513022e-01,
        2.22578546e-01,  4.07793404e-02,  7.15129197e-02,  5.81144844e-02,
       -3.24955073e-03, -3.20315378e-02,  9.24156807e-03, -1.43223601e-04,
        1.06376643e-01, -1.41167328e-02,  2.99584528e-02,  3.87685640e-01,
       -2.45025647e-01,  1.33207188e-02, -5.42871852e-01,  5.94683068e-02]), array([-2.02169875e-02, -3.20466275e-02,  2.85908078e-03, -1.42057698e-01,
        4.33723463e-02,  5.89407886e-02,  1.14122699e-01, -1.35226694e-01,
       -2.92661186e-02, -2.51265532e-02, -1.43980883e-01,  7.06777733e-02,
        2.34788823e-01, -2.90144895e-03, -3.77788413e-02, -2.32383967e-02,
        4.16558085e-03, -7.83884285e-02, -9.78777127e-03, -2.63105679e-01,
        1.68416939e-01,  9.70315411e-02,  1.59337552e-02,  4.30581134e-03,
       -3.47072877e-03, -2.22031788e-01, -1.58505346e-01, -2.56175842e-04,
        3.33025938e-02, -4.37089253e-02, -3.31618407e-01,  1.06419734e-01,
        1.11445281e-01,  1.28067473e-01, -4.61964196e-02, -7.17046551e-01]), array([-8.74415233e-02,  4.96038008e-02, -3.38728043e-03,  5.50089444e-02,
       -1.25506298e-03, -2.58443081e-02, -5.86499657e-02,  9.60005642e-02,
        3.01272698e-03, -3.97001713e-02, -4.18425211e-02, -2.66652559e-02,
       -2.67601047e-01,  5.26165662e-04, -2.11925045e-02,  4.82376222e-02,
        1.26580001e-04,  1.19342109e-01,  3.76432730e-03,  4.09660496e-01,
       -2.19060183e-01, -8.33487069e-02,  9.45004362e-03,  1.13716637e-03,
       -2.01238132e-04, -5.59900530e-01, -1.53309238e-02, -4.57108602e-05,
        1.26811582e-02, -1.74196152e-02,  3.94044665e-01,  1.38476178e-02,
        7.47281020e-02, -1.68501617e-01,  1.17054436e-01, -3.75051760e-01]), array([ 1.86981505e-01,  7.34668980e-02, -1.10027718e-03,  3.16366983e-01,
        1.37886890e-01, -2.01695244e-02, -4.79344605e-01,  3.74962041e-02,
        7.42127947e-02,  1.74229836e-01, -1.37655766e-01, -1.76266352e-01,
        2.42287971e-01,  1.61416093e-02, -5.73188164e-02,  9.55778693e-02,
       -5.10670898e-02,  1.46176884e-01, -1.28606198e-02, -1.80723653e-01,
       -5.26566413e-01,  5.41549956e-02, -7.93477738e-02,  2.65475290e-02,
        2.00216370e-03,  1.64506348e-01,  5.71150745e-02,  3.58732870e-04,
        2.37598654e-02, -1.14711482e-02, -9.75062510e-02,  1.06946673e-01,
        9.45889985e-02, -1.10242303e-01,  1.12245266e-01, -1.48358855e-01]), array([-1.32909130e-01,  5.30476376e-02,  7.95355148e-03, -8.02453927e-02,
        1.53743479e-01, -8.14830385e-04,  1.15214164e-01, -1.57489770e-01,
       -4.76109430e-02,  7.25361315e-03, -2.69528018e-01,  3.86590300e-01,
       -1.00485198e-01, -4.60341650e-04, -7.80794447e-03,  4.00288346e-02,
        6.96136885e-02, -4.39149093e-02, -1.89395401e-03,  3.09914698e-01,
       -7.12924391e-02,  1.14284115e-01,  1.58885505e-01,  2.78029631e-02,
       -3.87823933e-03,  4.97748475e-01, -1.04048830e-01,  4.56587087e-06,
        7.39080947e-02, -2.88573851e-02,  4.65341203e-02,  1.67588690e-01,
        1.28832327e-01, -3.09441685e-02,  4.51297320e-01, -1.08506428e-01]), array([ 4.47091970e-01, -1.01861090e-01, -7.17559480e-03,  3.61544479e-01,
        8.87554225e-02, -3.13548629e-03, -3.81153775e-02,  1.29851821e-01,
        2.23797000e-03, -1.41243998e-01,  1.63316230e-01,  4.78194331e-01,
        1.37962381e-01,  1.25204774e-02,  8.66934952e-02, -5.40460492e-02,
        1.14073621e-01,  1.11683102e-01,  4.94383239e-03, -1.14928336e-01,
        2.38037773e-01, -1.47433413e-01,  2.13221390e-01, -3.01381471e-02,
        4.08466863e-03, -9.94945941e-02,  2.62862932e-01,  2.99364709e-04,
        2.75209617e-02,  8.04823433e-02,  1.30682680e-01, -5.47170483e-02,
       -3.04424760e-02,  8.88380976e-02,  2.08991138e-01, -7.44081564e-02]), array([-2.16843768e-01, -2.45096567e-03, -1.00329278e-01,  2.38036231e-01,
       -9.57684395e-02,  2.86453994e-02, -1.68210290e-01,  3.57382409e-01,
        1.13824415e-01, -2.12975724e-01, -1.51184504e-01, -7.74599540e-02,
       -1.55052672e-01, -1.04854477e-02, -4.07152468e-02, -6.60688744e-02,
       -2.48879308e-02, -4.71615737e-01,  1.97755040e-02,  2.73834125e-02,
       -1.59073336e-02, -3.23118949e-01, -5.07529121e-02, -1.01880146e-02,
       -1.02664545e-03,  1.36620038e-01,  1.67739123e-02,  3.08686327e-04,
        1.04305048e-02,  7.59449766e-02, -1.30781659e-02,  6.54475435e-02,
        3.25840282e-02,  4.77897648e-01,  9.90896208e-03, -1.12009847e-01]), array([ 0.19153323, -0.00921634, -0.00997571, -0.07457026, -0.19288593,
       -0.05205788,  0.02150002,  0.2163787 ,  0.01198597, -0.28859086,
       -0.29153659, -0.35890563,  0.12909601,  0.01139756,  0.01630548,
        0.03369537, -0.10761617,  0.38349396, -0.02038239,  0.12089059,
        0.40421078, -0.20082328, -0.19067981, -0.0319221 , -0.00603228,
        0.21292065, -0.02325237, -0.00046895,  0.00143338, -0.05577859,
        0.03660085,  0.16420738, -0.00136722, -0.17394286,  0.15912563,
       -0.0852016 ]), array([ 5.63168807e-01,  2.67676923e-01,  1.31206540e-01, -9.37311574e-02,
       -8.88069117e-02,  3.38418324e-02,  2.16945081e-01, -1.88012473e-01,
       -5.07545814e-02,  1.11675064e-01, -1.73583817e-02, -2.80463944e-01,
       -1.03422113e-01,  4.12853112e-02,  5.15924035e-02,  1.65960669e-01,
       -5.75333085e-02, -3.05139775e-01,  1.63389116e-02,  1.70754688e-01,
       -5.65763690e-02,  1.49291871e-01, -6.59400941e-02, -5.96087299e-02,
        4.20810598e-03, -6.87033035e-02,  2.37019012e-01,  4.74789741e-04,
        1.87439573e-02,  1.19468412e-01, -1.64803808e-02,  1.30452247e-01,
       -1.64159341e-02,  2.79567970e-01,  1.63016791e-01, -7.93762556e-03]), array([ 2.72210505e-01,  6.66458314e-02,  1.02178043e-01, -6.16739611e-01,
        1.78783843e-01,  1.49531485e-01, -2.48130899e-01,  2.03391434e-01,
        8.43723338e-02, -3.44197033e-02, -6.18050909e-02,  1.51416369e-01,
       -5.94724284e-02,  1.23907359e-02, -1.87514109e-02,  4.52014394e-02,
       -4.06518511e-03, -9.85822348e-02,  5.46034483e-03, -3.09482553e-01,
       -6.92436273e-02, -1.51253968e-01, -1.20110830e-02,  2.10705831e-02,
       -6.13765662e-03,  1.19638753e-02, -2.23070489e-01, -4.92354334e-04,
        1.25356391e-02, -8.06524611e-02,  3.62905228e-01, -2.82586996e-03,
        7.05720206e-02,  4.49224794e-02, -1.89314086e-02,  5.27488579e-02]), array([-1.71048218e-01,  6.91724119e-01, -7.23504255e-02,  3.86610020e-02,
        9.45770004e-02,  1.62909480e-03,  1.53578899e-03,  1.15451315e-01,
       -1.08925214e-02, -9.86437792e-02,  1.18293573e-01,  1.18997714e-01,
        2.24820122e-02,  3.84041562e-02, -2.00865379e-02,  5.66562109e-01,
        3.11741456e-02,  1.58565462e-01, -8.60087704e-03, -3.02338022e-02,
        1.00555157e-01, -9.81743809e-02,  3.94310156e-02,  2.21316781e-02,
        1.09386409e-03, -1.93241738e-02, -2.93034481e-02, -1.41498782e-04,
       -3.38406062e-02, -3.40396112e-02, -1.57503633e-01, -1.31249334e-01,
       -5.37083359e-02,  5.90147716e-02, -3.12000563e-02,  7.99135770e-03]), array([-1.63176815e-01,  7.35719929e-02, -8.71941335e-03,  1.92734103e-01,
       -7.74561943e-02, -3.95069670e-02,  2.49359945e-01, -9.53693954e-02,
       -5.23774582e-02,  1.90860961e-02, -1.51679302e-01, -3.15203501e-01,
       -1.96982725e-02,  1.13373477e-02, -1.50525288e-02,  8.97368490e-02,
        1.99899596e-01,  1.55258916e-02,  1.85347641e-02, -4.65880344e-01,
        2.14011754e-02,  6.72698443e-02,  4.25202850e-01,  1.12640560e-02,
       -2.63037791e-03,  5.95822616e-02,  6.64131560e-03,  1.70986781e-04,
        1.43601025e-01,  1.76883477e-02,  4.82339863e-01,  1.26745505e-01,
       -1.35560016e-02,  1.31705348e-02,  1.12536641e-02, -2.01716644e-02]), array([ 1.54674056e-02, -6.67715141e-02,  2.74829364e-02, -2.51053321e-01,
        1.20930857e-01,  4.44856301e-02, -2.58488218e-01,  9.94438700e-02,
        5.95863918e-02, -5.38387485e-02,  1.34688842e-01, -3.14395388e-01,
        3.13925452e-02,  7.60484547e-03,  2.36723006e-02, -4.79403810e-02,
        3.50149059e-01,  1.01276351e-02, -3.97789380e-03,  2.51355410e-01,
        3.91708169e-02, -7.28119181e-02,  6.15838341e-01,  4.87565081e-02,
        4.43575797e-03, -3.82929421e-02,  7.16752354e-02,  5.65615301e-04,
        1.28553710e-01,  5.16069068e-02, -3.27381264e-01,  8.73326858e-03,
        4.75421367e-03, -2.33595415e-02,  4.85839982e-02,  2.24798247e-02]), array([-0.2738455 , -0.01543064, -0.00711868, -0.30707495,  0.02691845,
        0.08256464, -0.03007844,  0.03583212,  0.00337926,  0.02035078,
       -0.05282627,  0.07313954,  0.01261337, -0.01587307, -0.00863038,
        0.02195906, -0.05977297,  0.10499819,  0.05207376, -0.10803392,
       -0.0368776 , -0.01531721, -0.10697552, -0.00778927,  0.01996952,
        0.07238732,  0.65347722,  0.00284487, -0.00469494,  0.54343465,
        0.04778483,  0.06407459, -0.0897328 , -0.08232349, -0.02129959,
       -0.13923941]), array([ 1.29251865e-01, -5.30890158e-04, -8.00952530e-02,  9.42398240e-02,
       -1.17489853e-02,  9.99513976e-03,  1.72575706e-02, -8.13899617e-03,
       -1.07974524e-02,  3.13194081e-03, -4.05672425e-02, -9.61140874e-03,
        5.74432670e-03,  6.57280286e-03, -3.95192060e-03,  1.75898499e-02,
        2.09168829e-02,  7.29708153e-02,  9.15658522e-02,  4.10241268e-02,
       -3.57318399e-03,  1.13882330e-02,  2.30092386e-02, -2.40778731e-02,
       -1.77757208e-02, -1.36706424e-02, -5.47772575e-01,  6.09214447e-03,
       -1.75872105e-02,  7.96147885e-01, -1.38777510e-02, -8.93259874e-02,
        4.90775926e-02, -9.32575568e-03, -2.53558478e-02,  4.45269604e-02]), array([-6.90394956e-02, -4.98760100e-01,  5.46842645e-01,  5.45434014e-02,
       -1.32464770e-02, -4.08779999e-02, -4.19446322e-02, -2.40176137e-02,
        2.59192148e-02, -1.79964438e-02, -5.80896563e-02,  5.17328213e-03,
        1.03563540e-02,  3.67469873e-02, -1.09669252e-02,  6.32329655e-01,
       -2.46006872e-02, -4.69924497e-02,  3.28862720e-04,  3.97948270e-02,
        2.86730424e-02, -2.64381496e-02,  1.49611946e-02, -4.96258911e-02,
       -8.58730153e-06,  2.12758793e-02, -2.44265890e-02,  8.30945512e-05,
       -4.25454861e-02,  1.74502718e-02,  2.67662258e-03, -1.26136492e-01,
       -4.72129521e-02,  5.83319143e-02, -2.61346556e-02, -2.14644552e-02]), array([-1.91995669e-02,  3.36249524e-01,  6.45872875e-01,  8.21155618e-02,
        8.54002071e-03, -1.59427886e-02,  7.62814882e-03,  7.70429703e-03,
       -1.72164234e-02, -4.76507226e-02, -2.33381418e-01, -5.92547276e-03,
        8.25616955e-03, -1.63815995e-02, -3.75255470e-02, -4.22786979e-01,
        8.25198132e-02,  7.13885666e-02, -1.02669962e-02,  3.37350011e-02,
       -1.46503683e-02, -1.16609270e-02,  5.38900075e-02, -5.71388968e-02,
       -1.25083032e-04,  8.43200379e-02,  3.89662831e-02,  2.70262552e-04,
       -1.60773793e-01,  2.61817039e-02,  8.67555058e-03, -3.86673209e-01,
       -1.39720964e-03,  1.60155730e-02, -1.15921316e-01, -8.09145948e-02]), array([-0.19041083,  0.13448422,  0.46494388,  0.05861309,  0.02731638,
        0.00928032, -0.04297485,  0.02580635,  0.010745  , -0.07311843,
        0.39034995,  0.02451059,  0.02366454, -0.00678017,  0.04639477,
       -0.14570519, -0.15447134,  0.04750942,  0.00673866, -0.08212548,
        0.03903111, -0.03323388, -0.08653371, -0.04054819, -0.00245478,
       -0.11410658, -0.13012493,  0.0007357 ,  0.25607559,  0.07746087,
       -0.04813575,  0.58885699,  0.03418903,  0.00837767,  0.17160551,
        0.11966669]), array([-1.22990873e-02, -1.38950733e-02,  3.49046942e-02,  1.06491829e-02,
       -4.76239011e-03, -3.79278454e-03,  1.15215670e-02, -2.18794113e-03,
       -1.15442156e-04,  3.35146936e-03,  2.79580243e-02,  6.23752526e-03,
        9.17167210e-04, -1.88353524e-02, -1.17743589e-03,  3.09348165e-02,
        8.55694226e-01, -4.11056711e-04, -2.10407173e-04, -2.09107844e-02,
       -6.87298134e-03, -8.94276437e-04, -4.25951594e-01, -1.07858298e-02,
       -1.80430819e-03, -2.10440967e-02, -1.05251290e-02,  2.01293106e-04,
       -2.07574511e-01,  4.44221213e-03,  9.76389343e-03,  1.92371607e-01,
        5.22771528e-03,  3.63209111e-03,  2.39982587e-02,  1.92845330e-02]), array([ 1.76452242e-02,  3.56052590e-03,  1.89447436e-03,  1.14075464e-03,
       -9.70341497e-03, -1.26447974e-02,  5.91496527e-03, -4.53850859e-04,
       -6.10299554e-03,  1.27521326e-02, -4.35770921e-02,  7.04272518e-03,
       -1.03510484e-03, -1.31451452e-02,  3.80123029e-02,  2.42201457e-03,
        1.61968728e-01, -1.18914122e-03, -6.50435739e-03,  2.99205167e-02,
        3.86032718e-03,  6.30131894e-03, -2.62294669e-01, -4.29770486e-03,
        1.65359781e-03,  2.98431846e-02,  1.45826206e-02, -2.01748706e-04,
        8.90731820e-01, -6.71938162e-03, -3.14444197e-03, -3.20233683e-01,
       -2.00490508e-02,  1.24215059e-03, -3.71844424e-02, -2.57737710e-02]), array([-4.30852356e-03, -3.99284022e-02,  2.23274727e-02, -4.22184423e-03,
        3.26265249e-03, -3.67014990e-04, -3.39861510e-02,  5.73465315e-01,
       -6.20717649e-01, -2.64258575e-02,  3.96177238e-03,  2.21218363e-03,
        1.10934631e-03,  1.10870088e-02, -1.38000480e-03,  1.15147079e-02,
        1.07977234e-03, -3.65773278e-02,  9.08859859e-03,  3.05876002e-03,
        7.58519135e-03,  5.28297189e-01,  4.72668787e-03,  1.23539457e-02,
       -9.19812994e-04,  5.60526551e-04, -5.18884382e-03,  1.00286846e-04,
       -3.28252955e-03, -6.52980813e-03, -3.88229154e-03,  5.46148248e-03,
        3.43519218e-03,  2.59437821e-02,  1.32487094e-03,  2.62278414e-04]), array([ 2.96481573e-03,  2.53180443e-03,  8.39273518e-03,  1.38601866e-03,
        6.06227936e-04, -4.41757832e-03, -5.54927618e-03, -5.49233160e-03,
        4.94582883e-03, -2.70482287e-03, -9.02393564e-03,  6.55405195e-04,
        6.90955616e-03,  3.35102282e-04,  1.31296644e-03, -3.42584133e-03,
        1.21625642e-03,  2.35808456e-02,  9.93011273e-01,  5.64641431e-03,
        5.49270245e-04, -5.11295321e-03, -3.26851233e-03, -1.17479771e-02,
       -1.92450375e-03, -3.79140845e-05,  1.21614497e-02, -4.07289427e-03,
        3.40609419e-03, -1.09362125e-01, -2.55550106e-02, -2.50343769e-03,
        2.22692338e-03, -4.06148786e-03, -3.02511286e-05, -2.14218203e-04])])
        whiten = False
        explained_variance = np.array([0.8318362404171966, 0.5204234548502742, 0.402559097604253, 0.31614221769037176, 0.2869459741722502, 0.2740337114289539, 0.25464441863702597, 0.23656390286085824, 0.20761666147680555, 0.18643955470155105, 0.16535087429312234, 0.14237135606728496, 0.13745003131470945, 0.1271312683748687, 0.11486562265186306, 0.11268666478324604, 0.10147453600263687, 0.09252707190443106, 0.08935531890110922, 0.0843723728735332, 0.07347785652713926, 0.06894230218296149, 0.06463940417236236, 0.0589398077847181, 0.05568676793794839, 0.03835436555742652, 0.03279528585778297, 0.03096964152111016, 0.028614469711428753, 0.023022533822815165, 0.016738311428215066, 0.01290468395981081, 0.008883122336426933])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    il=[]
    
    ignorelabels=[]
    ignorecolumns=[]
    target="class"


    if (testfile):
        target=''
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless==False):
                header=next(reader, None)
                try:
                    if (target!=''): 
                        hc=header.index(target)
                    else:
                        hc=len(header)-1
                        target=header[hc]
                except:
                    raise NameError("Target '"+target+"' not found! Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=header.index(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute '"+ignorecolumns[i]+"' is the target. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '"+ignorecolumns[i]+"' not found in header. Header must be same as in file passed to btc.")
                for i in range(0,len(header)):      
                    if (i==hc):
                        continue
                    if (i in il):
                        continue
                    print(header[i]+",", end = '', file=outputfile)
                print(header[hc],file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if (row[target] in ignorelabels):
                        continue
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name==target):
                            continue
                        if (',' in row[name]):
                            print ('"'+row[name]+'"'+",",end = '', file=outputfile)
                        else:
                            print (row[name]+",",end = '', file=outputfile)
                    print (row[target], file=outputfile)

            else:
                try:
                    if (target!=""): 
                        hc=int(target)
                    else:
                        hc=-1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=int(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute "+str(col)+" is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    if (hc==-1):
                        hc=len(row)-1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0,len(row)):
                        if (i in il):
                            continue
                        if (i==hc):
                            continue
                        if (',' in row[i]):
                            print ('"'+row[i]+'"'+",",end = '', file=outputfile)
                        else:
                            print(row[i]+",",end = '', file=outputfile)
                    print (row[hc], file=outputfile)

def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'nowin': 0, 'won': 1}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    # function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping

# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)
# Classifier
def classify(row):
    #inits
    x=row
    o=[0]*num_output_logits


    #Nueron Equations
    h_0 = max((((4.007484 * float(x[0]))+ (-27.23602 * float(x[1]))+ (22.629873 * float(x[2]))+ (22.396378 * float(x[3]))+ (34.418816 * float(x[4]))+ (-18.519026 * float(x[5]))+ (20.102104 * float(x[6]))+ (-7.6879506 * float(x[7]))+ (-4.028544 * float(x[8]))+ (-16.09283 * float(x[9]))+ (-64.784584 * float(x[10]))+ (7.044579 * float(x[11]))+ (14.466705 * float(x[12]))+ (-34.209545 * float(x[13]))+ (-91.66121 * float(x[14]))+ (-22.143795 * float(x[15]))+ (50.321793 * float(x[16]))+ (6.619984 * float(x[17]))+ (68.8905 * float(x[18]))+ (-11.121458 * float(x[19]))+ (-10.862219 * float(x[20]))+ (24.472383 * float(x[21]))+ (-8.692828 * float(x[22]))+ (5.0032067 * float(x[23]))+ (3.0002337 * float(x[24]))+ (-2.776432 * float(x[25]))+ (-4.6590643 * float(x[26]))+ (9.616032 * float(x[27]))+ (-13.289782 * float(x[28]))+ (-1.5542876 * float(x[29]))+ (2.0850492 * float(x[30]))+ (0.8652772 * float(x[31]))+ (9.494038 * float(x[32]))) + 2.8690097), 0)
    h_1 = max((((-4.9904075 * float(x[0]))+ (17.9484 * float(x[1]))+ (-16.18226 * float(x[2]))+ (-16.446058 * float(x[3]))+ (-22.432932 * float(x[4]))+ (10.878271 * float(x[5]))+ (-11.117444 * float(x[6]))+ (6.4577804 * float(x[7]))+ (1.2727983 * float(x[8]))+ (11.5746975 * float(x[9]))+ (44.081806 * float(x[10]))+ (-7.465906 * float(x[11]))+ (-9.841961 * float(x[12]))+ (23.334446 * float(x[13]))+ (63.679283 * float(x[14]))+ (14.603613 * float(x[15]))+ (-31.719551 * float(x[16]))+ (-6.43648 * float(x[17]))+ (-45.520927 * float(x[18]))+ (9.396109 * float(x[19]))+ (9.095315 * float(x[20]))+ (-19.658224 * float(x[21]))+ (2.2839072 * float(x[22]))+ (-3.235001 * float(x[23]))+ (-6.74978 * float(x[24]))+ (2.890021 * float(x[25]))+ (7.5217667 * float(x[26]))+ (-10.15495 * float(x[27]))+ (2.6153555 * float(x[28]))+ (0.62311757 * float(x[29]))+ (3.4012501 * float(x[30]))+ (-3.7097437 * float(x[31]))+ (-18.566734 * float(x[32]))) + 3.7068532), 0)
    h_2 = max((((-0.20017205 * float(x[0]))+ (-1.7328577 * float(x[1]))+ (0.9696087 * float(x[2]))+ (3.59828 * float(x[3]))+ (-1.7166319 * float(x[4]))+ (0.39701495 * float(x[5]))+ (12.010789 * float(x[6]))+ (-5.6289873 * float(x[7]))+ (2.8260849 * float(x[8]))+ (-0.104618065 * float(x[9]))+ (0.47455582 * float(x[10]))+ (3.6758971 * float(x[11]))+ (-2.207417 * float(x[12]))+ (0.5921252 * float(x[13]))+ (6.922502 * float(x[14]))+ (-3.7241774 * float(x[15]))+ (3.6072426 * float(x[16]))+ (6.3147383 * float(x[17]))+ (0.33185333 * float(x[18]))+ (-3.164622 * float(x[19]))+ (5.4488835 * float(x[20]))+ (3.6761732 * float(x[21]))+ (-5.0790935 * float(x[22]))+ (3.800453 * float(x[23]))+ (1.6448838 * float(x[24]))+ (0.5120955 * float(x[25]))+ (-7.2336535 * float(x[26]))+ (-12.047048 * float(x[27]))+ (-8.721835 * float(x[28]))+ (-0.4623819 * float(x[29]))+ (-1.3887696 * float(x[30]))+ (4.9090776 * float(x[31]))+ (-3.121628 * float(x[32]))) + -10.065026), 0)
    h_3 = max((((5.025284 * float(x[0]))+ (-0.054415207 * float(x[1]))+ (0.4677891 * float(x[2]))+ (-3.359382 * float(x[3]))+ (-1.731863 * float(x[4]))+ (0.76378447 * float(x[5]))+ (-0.34267023 * float(x[6]))+ (3.1993878 * float(x[7]))+ (-2.4943123 * float(x[8]))+ (-0.47013324 * float(x[9]))+ (-3.9820554 * float(x[10]))+ (-1.2265524 * float(x[11]))+ (0.14565507 * float(x[12]))+ (-3.143042 * float(x[13]))+ (5.984959 * float(x[14]))+ (-6.1307697 * float(x[15]))+ (4.390726 * float(x[16]))+ (-2.7457187 * float(x[17]))+ (5.4952927 * float(x[18]))+ (7.1355433 * float(x[19]))+ (4.202882 * float(x[20]))+ (-0.5828808 * float(x[21]))+ (-11.889339 * float(x[22]))+ (-5.75422 * float(x[23]))+ (-5.4721565 * float(x[24]))+ (1.5937252 * float(x[25]))+ (-6.3268456 * float(x[26]))+ (-3.6102483 * float(x[27]))+ (7.344638 * float(x[28]))+ (7.61579 * float(x[29]))+ (16.712439 * float(x[30]))+ (1.3295904 * float(x[31]))+ (-4.418429 * float(x[32]))) + 1.4632828), 0)
    h_4 = max((((0.64444566 * float(x[0]))+ (-0.79552966 * float(x[1]))+ (0.48214424 * float(x[2]))+ (-0.37936568 * float(x[3]))+ (-0.12414922 * float(x[4]))+ (6.7762914 * float(x[5]))+ (-0.6745846 * float(x[6]))+ (1.7280045 * float(x[7]))+ (1.6558955 * float(x[8]))+ (-1.7276839 * float(x[9]))+ (-2.818765 * float(x[10]))+ (2.648439 * float(x[11]))+ (1.4795607 * float(x[12]))+ (-0.9663667 * float(x[13]))+ (-3.247905 * float(x[14]))+ (2.6978183 * float(x[15]))+ (-3.8288567 * float(x[16]))+ (0.23446348 * float(x[17]))+ (-1.8504448 * float(x[18]))+ (-5.2729177 * float(x[19]))+ (-3.7619271 * float(x[20]))+ (1.1195811 * float(x[21]))+ (4.1979856 * float(x[22]))+ (1.3042744 * float(x[23]))+ (1.2617645 * float(x[24]))+ (-1.055765 * float(x[25]))+ (5.036474 * float(x[26]))+ (1.2233199 * float(x[27]))+ (4.2208104 * float(x[28]))+ (0.19516368 * float(x[29]))+ (-4.558482 * float(x[30]))+ (-0.9645055 * float(x[31]))+ (-1.0076705 * float(x[32]))) + 1.4646009), 0)
    h_5 = max((((0.34292883 * float(x[0]))+ (-1.736782 * float(x[1]))+ (1.9086188 * float(x[2]))+ (0.23887011 * float(x[3]))+ (2.3226433 * float(x[4]))+ (2.5103393 * float(x[5]))+ (2.1532133 * float(x[6]))+ (2.319672 * float(x[7]))+ (-0.7942701 * float(x[8]))+ (-0.40381035 * float(x[9]))+ (-3.8888628 * float(x[10]))+ (1.3504713 * float(x[11]))+ (1.5034649 * float(x[12]))+ (-2.0950115 * float(x[13]))+ (-5.8048534 * float(x[14]))+ (-0.93438584 * float(x[15]))+ (4.0235443 * float(x[16]))+ (0.29706785 * float(x[17]))+ (5.0108337 * float(x[18]))+ (0.3039672 * float(x[19]))+ (-0.8264634 * float(x[20]))+ (1.2414895 * float(x[21]))+ (-0.15368856 * float(x[22]))+ (0.71595615 * float(x[23]))+ (0.9170495 * float(x[24]))+ (-1.1808627 * float(x[25]))+ (0.40826765 * float(x[26]))+ (0.004462907 * float(x[27]))+ (0.25630376 * float(x[28]))+ (0.037682347 * float(x[29]))+ (0.29065707 * float(x[30]))+ (0.20804456 * float(x[31]))+ (0.30416402 * float(x[32]))) + 1.7116077), 0)
    h_6 = max((((3.1391335 * float(x[0]))+ (-0.32975018 * float(x[1]))+ (0.43812442 * float(x[2]))+ (0.9070143 * float(x[3]))+ (0.8975758 * float(x[4]))+ (3.120573 * float(x[5]))+ (1.1271969 * float(x[6]))+ (3.5379155 * float(x[7]))+ (-1.4150316 * float(x[8]))+ (1.170454 * float(x[9]))+ (-0.5205269 * float(x[10]))+ (0.30274355 * float(x[11]))+ (0.74418116 * float(x[12]))+ (-0.24353263 * float(x[13]))+ (-0.8155401 * float(x[14]))+ (-0.9332893 * float(x[15]))+ (2.0525846 * float(x[16]))+ (-0.16483523 * float(x[17]))+ (1.6824818 * float(x[18]))+ (1.1899967 * float(x[19]))+ (0.6664927 * float(x[20]))+ (-0.16189872 * float(x[21]))+ (-0.92558277 * float(x[22]))+ (0.35093942 * float(x[23]))+ (-0.25670993 * float(x[24]))+ (-0.32824138 * float(x[25]))+ (-0.5481606 * float(x[26]))+ (-1.0183493 * float(x[27]))+ (0.7140369 * float(x[28]))+ (0.47832152 * float(x[29]))+ (0.594315 * float(x[30]))+ (0.40842426 * float(x[31]))+ (-0.7366008 * float(x[32]))) + 0.4161262), 0)
    h_7 = max((((-1.8249804 * float(x[0]))+ (-0.33282694 * float(x[1]))+ (0.54862624 * float(x[2]))+ (-2.0601172 * float(x[3]))+ (0.22496969 * float(x[4]))+ (2.0134852 * float(x[5]))+ (1.2212113 * float(x[6]))+ (1.4780446 * float(x[7]))+ (-0.25187424 * float(x[8]))+ (0.00027519366 * float(x[9]))+ (-0.4033195 * float(x[10]))+ (0.68529534 * float(x[11]))+ (0.51336426 * float(x[12]))+ (-0.17711174 * float(x[13]))+ (-0.037690047 * float(x[14]))+ (-0.024365505 * float(x[15]))+ (1.6935642 * float(x[16]))+ (-0.34075978 * float(x[17]))+ (1.296378 * float(x[18]))+ (1.4953986 * float(x[19]))+ (0.4383006 * float(x[20]))+ (-0.6944257 * float(x[21]))+ (-0.77316594 * float(x[22]))+ (0.07471105 * float(x[23]))+ (-0.0056868587 * float(x[24]))+ (-0.46136203 * float(x[25]))+ (-0.20192893 * float(x[26]))+ (-0.79794014 * float(x[27]))+ (0.316707 * float(x[28]))+ (0.47473782 * float(x[29]))+ (1.43038 * float(x[30]))+ (0.12343818 * float(x[31]))+ (-1.1882517 * float(x[32]))) + 3.5892324), 0)
    o[0] = (0.23518734 * h_0)+ (-0.3054972 * h_1)+ (-0.4785025 * h_2)+ (-0.25945395 * h_3)+ (0.61764973 * h_4)+ (-4.047899 * h_5)+ (2.1087313 * h_6)+ (2.8822942 * h_7) + -5.735834

    

    #Output Decision Rule
    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)


def Predict(arr,headerless,csvfile, get_key, classmapping):
    with open(csvfile, 'r') as csvinput:
        #readers and writers
        writer = csv.writer(sys.stdout, lineterminator=os.linesep)
        reader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            writer.writerow(','.join(next(reader, None) + ["Prediction"]))
        
        
        for i, row in enumerate(reader):
            #use the transformed array as input to predictor
            pred = str(get_key(int(classify(arr[i])), classmapping))
            #use original untransformed line to write out
            row.append(pred)
            writer.writerow(row)


def Validate(arr):
    if n_classes == 2:
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        outputs=[]
        for i, row in enumerate(arr):
            outputs.append(int(classify(arr[i, :-1].tolist())))
        outputs=np.array(outputs)
        correct_count = int(np.sum(outputs.reshape(-1) == arr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(arr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(arr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0
    else:
        numeachclass = {}
        count, correct_count = 0, 0
        preds = []
        for i, row in enumerate(arr):
            pred = int(classify(arr[i].tolist()))
            preds.append(pred)
            if pred == int(float(arr[i, -1])):
                correct_count += 1
                if int(float(arr[i, -1])) in numeachclass.keys():
                    numeachclass[int(float(arr[i, -1]))] += 1
                else:
                    numeachclass[int(float(arr[i, -1]))] = 0
            count += 1
        return count, correct_count, numeachclass, preds
    


# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()


    #clean if not already clean
    if not args.cleanfile:
        tempdir = tempfile.gettempdir()
        cleanfile = tempdir + os.sep + "clean.csv"
        preprocessedfile = tempdir + os.sep + "prep.csv"
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x,y: x
        classmapping = {}


    #load file
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')


    #Normalize
    cleanarr = Normalize(cleanarr)


    #Transform
    if transform_true:
        if args.validate:
            trans = transform(cleanarr[:, :-1])
            cleanarr = np.concatenate((trans, cleanarr[:, -1].reshape(-1, 1)), axis = 1)
        else:
            cleanarr = transform(cleanarr)


    #Predict
    if not args.validate:
        Predict(cleanarr, args.headerless, preprocessedfile, get_key, classmapping)


    #Validate
    else: 
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = Validate(cleanarr)
        else:
            count, correct_count, numeachclass, preds = Validate(cleanarr)
            #Correct Labels
            true_labels = cleanarr[:, -1]


        #Report Metrics
        model_cap=281
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
            print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
            print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
            print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
            if int(num_TP + num_FN) != 0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN + num_FP) != 0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP + num_FP) != 0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2 * num_TP + num_FP + num_FN) != 0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP + num_FN) != 0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP + num_FN + num_FP) != 0:
                print("Critical Success Index:             {:.2f}".format(TS))

        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            print("System Type:                        " + str(n_classes) + "-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")





            def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
                #check for numpy/scipy is imported
                try:
                    from scipy.sparse import coo_matrix #required for multiclass metrics
                    try:
                        np.array
                    except:
                        import numpy as np
                except:
                    raise ValueError("Scipy and Numpy Required for Multiclass Metrics")
                # Compute confusion matrix to evaluate the accuracy of a classification.
                # By definition a confusion matrix :math:C is such that :math:C_{i, j}
                # is equal to the number of observations known to be in group :math:i and
                # predicted to be in group :math:j.
                # Thus in binary classification, the count of true negatives is
                # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
                # :math:C_{1,1} and false positives is :math:C_{0,1}.
                # Read more in the :ref:User Guide <confusion_matrix>.
                # Parameters
                # ----------
                # y_true : array-like of shape (n_samples,)
                # Ground truth (correct) target values.
                # y_pred : array-like of shape (n_samples,)
                # Estimated targets as returned by a classifier.
                # labels : array-like of shape (n_classes), default=None
                # List of labels to index the matrix. This may be used to reorder
                # or select a subset of labels.
                # If None is given, those that appear at least once
                # in y_true or y_pred are used in sorted order.
                # sample_weight : array-like of shape (n_samples,), default=None
                # Sample weights.
                # normalize : {'true', 'pred', 'all'}, default=None
                # Normalizes confusion matrix over the true (rows), predicted (columns)
                # conditions or all the population. If None, confusion matrix will not be
                # normalized.
                # Returns
                # -------
                # C : ndarray of shape (n_classes, n_classes)
                # Confusion matrix.
                # References
                # ----------
                if labels is None:
                    labels = np.array(list(set(list(y_true.astype('int')))))
                else:
                    labels = np.asarray(labels)
                    if np.all([l not in y_true for l in labels]):
                        raise ValueError("At least one label specified must be in y_true")


                if sample_weight is None:
                    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
                else:
                    sample_weight = np.asarray(sample_weight)
                if y_true.shape[0]!=y_pred.shape[0]:
                    raise ValueError("y_true and y_pred must be of the same length")

                if normalize not in ['true', 'pred', 'all', None]:
                    raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


                n_labels = labels.size
                label_to_ind = {y: x for x, y in enumerate(labels)}
                # convert yt, yp into index
                y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
                y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
                # intersect y_pred, y_true with labels, eliminate items not in labels
                ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
                y_pred = y_pred[ind]
                y_true = y_true[ind]
                # also eliminate weights of eliminated items
                sample_weight = sample_weight[ind]
                # Choose the accumulator dtype to always have high precision
                if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                    dtype = np.int64
                else:
                    dtype = np.float64
                cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


                with np.errstate(all='ignore'):
                    if normalize == 'true':
                        cm = cm / cm.sum(axis=1, keepdims=True)
                    elif normalize == 'pred':
                        cm = cm / cm.sum(axis=0, keepdims=True)
                    elif normalize == 'all':
                        cm = cm / cm.sum()
                    cm = np.nan_to_num(cm)
                return cm


            print("Confusion Matrix:")
            mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #Clean Up
    if not args.cleanfile:
        os.remove(cleanfile)
        os.remove(preprocessedfile)
