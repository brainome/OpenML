#!/usr/bin/env python3
#
# This code is licensed under GNU GPL v2.0 or higher. Please see LICENSE for details.
#
#
# Output of Brainome Daimensions(tm) Table Compiler v0.91.
# Compile time: Mar-20-2020 00:29:07
# Invocation: btc -server brain.brainome.ai Data/spectrometer.csv -o Models/spectrometer.py -v -v -v -stopat 97.93 -port 8100 -f NN -e 10
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                89.64%
Model accuracy:                     98.30% (522/531 correct)
Improvement over best guess:        8.66% (of possible 10.36%)
Model capacity (MEC):               33 bits
Generalization ratio:               15.81 bits/bit
Model efficiency:                   0.26%/parameter
System behavior
True Negatives:                     88.32% (469/531)
True Positives:                     9.98% (53/531)
False Negatives:                    0.38% (2/531)
False Positives:                    1.32% (7/531)
True Pos. Rate/Sensitivity/Recall:  0.96
True Neg. Rate/Specificity:         0.99
Precision:                          0.88
F-1 Measure:                        0.92
False Negative Rate/Miss Rate:      0.04
Critical Success Index:             0.85

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF=100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE="spectrometer.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 102
n_classes = 2

mappings = [{23348331.0: 0, 25559033.0: 1, 38061252.0: 2, 38602410.0: 3, 86774930.0: 4, 91012968.0: 5, 109300583.0: 6, 119792225.0: 7, 125509343.0: 8, 161349050.0: 9, 168132081.0: 10, 174415665.0: 11, 184456274.0: 12, 188006130.0: 13, 199859779.0: 14, 218378534.0: 15, 233250084.0: 16, 262013015.0: 17, 273067914.0: 18, 296913859.0: 19, 298045331.0: 20, 299341624.0: 21, 334022802.0: 22, 343713860.0: 23, 345464906.0: 24, 354641221.0: 25, 383836606.0: 26, 384396485.0: 27, 384681946.0: 28, 384751539.0: 29, 414801568.0: 30, 423762791.0: 31, 425571129.0: 32, 430526275.0: 33, 535239784.0: 34, 546982200.0: 35, 571659272.0: 36, 583397464.0: 37, 614842720.0: 38, 620562669.0: 39, 630853198.0: 40, 633491979.0: 41, 636665188.0: 42, 649794698.0: 43, 654412227.0: 44, 662336163.0: 45, 709066325.0: 46, 711951533.0: 47, 724481309.0: 48, 731596317.0: 49, 751524220.0: 50, 753464870.0: 51, 780198305.0: 52, 786791059.0: 53, 800857581.0: 54, 804373186.0: 55, 825063393.0: 56, 831022265.0: 57, 831946241.0: 58, 842151872.0: 59, 848931938.0: 60, 854421628.0: 61, 862180365.0: 62, 875610277.0: 63, 879679011.0: 64, 910723369.0: 65, 915608694.0: 66, 922764325.0: 67, 936496254.0: 68, 962711257.0: 69, 977888143.0: 70, 1010739497.0: 71, 1040496559.0: 72, 1101459911.0: 73, 1105624585.0: 74, 1131417354.0: 75, 1141691946.0: 76, 1149631284.0: 77, 1156580910.0: 78, 1199699668.0: 79, 1213678071.0: 80, 1224969054.0: 81, 1244904046.0: 82, 1258999759.0: 83, 1260595919.0: 84, 1264614336.0: 85, 1265048915.0: 86, 1269561068.0: 87, 1274939019.0: 88, 1290498000.0: 89, 1297804077.0: 90, 1315346733.0: 91, 1336234720.0: 92, 1359037266.0: 93, 1391571512.0: 94, 1393147811.0: 95, 1393421002.0: 96, 1405550051.0: 97, 1408204714.0: 98, 1408720125.0: 99, 1424922869.0: 100, 1427595713.0: 101, 1449238608.0: 102, 1452183213.0: 103, 1460941593.0: 104, 1471951232.0: 105, 1478417691.0: 106, 1480841914.0: 107, 1485031896.0: 108, 1508396632.0: 109, 1510126952.0: 110, 1552744342.0: 111, 1561602912.0: 112, 1562243179.0: 113, 1595558148.0: 114, 1603057064.0: 115, 1609905507.0: 116, 1615741628.0: 117, 1615788900.0: 118, 1629745460.0: 119, 1680930698.0: 120, 1684816005.0: 121, 1698629485.0: 122, 1719291999.0: 123, 1721079684.0: 124, 1728978383.0: 125, 1732350810.0: 126, 1739640145.0: 127, 1755482099.0: 128, 1762808275.0: 129, 1781428909.0: 130, 1791979456.0: 131, 1818802479.0: 132, 1819090709.0: 133, 1856086907.0: 134, 1856400329.0: 135, 1863470125.0: 136, 1881635922.0: 137, 1882352747.0: 138, 1886195044.0: 139, 1894440665.0: 140, 1895863148.0: 141, 1899934940.0: 142, 1903336100.0: 143, 1927380194.0: 144, 1937706868.0: 145, 1949148063.0: 146, 1950260940.0: 147, 1984268439.0: 148, 1993089788.0: 149, 2037419976.0: 150, 2041355748.0: 151, 2042547590.0: 152, 2069122429.0: 153, 2076523148.0: 154, 2092076078.0: 155, 2094614699.0: 156, 2098253417.0: 157, 2175103328.0: 158, 2176499031.0: 159, 2190445080.0: 160, 2194846467.0: 161, 2219240384.0: 162, 2223856877.0: 163, 2228873448.0: 164, 2270076545.0: 165, 2278958379.0: 166, 2281594658.0: 167, 2282247827.0: 168, 2291138732.0: 169, 2329159818.0: 170, 2344207782.0: 171, 2361664330.0: 172, 2361688722.0: 173, 2362040188.0: 174, 2367013849.0: 175, 2367641050.0: 176, 2396958102.0: 177, 2397504735.0: 178, 2416361653.0: 179, 2418657568.0: 180, 2424380733.0: 181, 2424533487.0: 182, 2436199156.0: 183, 2438648499.0: 184, 2470243115.0: 185, 2480749744.0: 186, 2502085655.0: 187, 2502736717.0: 188, 2515765850.0: 189, 2536171673.0: 190, 2583614056.0: 191, 2585309694.0: 192, 2600393076.0: 193, 2619642019.0: 194, 2638892969.0: 195, 2653350074.0: 196, 2654935236.0: 197, 2680256784.0: 198, 2698538993.0: 199, 2708039104.0: 200, 2719997088.0: 201, 2728369254.0: 202, 2733154694.0: 203, 2752984677.0: 204, 2769241875.0: 205, 2773419789.0: 206, 2786617479.0: 207, 2789649932.0: 208, 2820857143.0: 209, 2821907415.0: 210, 2824560688.0: 211, 2831932330.0: 212, 2855606440.0: 213, 2856833130.0: 214, 2857146073.0: 215, 2895931572.0: 216, 2905552483.0: 217, 2923524600.0: 218, 2936962036.0: 219, 2951175841.0: 220, 2953483848.0: 221, 2964625084.0: 222, 2969996456.0: 223, 2971263201.0: 224, 2979361307.0: 225, 2984729252.0: 226, 2987866105.0: 227, 3006068448.0: 228, 3006915604.0: 229, 3011145050.0: 230, 3030283117.0: 231, 3034530140.0: 232, 3045336044.0: 233, 3084341057.0: 234, 3088637367.0: 235, 3113233047.0: 236, 3124675403.0: 237, 3131553774.0: 238, 3136145483.0: 239, 3153408193.0: 240, 3230561416.0: 241, 3231424525.0: 242, 3266496511.0: 243, 3269980854.0: 244, 3281381822.0: 245, 3299207448.0: 246, 3316397423.0: 247, 3318383810.0: 248, 3319384568.0: 249, 3340344274.0: 250, 3351278157.0: 251, 3367012336.0: 252, 3368260257.0: 253, 3381918623.0: 254, 3388509122.0: 255, 3389010798.0: 256, 3411681748.0: 257, 3434765236.0: 258, 3461658625.0: 259, 3465069838.0: 260, 3567083521.0: 261, 3573867197.0: 262, 3605624243.0: 263, 3607546831.0: 264, 3626635319.0: 265, 3635906240.0: 266, 3651309323.0: 267, 3651495614.0: 268, 3659864761.0: 269, 3673344594.0: 270, 3673652709.0: 271, 3682419490.0: 272, 3684409129.0: 273, 3703763090.0: 274, 3712367430.0: 275, 3714043866.0: 276, 3725504054.0: 277, 3728028160.0: 278, 3732007525.0: 279, 3745353841.0: 280, 3755886702.0: 281, 3794140985.0: 282, 3827057765.0: 283, 3832257131.0: 284, 3848971698.0: 285, 3850278977.0: 286, 3861968832.0: 287, 3884485244.0: 288, 3922925967.0: 289, 3935874256.0: 290, 3944441814.0: 291, 3979114470.0: 292, 3980022793.0: 293, 3982323801.0: 294, 3987170506.0: 295, 4000454467.0: 296, 4018219678.0: 297, 4022765042.0: 298, 4072337212.0: 299, 4092606073.0: 300, 4109785748.0: 301, 4114051570.0: 302, 4135130014.0: 303, 4136539668.0: 304, 4145942959.0: 305, 4147344872.0: 306, 4167606728.0: 307, 4175946824.0: 308, 4186584856.0: 309, 4199748617.0: 310, 4211735132.0: 311, 4230439124.0: 312, 4252952229.0: 313, 4264802653.0: 314, 4277166646.0: 315, 4288806133.0: 316, 4292300670.0: 317, 1788175167.0: 318, 3262722028.0: 319, 3743009523.0: 320, 3852017345.0: 321, 1716134778.0: 322, 3697331230.0: 323, 418076049.0: 324, 545932207.0: 325, 1457526258.0: 326, 3944322266.0: 327, 2135261105.0: 328, 2443967942.0: 329, 362813769.0: 330, 3720994225.0: 331, 1887859267.0: 332, 2174355332.0: 333, 1834061884.0: 334, 2550096478.0: 335, 3744073227.0: 336, 1243125259.0: 337, 3477357573.0: 338, 2263632572.0: 339, 4014819284.0: 340, 690353904.0: 341, 2147525791.0: 342, 1471572028.0: 343, 2983769086.0: 344, 3417670682.0: 345, 3376738937.0: 346, 106552395.0: 347, 1268105659.0: 348, 4058056337.0: 349, 3325152634.0: 350, 1081618286.0: 351, 3029812027.0: 352, 3017169328.0: 353, 2540679764.0: 354, 2507810812.0: 355, 1515413251.0: 356, 1408191589.0: 357, 208944628.0: 358, 2026356848.0: 359, 3529401203.0: 360, 3933136696.0: 361, 558963494.0: 362, 181454204.0: 363, 3391781255.0: 364, 2273387291.0: 365, 541035355.0: 366, 3934068655.0: 367, 487995803.0: 368, 3476609215.0: 369, 3426259749.0: 370, 1996580163.0: 371, 3091056213.0: 372, 160734547.0: 373, 3202314838.0: 374, 1511306530.0: 375, 588640262.0: 376, 1244067738.0: 377, 2687615507.0: 378, 541017236.0: 379, 2324601323.0: 380, 1419865583.0: 381, 2030680114.0: 382, 2345832027.0: 383, 1359805545.0: 384, 2251851080.0: 385, 109768202.0: 386, 2302091470.0: 387, 510886413.0: 388, 3361705054.0: 389, 1143463705.0: 390, 3851346818.0: 391, 3764136716.0: 392, 2635085564.0: 393, 546854341.0: 394, 2510976615.0: 395, 153011272.0: 396, 2042088898.0: 397, 558431427.0: 398, 3687922345.0: 399, 1233709753.0: 400, 4037755128.0: 401, 1814597409.0: 402, 2692450117.0: 403, 3674169666.0: 404, 2969458129.0: 405, 2920429991.0: 406, 378598744.0: 407, 2608602717.0: 408, 2716998122.0: 409, 1413277611.0: 410, 1603258338.0: 411, 4087318976.0: 412, 2050503892.0: 413, 3335961797.0: 414, 3444870002.0: 415, 4118175972.0: 416, 1300873656.0: 417, 3380194261.0: 418, 1330520959.0: 419, 1831645480.0: 420, 1679215821.0: 421, 4163010534.0: 422, 1982069323.0: 423, 2140923543.0: 424, 221170312.0: 425, 1295885240.0: 426, 2828820373.0: 427, 4004856071.0: 428, 2786005050.0: 429, 1431292245.0: 430, 1674949354.0: 431, 2009607175.0: 432, 3965548273.0: 433, 3435832325.0: 434, 1040819615.0: 435, 1235652190.0: 436, 1464700339.0: 437, 1834866438.0: 438, 1929408257.0: 439, 2587214501.0: 440, 4097233706.0: 441, 1925988044.0: 442, 2416279450.0: 443, 1813264444.0: 444, 1542443272.0: 445, 775346484.0: 446, 2090307474.0: 447, 133828950.0: 448, 1209866743.0: 449, 1679892458.0: 450, 1001744452.0: 451, 2585578819.0: 452, 2468405095.0: 453, 1318518149.0: 454, 184695789.0: 455, 463700242.0: 456, 1824345750.0: 457, 2025116234.0: 458, 547010295.0: 459, 491629898.0: 460, 3726682671.0: 461, 3750496083.0: 462, 3852185910.0: 463, 3519722214.0: 464, 359038278.0: 465, 448390488.0: 466, 841137832.0: 467, 759173473.0: 468, 2441440671.0: 469, 3087066200.0: 470, 2769554661.0: 471, 3316738094.0: 472, 3417868334.0: 473, 363773262.0: 474, 1954166276.0: 475, 3232540693.0: 476, 3414256357.0: 477, 1085196.0: 478, 4010025716.0: 479, 3404292542.0: 480, 779766362.0: 481, 802166244.0: 482, 3700004288.0: 483, 4108641434.0: 484, 3039310747.0: 485, 1009782162.0: 486, 1687658496.0: 487, 3750703687.0: 488, 1098960550.0: 489, 3527347559.0: 490, 366727047.0: 491, 3783278189.0: 492, 1015874009.0: 493, 2626371972.0: 494, 118621575.0: 495, 2022666289.0: 496, 3492484198.0: 497, 2798130732.0: 498, 1147670791.0: 499, 2082277058.0: 500, 171574187.0: 501, 1782605130.0: 502, 1502048324.0: 503, 364349170.0: 504, 196898627.0: 505, 913866012.0: 506, 4170640359.0: 507, 3754660787.0: 508, 3920616253.0: 509, 2003270807.0: 510, 2007554972.0: 511, 2151719954.0: 512, 4282390830.0: 513, 3541378818.0: 514, 84852501.0: 515, 4039526604.0: 516, 2863695910.0: 517, 3332840706.0: 518, 1834434176.0: 519, 1905811453.0: 520, 2507175545.0: 521, 3541570877.0: 522, 547587037.0: 523, 611786515.0: 524, 4205653237.0: 525, 1132867658.0: 526, 4242160080.0: 527, 644286260.0: 528, 1395902443.0: 529, 2908668711.0: 530}]
list_of_cols_to_normalize = [0]

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values()))+1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize,mappings):
            if i>=data_arr.shape[1]:
                break
            col = data_arr[:,i]
            normcol = column_norm(col,mapping)
            data_arr[:,i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([158.5, 2.449685534591195, 18.005122641509434, -10.152066037735846, 0.0, 828.6320754716982, 860.6509433962265, 473.2138364779874, 505.87106918238993, 12499.70407342768, 12230.604419496853, 11992.236893081756, 11562.221435534602, 11139.298971698108, 10856.630129874218, 10471.045083647794, 10341.628637106915, 10233.00739937107, 10144.582082169813, 10058.105925471698, 10078.76703251573, 10036.575027672952, 9796.408087327045, 9557.991257154086, 9213.658567213835, 9006.466217264146, 8770.697651572335, 8500.668542452822, 8298.080747641512, 8135.740477248424, 7864.098644622637, 7637.965282515716, 7419.601235660377, 7110.206179937112, 6915.607956289304, 6598.011614150942, 6391.203945283021, 6085.7830534591185, 5854.551259748425, 5585.49447421384, 5320.231474213836, 5029.445747798743, 4826.227892138365, 4595.820401257857, 4453.398939622641, 4358.485995283017, 4206.351212578615, 4062.9719915094347, 3998.322160377362, 3849.97641509434, 3726.293012893083, 3636.3534232704424, 3538.2693959119524, 8030.732540682392, 7483.774282547165, 6823.180189937108, 6193.102223899377, 5444.014241823902, 4821.002488364779, 4341.820583647798, 3971.5330792452846, 3642.631366666667, 3384.104109748429, 3159.1385628930816, 2999.805420754719, 2827.0716984276737, 2732.208795283019, 2604.273921383649, 2575.1439663522015, 2508.3464569182393, 2460.3862402515715, 2393.0054789308165, 2374.6258216981137, 2337.516435125785, 2294.3728370125773, 2233.353575817609, 2189.6838182075476, 2142.5057611635216, 2080.109304779875, 2042.3684130817612, 1976.5735672012574, 1933.4333143396232, 1878.5400549685542, 1823.0285983647793, 1771.7712827358496, 1702.7918167295609, 1683.706118396227, 1616.6047715723273, 1535.535955062893, 1552.8828295283017, 1485.669201446541, 1410.9557594339624, 1428.9726040251574, 1411.3548195283024, 1313.3987595597484, 1274.274691603773, 1292.4647963773577, 1253.5291337106921, 1219.2132319496848, 1198.5095715408806, 1137.231874198113, 1020.9569044339623])
        components = np.array([array([ 6.44631459e-05,  2.14667429e-05, -2.15241641e-06,  3.53598248e-04,
        1.38777878e-17, -1.18875050e-02, -1.21771048e-02, -6.94234065e-03,
       -8.12335555e-03,  4.52949640e-01,  4.24148905e-01,  3.90351157e-01,
        3.46655090e-01,  2.98037077e-01,  2.43648947e-01,  1.92479430e-01,
        1.43132547e-01,  8.99394688e-02,  4.20305028e-02, -2.86769919e-03,
       -3.40947760e-02, -5.64050801e-02, -7.22484201e-02, -8.09375405e-02,
       -8.32967308e-02, -8.35220551e-02, -7.93171784e-02, -7.83004624e-02,
       -7.27274325e-02, -6.57943518e-02, -6.01962127e-02, -5.63412402e-02,
       -5.15283057e-02, -5.07690840e-02, -4.44267236e-02, -3.81688602e-02,
       -3.47694466e-02, -2.75767837e-02, -2.23393758e-02, -1.68123459e-02,
       -9.18052720e-03, -7.08095624e-03, -4.11093072e-03, -4.17252164e-03,
       -8.04740067e-04, -2.53268670e-04,  5.17809243e-04, -2.56813343e-04,
        1.26303764e-04,  6.90623495e-04,  1.44289384e-03,  2.88315208e-03,
        2.57670212e-03, -6.16389239e-02, -5.18283309e-02, -4.04777347e-02,
       -2.69559264e-02, -1.43938852e-02, -5.95549583e-03, -2.52532638e-03,
       -1.14746728e-04, -3.04508566e-03, -3.53076820e-03, -5.13264644e-03,
       -7.58475076e-03, -1.05475622e-02, -1.25944589e-02, -1.87015855e-02,
       -2.27639514e-02, -2.63098707e-02, -3.12201016e-02, -3.44590476e-02,
       -3.74832484e-02, -3.95309618e-02, -4.26521839e-02, -4.45495659e-02,
       -4.39874755e-02, -4.51488080e-02, -4.61148312e-02, -4.63211527e-02,
       -4.48067708e-02, -4.67859172e-02, -4.46106616e-02, -4.27927933e-02,
       -4.22387747e-02, -3.87132052e-02, -3.80446191e-02, -3.73678546e-02,
       -3.59652661e-02, -3.63353811e-02, -3.51879264e-02, -3.40261671e-02,
       -3.38426202e-02, -3.32117084e-02, -3.07385166e-02, -3.04529667e-02,
       -3.02138083e-02, -2.90964981e-02, -2.75487672e-02, -2.68822979e-02,
       -2.55978311e-02, -2.32818575e-02]), array([ 2.40247128e-04, -4.15753279e-05, -2.84725853e-05, -4.32654924e-04,
        0.00000000e+00,  1.01768155e-02,  9.74663316e-03,  7.69094070e-03,
        9.68929082e-03, -2.09392052e-02, -1.69786456e-03, -1.06616596e-03,
       -1.14377839e-02, -2.93656360e-02, -5.38514295e-02, -8.11928153e-02,
       -1.15011050e-01, -1.51628638e-01, -1.84553373e-01, -2.12822528e-01,
       -2.28956470e-01, -2.40207262e-01, -2.41744809e-01, -2.37107448e-01,
       -2.25487015e-01, -2.10318348e-01, -1.91788990e-01, -1.80309696e-01,
       -1.66904233e-01, -1.54880070e-01, -1.38740193e-01, -1.23635602e-01,
       -1.08457665e-01, -9.39526060e-02, -7.66174883e-02, -5.95633566e-02,
       -4.76062033e-02, -3.29855846e-02, -2.23133025e-02, -1.04928360e-02,
        3.15105575e-03,  1.33254622e-02,  2.11290245e-02,  2.73282207e-02,
        3.54314608e-02,  4.21079947e-02,  5.41070852e-02,  5.30371889e-02,
        4.96850551e-02,  4.63950206e-02,  4.35379366e-02,  4.29947280e-02,
        4.80522671e-02, -1.47444765e-01, -1.19446968e-01, -7.90960074e-02,
       -3.80284828e-02,  2.72671527e-04,  2.85041896e-02,  4.50052597e-02,
        5.36421703e-02,  5.78517196e-02,  6.25280471e-02,  6.51240677e-02,
        6.90871402e-02,  7.01170855e-02,  7.22599794e-02,  7.39615406e-02,
        7.55336424e-02,  7.51757931e-02,  7.44652857e-02,  7.61037497e-02,
        7.34801928e-02,  7.35688160e-02,  7.45914163e-02,  7.51334577e-02,
        7.49139887e-02,  7.52717769e-02,  7.69696596e-02,  7.86722212e-02,
        8.34463972e-02,  8.59995918e-02,  8.48377181e-02,  8.54746811e-02,
        8.55443373e-02,  8.65909050e-02,  8.86448306e-02,  9.06398615e-02,
        8.86888525e-02,  9.70437160e-02,  9.38312917e-02,  9.39445034e-02,
        1.01480155e-01,  1.02593272e-01,  9.72230055e-02,  9.86817199e-02,
        1.02374194e-01,  1.03075148e-01,  1.01286338e-01,  1.02192077e-01,
        9.80901563e-02,  8.54533062e-02]), array([-1.21999652e-03,  1.50503734e-04, -2.77618907e-05, -7.49749173e-04,
        2.22044605e-16, -8.37810023e-03, -7.98282476e-03, -2.39310174e-03,
        1.23960840e-03,  1.77550325e-01,  1.05497115e-01,  3.21369382e-02,
       -2.37643443e-02, -4.05496020e-02, -3.93157809e-02, -3.57069900e-02,
       -1.77558415e-02,  2.44908074e-02,  6.94787753e-02,  1.27299866e-01,
        1.52357881e-01,  1.54076904e-01,  1.50105677e-01,  1.36731429e-01,
        1.13742217e-01,  8.25739724e-02,  6.33723330e-02,  3.25687416e-02,
        6.64612601e-03, -1.70168762e-02, -4.52866303e-02, -6.35670619e-02,
       -7.28332436e-02, -8.71951445e-02, -9.49956277e-02, -1.08870466e-01,
       -1.32836873e-01, -1.43234204e-01, -1.45058191e-01, -1.51903209e-01,
       -1.58250000e-01, -1.56755650e-01, -1.51934616e-01, -1.47229575e-01,
       -1.35276983e-01, -1.21221181e-01, -1.03309048e-01, -1.07753504e-01,
       -1.19473592e-01, -1.24193769e-01, -1.26045539e-01, -1.24247103e-01,
       -1.22450044e-01, -2.18328933e-02, -6.40167673e-02, -1.04930574e-01,
       -1.42387479e-01, -1.52633098e-01, -1.41541209e-01, -1.29974910e-01,
       -1.20757240e-01, -1.11126596e-01, -9.50011324e-02, -8.64021742e-02,
       -8.06976285e-02, -6.99391644e-02, -5.36973640e-02, -3.99750890e-02,
       -2.90318303e-02, -1.01675486e-02, -1.41822347e-03,  1.67400719e-02,
        2.87728251e-02,  3.36068026e-02,  4.07603745e-02,  5.30902052e-02,
        6.14370027e-02,  6.82538357e-02,  7.57040741e-02,  8.37650238e-02,
        9.38670934e-02,  9.76200866e-02,  9.89012330e-02,  9.65662772e-02,
        9.97292113e-02,  1.00335851e-01,  1.04538975e-01,  1.04137363e-01,
        9.94808069e-02,  1.12731300e-01,  1.10184474e-01,  1.16089467e-01,
        1.25760578e-01,  1.25191518e-01,  1.21636246e-01,  1.23700743e-01,
        1.28143055e-01,  1.30970792e-01,  1.31509538e-01,  1.29366399e-01,
        1.24747852e-01,  1.07593246e-01]), array([ 1.49217692e-04,  1.84171458e-04,  1.35872410e-04,  2.52882502e-03,
       -6.93889390e-18,  7.76075734e-03,  6.27301368e-03,  5.82761216e-03,
        4.00829152e-03,  3.25733340e-01,  2.71992385e-01,  1.45962899e-01,
       -1.99731700e-02, -1.50713740e-01, -2.68784970e-01, -3.21969584e-01,
       -3.54497008e-01, -3.37211099e-01, -2.83085777e-01, -1.83995656e-01,
       -5.40067206e-02,  6.71577543e-02,  1.18340480e-01,  1.61380812e-01,
        1.74997407e-01,  1.58113897e-01,  1.45154739e-01,  9.65387019e-02,
        8.03519711e-02,  4.88896451e-02,  2.24779143e-02, -2.37528960e-04,
       -1.26697174e-02, -3.10888186e-02, -2.87313119e-02, -2.52360156e-02,
       -3.12353983e-02, -2.47490047e-02, -1.75076354e-02, -1.67094776e-02,
        6.94735095e-03,  1.49323087e-02,  3.22585959e-02,  3.79968072e-02,
        5.05672837e-02,  6.06553113e-02,  7.12439160e-02,  7.92349876e-02,
        9.77074337e-02,  9.95463741e-02,  8.82849881e-02,  8.44804759e-02,
        9.24664993e-02,  4.06229037e-02,  2.26119970e-02, -1.43350289e-02,
       -1.96326246e-02, -6.58720775e-03,  1.62294018e-02,  4.26643158e-02,
        6.09394303e-02,  4.81817387e-02,  4.53453625e-02,  3.65440098e-02,
        1.71328657e-02,  9.84908378e-03,  8.12477338e-04, -1.88387234e-02,
       -3.14856101e-02, -2.22679151e-02, -3.20566220e-02, -3.17381114e-02,
       -2.67804128e-02, -2.91735453e-02, -2.74670905e-02, -3.77557545e-02,
       -2.74533664e-02, -2.89267047e-02, -3.28411137e-02, -3.76017842e-02,
       -3.23658654e-02, -3.65444175e-02, -3.23128881e-02, -3.15587432e-02,
       -2.77904942e-02, -2.77023173e-02, -2.00694748e-02, -8.77645168e-03,
       -1.41676143e-02, -1.43404972e-02, -2.18488506e-02, -1.94700130e-02,
       -1.08724374e-02, -1.07942016e-02, -1.34809156e-02, -1.00708694e-02,
       -1.28698299e-02, -1.21812524e-02, -5.87108882e-03, -8.20957117e-03,
       -6.03063615e-03, -9.67524129e-03]), array([-5.13196245e-03, -5.73257397e-05, -3.29361984e-05, -2.88153495e-04,
       -3.60822483e-16,  5.57764177e-02,  5.57614740e-02,  2.71065367e-02,
        2.85236397e-02,  1.97164529e-02,  5.93560704e-02,  1.02865155e-01,
        9.73167740e-02,  5.88384822e-02,  1.09027967e-02, -4.63594097e-02,
       -9.26809069e-02, -1.42578900e-01, -1.65073427e-01, -1.74166108e-01,
       -1.77083587e-01, -1.79331061e-01, -1.49443482e-01, -1.08856855e-01,
       -9.99560937e-02, -4.80514152e-02, -3.49892246e-03,  4.19173338e-02,
        9.35865064e-02,  1.34978773e-01,  1.72624795e-01,  1.96294004e-01,
        2.41034806e-01,  2.17062400e-01,  2.15202622e-01,  1.75812953e-01,
        1.52278958e-01,  1.24200145e-01,  9.98127711e-02,  5.88613876e-02,
        2.76162509e-02, -7.89579207e-03, -2.87426812e-02, -5.35988963e-02,
       -8.06931220e-02, -1.00418938e-01, -9.42985281e-02, -1.22466663e-01,
       -1.53267202e-01, -1.58564993e-01, -1.52071434e-01, -1.55572951e-01,
       -1.50945189e-01,  1.12158977e-01,  1.58446675e-01,  1.80482728e-01,
        1.34803986e-01,  6.06055884e-02, -1.73624388e-02, -7.36852087e-02,
       -1.05450199e-01, -1.20629845e-01, -1.18140926e-01, -1.19288422e-01,
       -1.04564400e-01, -1.03236797e-01, -9.30061198e-02, -8.28120749e-02,
       -8.15091178e-02, -6.84557147e-02, -4.88595573e-02, -4.32382129e-02,
       -3.66672368e-02, -3.05594657e-02, -3.14427269e-02, -1.84165863e-02,
       -1.26150051e-02, -1.91094155e-02, -3.81435387e-03,  2.69294561e-03,
        9.17716698e-03,  5.77618608e-03,  2.28476662e-02,  2.02973020e-02,
        1.26356390e-02,  2.35633319e-02,  2.24165788e-02,  3.39069098e-02,
        3.62635731e-02,  3.35312762e-02,  4.64768014e-02,  5.48233654e-02,
        5.20782597e-02,  6.10923501e-02,  6.68049863e-02,  7.02695646e-02,
        7.18623805e-02,  7.19446441e-02,  8.44988994e-02,  8.53434813e-02,
        8.28706737e-02,  6.05285519e-02]), array([-5.14435166e-04, -4.07495312e-04,  5.52451205e-05, -5.38553551e-04,
        1.66533454e-16,  5.86675657e-01,  5.98713858e-01,  3.50081043e-01,
        3.48533502e-01,  6.97231474e-02, -3.58288346e-03, -2.76606121e-02,
       -3.61821918e-02, -6.31179936e-02, -1.66755424e-02,  1.61847466e-02,
        3.18343312e-02,  7.95325439e-02,  5.49869725e-02,  3.17362991e-02,
        2.15717106e-02,  4.62134358e-03, -6.41295572e-03, -1.15983754e-02,
        1.10495434e-02, -1.63644882e-02,  3.89210971e-05, -2.53755223e-02,
       -2.25772367e-02, -1.61680722e-02, -2.56573515e-02, -1.16357380e-02,
       -3.72858537e-02, -3.62145007e-03, -9.95830088e-03, -1.44023786e-02,
       -2.18904253e-03, -1.65893805e-02, -1.44787559e-02,  1.80508500e-02,
       -3.91748984e-03, -3.25249469e-02, -2.69978147e-02,  1.59308654e-02,
       -9.83674058e-03,  1.48523479e-02,  3.98976204e-02,  2.24873481e-02,
        5.32409214e-02,  1.23954593e-02,  5.63278975e-03,  8.80356081e-03,
        1.43510746e-04,  5.27737268e-02,  1.63060908e-02, -2.75472056e-02,
        1.01041083e-03, -7.88115222e-03, -5.45684579e-03,  2.28824178e-02,
        8.58934419e-03,  3.61090158e-02,  2.38283838e-02,  1.62422200e-02,
        1.43638043e-02, -1.51670605e-02,  1.60485667e-03,  1.99549167e-03,
       -1.01182665e-02, -9.41451986e-03,  4.92265862e-03, -1.97770034e-02,
       -1.95655232e-02, -3.00376117e-02, -1.51738412e-02, -1.50304846e-02,
       -2.20305387e-02, -3.31863399e-02, -8.42305255e-03, -2.11387587e-02,
       -2.52440663e-02, -2.89910793e-02, -2.87318110e-02, -1.05582756e-02,
       -1.68274157e-02, -1.54236492e-02, -9.61456395e-03,  2.40209603e-03,
        7.45839174e-03, -7.95484985e-03,  1.33651894e-03,  3.74386845e-03,
       -5.32360137e-04,  1.29040673e-02,  1.59620093e-03,  5.34099118e-03,
        2.69583792e-02,  1.35703935e-02,  1.65065937e-02,  2.51285245e-02,
        1.64331191e-02,  1.19064574e-02])])
        whiten = False
        explained_variance = np.array([138663649.04107028, 86413897.3725988, 9752329.146999847, 5994322.326694637, 2740355.0034875646, 774595.3457878815])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist=[]
    clean.testfile=testfile
    clean.mapping={}
    

    def convert(cell):
        value=str(cell)
        try:
            result=int(value)
            return result
        except:
            try:
                result=float(value)
                if (rounding!=-1):
                    result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
                return result
            except:
                result=(binascii.crc32(value.encode('utf8')) % (1<<32))
                return result

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value=str(cell)
        if (value==''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping=={}):
            result=-1
            try:
                result=clean.mapping[cell]
            except:
                raise ValueError("Class label '"+value+"' encountered in input not defined in user-provided mapping.")
            if (not result==int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist=clean.classlist+[str(result)]
            return result
        try:
            result=float(cell)
            if (rounding!=-1):
                result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
            else:
                result=int(int(result*100)/100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist=clean.classlist+[str(result)]
        except:
            result=(binascii.crc32(value.encode('utf8')) % (1<<32))
            if (result in clean.classlist):
                result=clean.classlist.index(result)
            else:
                clean.classlist=clean.classlist+[result]
                result=clean.classlist.index(result)
            if (not result==int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result<0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount=0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f=open(outfile,"w+")
        if (headerless==False):
            next(reader,None)
        outbuf=[]
        for row in reader:
            if (row==[]):  # Skip empty rows
                continue
            rowcount=rowcount+1
            rowlen=num_attr
            if (not testfile):
                rowlen=rowlen+1    
            if (not len(row)==rowlen):
                raise ValueError("Column count must match trained predictor. Row "+str(rowcount)+" differs.")
            i=0
            for elem in row:
                if(i+1<len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid=str(convertclassid(elem))
                    outbuf.append(classid)
                i=i+1
            if (len(outbuf)<IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf=[]
        print(''.join(outbuf),end="", file=f)
        f.close()

        if (testfile==False and not len(clean.classlist)>=2):
            raise ValueError("Number of classes must be at least 2.")



# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)

# Classifier
def classify(row):
    x=row
    o=[0]*num_output_logits
    h_0 = max((((-0.46308863 * float(x[0]))+ (0.35356742 * float(x[1]))+ (0.2412946 * float(x[2]))+ (-0.9819495 * float(x[3]))+ (0.25020337 * float(x[4]))+ (-0.15525934 * float(x[5]))) + 4.484553), 0)
    h_1 = max((((0.6430569 * float(x[0]))+ (1.1686436 * float(x[1]))+ (0.32232773 * float(x[2]))+ (-0.06011982 * float(x[3]))+ (0.8299489 * float(x[4]))+ (0.67895573 * float(x[5]))) + -0.06646083), 0)
    h_2 = max((((-5.942523 * float(x[0]))+ (-0.029895559 * float(x[1]))+ (4.2076077 * float(x[2]))+ (-7.1426873 * float(x[3]))+ (2.0736613 * float(x[4]))+ (-2.5314245 * float(x[5]))) + -3.954196), 0)
    h_3 = max((((-0.1359809 * float(x[0]))+ (-0.5040405 * float(x[1]))+ (0.21925412 * float(x[2]))+ (0.43821618 * float(x[3]))+ (-0.0767498 * float(x[4]))+ (-0.11804261 * float(x[5]))) + 4.954756), 0)
    o[0] = (-2.5500925 * h_0)+ (-1.9656472 * h_1)+ (0.24017653 * h_2)+ (-1.8005823 * h_3) + -4.7803817

    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)

# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()
    
    if not args.validate: # Then predict
        if not args.cleanfile: # File is not preprocessed
            tempdir=tempfile.gettempdir()
            cleanfile=tempdir+os.sep+"clean.csv"
            clean(args.csvfile,cleanfile, -1, args.headerless, True)
            test_tensor = np.loadtxt(cleanfile,delimiter=',',dtype='float64')
            os.remove(cleanfile)
        else: # File is already preprocessed
            test_tensor = np.loadtxt(args.File,delimiter = ',',dtype = 'float64')               
        test_tensor = Normalize(test_tensor)
        if transform_true:
            test_tensor = transform(test_tensor)
        with open(args.csvfile,'r') as csvinput:
            writer = csv.writer(sys.stdout, lineterminator='\n')
            reader = csv.reader(csvinput)
            if (not args.headerless):
                writer.writerow((next(reader, None)+['Prediction']))
            i=0
            for row in reader:
                if (classify(test_tensor[i])):
                    pred="1"
                else:
                    pred="0"
                row.append(pred)
                writer.writerow(row)
                i=i+1
    elif args.validate: # Then validate this predictor, always clean first.
        if n_classes==2:
            tempdir=tempfile.gettempdir()
            temp_name = next(tempfile._get_candidate_names())
            cleanfile=tempdir+os.sep+temp_name
            clean(args.csvfile,cleanfile, -1, args.headerless)
            val_tensor = np.loadtxt(cleanfile,delimiter = ',',dtype = 'float64')
            os.remove(cleanfile)
            val_tensor = Normalize(val_tensor)
            if transform_true:
                trans = transform(val_tensor[:,:-1])
                val_tensor = np.concatenate((trans,val_tensor[:,-1].reshape(-1,1)),axis = 1)
            count,correct_count,num_TP,num_TN,num_FP,num_FN,num_class_1,num_class_0 = 0,0,0,0,0,0,0,0
            for i,row in enumerate(val_tensor):
                if int(classify(val_tensor[i].tolist())) == int(float(val_tensor[i,-1])):
                    correct_count+=1
                    if int(float(row[-1]))==1:
                        num_class_1+=1
                        num_TP+=1
                    else:
                        num_class_0+=1
                        num_TN+=1
                else:
                    if int(float(row[-1]))==1:
                        num_class_1+=1
                        num_FN+=1
                    else:
                        num_class_0+=1
                        num_FP+=1
                count+=1
        else:
            tempdir=tempfile.gettempdir()
            temp_name = next(tempfile._get_candidate_names())
            cleanvalfile=tempdir+os.sep+temp_name
            clean(args.csvfile,cleanvalfile, -1, args.headerless)
            val_tensor = np.loadtxt(cleanfile,delimiter = ',',dtype = 'float64')
            os.remove(cleanfile)
            val_tensor = Normalize(val_tensor)
            if transform_true:
                trans = transform(val_tensor[:,:-1])
                val_tensor = np.concatenate((trans,val_tensor[:,-1].reshape(-1,1)),axis = 1)
            numeachclass={}
            count,correct_count = 0,0
            for i,row in enumerate(val_tensor):
                if int(classify(val_tensor[i].tolist())) == int(float(val_tensor[i,-1])):
                    correct_count+=1
                    if int(float(val_tensor[i,-1])) in numeachclass.keys():
                        numeachclass[int(float(val_tensor[i,-1]))]+=1
                    else:
                        numeachclass[int(float(val_tensor[i,-1]))]=0
                count+=1

        model_cap=33

        if n_classes==2:

            FN=float(num_FN)*100.0/float(count)
            FP=float(num_FP)*100.0/float(count)
            TN=float(num_TN)*100.0/float(count)
            TP=float(num_TP)*100.0/float(count)
            num_correct=correct_count

            if int(num_TP+num_FN)!=0:
                TPR=num_TP/(num_TP+num_FN) # Sensitivity, Recall
            if int(num_TN+num_FP)!=0:
                TNR=num_TN/(num_TN+num_FP) # Specificity, 
            if int(num_TP+num_FP)!=0:
                PPV=num_TP/(num_TP+num_FP) # Recall
            if int(num_FN+num_TP)!=0:
                FNR=num_FN/(num_FN+num_TP) # Miss rate
            if int(2*num_TP+num_FP+num_FN)!=0:
                FONE=2*num_TP/(2*num_TP+num_FP+num_FN) # F1 Score
            if int(num_TP+num_FN+num_FP)!=0:
                TS=num_TP/(num_TP+num_FN+num_FP) # Critical Success Index

            randguess=int(float(10000.0*max(num_class_1,num_class_0))/count)/100.0
            modelacc=int(float(num_correct*10000)/count)/100.0

            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc)+" ("+str(int(num_correct))+"/"+str(count)+" correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc-randguess)+" (of possible "+str(round(100-randguess,2))+"%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct*100)/model_cap)/100.0)+" bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100*(modelacc-randguess)/model_cap)/100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN)+" ("+str(int(num_TN))+"/"+str(count)+")")
            print("True Positives:                     {:.2f}%".format(TP)+" ("+str(int(num_TP))+"/"+str(count)+")")
            print("False Negatives:                    {:.2f}%".format(FN)+" ("+str(int(num_FN))+"/"+str(count)+")")
            print("False Positives:                    {:.2f}%".format(FP)+" ("+str(int(num_FP))+"/"+str(count)+")")
            if int(num_TP+num_FN)!=0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN+num_FP)!=0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP+num_FP)!=0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2*num_TP+num_FP+num_FN)!=0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP+num_FN)!=0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP+num_FN+num_FP)!=0:    
                print("Critical Success Index:             {:.2f}".format(TS))
        else:
            num_correct=correct_count
            modelacc=int(float(num_correct*10000)/count)/100.0
            randguess=round(max(numeachclass.values())/sum(numeachclass.values())*100,2)
            print("System Type:                        "+str(n_classes)+"-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc)+" ("+str(int(num_correct))+"/"+str(count)+" correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc-randguess)+" (of possible "+str(round(100-randguess,2))+"%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct*100)/model_cap)/100.0)+" bits/bit")






