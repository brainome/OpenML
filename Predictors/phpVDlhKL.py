#!/usr/bin/env python3
#
# This code is licensed under GNU GPL v2.0 or higher. Please see LICENSE for details.
#
#
# Output of Brainome Daimensions(tm) Table Compiler v0.91.
# Compile time: Mar-19-2020 20:55:46
# Invocation: btc -server brain.brainome.ai Data/phpVDlhKL.csv -o Models/phpVDlhKL.py -v -v -v -stopat 93.75 -port 8100 -f NN -e 10 -target Class -cm {'1':0,'2':1}
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                54.68%
Model accuracy:                     93.75% (60/64 correct)
Improvement over best guess:        39.07% (of possible 45.32%)
Model capacity (MEC):               25 bits
Generalization ratio:               2.40 bits/bit
Model efficiency:                   1.56%/parameter
System behavior
True Negatives:                     50.00% (32/64)
True Positives:                     43.75% (28/64)
False Negatives:                    1.56% (1/64)
False Positives:                    4.69% (3/64)
True Pos. Rate/Sensitivity/Recall:  0.97
True Neg. Rate/Specificity:         0.91
Precision:                          0.90
F-1 Measure:                        0.93
False Negative Rate/Miss Rate:      0.03
Critical Success Index:             0.88

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF=100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE="phpVDlhKL.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 229
n_classes = 2

mappings = []
list_of_cols_to_normalize = []

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values()))+1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize,mappings):
            if i>=data_arr.shape[1]:
                break
            col = data_arr[:,i]
            normcol = column_norm(col,mapping)
            data_arr[:,i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.046875, 0.015625, 0.03125, 0.03125, 0.015625, 0.046875, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.1875, 0.015625, 0.015625, 0.015625, 0.25, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.03125, 0.015625, 0.0625, 0.015625, 0.0625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.109375, 0.015625, 0.015625, 0.046875, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.078125, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.046875, 0.03125, 0.015625, 0.015625, 0.015625, 0.09375, 0.015625, 0.0625, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.046875, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.09375, 0.015625, 0.015625, 0.0625, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.09375, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.09375, 0.015625, 0.046875, 0.015625, 0.078125, 0.265625, 0.046875, 0.046875, 0.03125, 0.015625, 0.015625, 0.015625, 0.03125, 0.03125, 0.015625, 0.015625, 0.0625, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.109375, 0.015625, 0.015625, 0.046875, 0.015625, 0.03125, 0.046875, 0.015625, 0.015625, 0.046875, 0.03125, 0.015625, 0.015625, 0.015625, 0.0625, 0.03125, 0.015625, 0.0625, 0.015625, 0.015625, 0.046875, 0.03125, 0.015625, 0.078125, 0.015625, 0.046875, 0.015625, 0.015625, 0.015625, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.03125, 0.015625, 0.015625, 0.109375, 0.015625, 0.046875, 0.03125, 0.015625, 0.015625, 0.078125, 0.015625, 0.015625, 0.0625, 0.015625])
        components = np.array([array([ 4.95227848e-02,  1.13234551e-01, -4.32377418e-03,  2.09210342e-02,
        2.51822711e-02,  4.37895493e-02, -6.78202906e-03,  1.07428092e-02,
       -1.01110990e-02, -1.02802319e-02, -2.97785076e-03,  1.20302472e-01,
        4.95227848e-02,  8.99649449e-04,  7.76369845e-02,  4.13292697e-02,
       -2.72187623e-03, -1.08033646e-02, -1.79854342e-03,  4.95227848e-02,
        3.95504399e-02, -2.55668155e-02, -2.60795846e-02, -3.20318130e-02,
       -3.63616374e-03,  2.09210342e-02, -3.73102513e-03,  3.44687563e-02,
       -3.62152568e-03, -7.28807818e-03, -2.42574035e-02, -3.97108593e-02,
       -3.73102513e-03, -6.35289199e-03, -2.97785076e-03,  5.82995339e-01,
       -2.83956493e-02, -2.49270178e-02, -3.31327077e-02, -3.17747215e-02,
        3.42398622e-02,  1.13234551e-01,  4.55029421e-02,  5.28543632e-02,
       -3.62152568e-03,  2.06546885e-01, -1.66832609e-03,  4.95227848e-02,
       -2.49270178e-02, -3.73102513e-03, -6.78202906e-03,  3.44687563e-02,
       -4.37185918e-02, -2.48897370e-02, -1.28456519e-02,  2.18079569e-02,
       -1.66832609e-03, -3.63616374e-03, -2.02160742e-04, -1.66832609e-03,
       -1.05004829e-02, -2.91910467e-03,  5.78808639e-02,  3.42398622e-02,
       -3.21530249e-02, -5.62701363e-02, -3.17747215e-02, -1.01110990e-02,
        9.72775566e-04, -3.62152568e-03,  1.13234551e-01,  9.72775566e-04,
        1.07428092e-02, -4.32377418e-03, -3.62152568e-03, -4.52593492e-03,
       -3.21530249e-02, -1.57577077e-03, -1.79854342e-03,  2.34121076e-02,
       -2.00407363e-02, -1.14431035e-01, -2.97785076e-03, -3.09242832e-02,
       -2.90452177e-02, -3.51528640e-03,  2.41287632e-02, -2.97785076e-03,
       -1.28456519e-02, -1.28456519e-02, -1.05004829e-02,  3.27172788e-02,
        2.51822711e-02, -3.51528640e-03, -1.30176827e-02,  2.09210342e-02,
       -1.57577077e-03, -4.47898244e-03, -3.73102513e-03,  2.09210342e-02,
       -2.83956493e-02, -3.62152568e-03,  4.55029421e-02, -1.30176827e-02,
       -4.32377418e-03,  4.19137803e-03, -5.65345202e-02,  4.37895493e-02,
       -1.79854342e-03, -3.21530249e-02,  2.25127217e-01, -1.66832609e-03,
        7.77638944e-02, -6.78202906e-03, -3.09242832e-02, -2.49270178e-02,
        3.27172788e-02, -1.34195875e-02,  1.19549491e-01, -6.78202906e-03,
        1.13234551e-01, -2.90504179e-03, -2.63506786e-02, -3.62152568e-03,
        7.76369845e-02, -1.02802319e-02, -1.02802319e-02, -2.90452177e-02,
       -2.00407363e-02, -1.79854342e-03,  9.41740462e-02, -1.79854342e-03,
       -2.60795846e-02, -1.69221667e-02,  4.37895493e-02, -2.82531443e-03,
        1.07428092e-02, -2.49270178e-02,  7.76369845e-02,  2.01169128e-02,
       -1.79854342e-03,  1.24552134e-01, -4.72651679e-03,  3.44687563e-02,
       -1.57577077e-03, -1.48123256e-02,  4.13292697e-02, -1.05004829e-02,
       -1.01110990e-02,  2.51822711e-02, -2.97785076e-03, -5.44451614e-03,
       -1.72792648e-02, -1.05004829e-02, -2.97785076e-03, -6.35289199e-03,
       -3.21530249e-02, -4.28497109e-03, -2.02160742e-04, -2.36517421e-02,
       -7.28807818e-03, -1.72115768e-02,  1.07428092e-02, -6.04108571e-02,
       -4.49566214e-01, -8.84231612e-02, -5.70489919e-02, -1.40157693e-02,
       -1.02802319e-02, -2.91910467e-03, -2.97785076e-03,  7.76369845e-02,
       -3.20318130e-02, -2.00407363e-02, -1.72792648e-02, -7.67680141e-03,
        4.13292697e-02, -2.72249187e-02, -2.90504179e-03, -1.01110990e-02,
       -2.90504179e-03, -1.43265097e-01, -2.48897370e-02, -3.51528640e-03,
       -6.20049436e-02, -2.72249187e-02,  6.02655939e-02, -1.96760241e-02,
       -1.02802319e-02, -2.90452177e-02, -3.35880336e-02,  7.76369845e-02,
       -1.01110990e-02,  1.07428092e-02, -3.09242832e-02, -2.53291256e-03,
        1.13234551e-01, -5.44451614e-03,  1.17422184e-01, -2.81388709e-02,
       -1.39771716e-02, -3.31757155e-02,  2.36065003e-02,  1.07428092e-02,
        2.78510719e-02, -1.72792648e-02, -2.38172414e-02, -7.28807818e-03,
       -2.83956493e-02,  7.06792087e-03, -1.01110990e-02, -2.83956493e-02,
       -5.93525129e-02,  4.55029421e-02, -1.01110990e-02, -4.48713183e-03,
        4.37895493e-02, -3.09242832e-02, -1.42476714e-01, -2.81388709e-02,
       -1.01627312e-02,  1.13234551e-01, -1.66832609e-03, -3.62152568e-03,
        1.08254671e-02,  2.51822711e-02, -2.72249187e-02,  3.34891335e-02,
       -1.28456519e-02]), array([-0.01311521, -0.04728254,  0.03294383, -0.0095721 , -0.00243526,
       -0.0236934 ,  0.04079417, -0.00522851,  0.01356804, -0.00637767,
        0.00062687,  0.01881165, -0.01311521,  0.15316744, -0.02738857,
       -0.00940345,  0.16473825,  0.05031554,  0.00117987, -0.01311521,
       -0.01399272, -0.02082809, -0.01684545, -0.02537122,  0.00239894,
       -0.0095721 , -0.00197617, -0.00963585,  0.01157081,  0.04582654,
       -0.01399919,  0.55101197, -0.00197617, -0.00409858,  0.00062687,
       -0.21556708, -0.02777016, -0.0226061 ,  0.02585112, -0.02215873,
       -0.01201086, -0.04728254, -0.0197162 , -0.08816454,  0.01157081,
       -0.08409114,  0.01484328, -0.01311521, -0.0226061 , -0.00197617,
        0.04079417, -0.00963585, -0.00874678, -0.01030264, -0.00762762,
        0.0102732 ,  0.01484328,  0.00239894,  0.07922331,  0.01484328,
        0.02665832,  0.00180822, -0.02168273, -0.01201086, -0.02868129,
       -0.04859196, -0.02215873,  0.01356804,  0.00184778,  0.01157081,
       -0.04728254,  0.00184778, -0.00522851,  0.03294383,  0.01157081,
        0.11216714, -0.02868129,  0.01152858,  0.00117987, -0.01204688,
       -0.02004982, -0.11829759,  0.00062687, -0.02088403, -0.02450991,
        0.004489  ,  0.00155718,  0.00062687, -0.00762762, -0.00762762,
        0.02665832, -0.01054909, -0.00243526,  0.004489  , -0.01751918,
       -0.0095721 ,  0.01152858,  0.02990091, -0.00197617, -0.0095721 ,
       -0.02777016,  0.01157081, -0.0197162 , -0.01751918,  0.03294383,
       -0.01845279, -0.05610062, -0.0236934 ,  0.00117987, -0.02868129,
       -0.08316762,  0.01484328,  0.07566459,  0.04079417, -0.02088403,
       -0.0226061 , -0.01054909,  0.02846654, -0.03966873,  0.04079417,
       -0.04728254,  0.00124863, -0.01494305,  0.01157081, -0.02738857,
       -0.00637767, -0.00637767, -0.02450991, -0.02004982,  0.00117987,
       -0.06247248,  0.00117987, -0.01684545,  0.20895898, -0.0236934 ,
        0.03675309, -0.00522851, -0.0226061 , -0.02738857, -0.0091456 ,
        0.00117987,  0.13585622,  0.03418942, -0.00963585,  0.01152858,
        0.08566223, -0.00940345,  0.02665832,  0.01356804, -0.00243526,
        0.00062687,  0.03403347, -0.02098118,  0.02665832,  0.00062687,
       -0.00409858, -0.02868129,  0.03747323,  0.07922331,  0.20601009,
        0.04582654,  0.10976092, -0.00522851, -0.02533252, -0.37676398,
       -0.07727325, -0.036228  ,  0.03114732, -0.00637767,  0.00180822,
        0.00062687, -0.02738857, -0.02537122, -0.02004982, -0.02098118,
        0.16301373, -0.00940345, -0.02408205,  0.00124863,  0.01356804,
        0.00124863,  0.00523079, -0.01030264,  0.004489  , -0.06558302,
       -0.02408205, -0.01834371,  0.05871561, -0.00637767, -0.02450991,
       -0.01008339, -0.02738857,  0.01356804, -0.00522851, -0.02088403,
        0.24591995, -0.04728254,  0.03403347,  0.02815653, -0.02833046,
       -0.00762152, -0.01934772,  0.00909333, -0.00522851, -0.04343219,
       -0.02098118, -0.0666497 ,  0.04582654, -0.02777016,  0.06609419,
        0.01356804, -0.02777016, -0.04483321, -0.0197162 ,  0.01356804,
        0.11669654, -0.0236934 , -0.02088403, -0.11393519, -0.02833046,
        0.01521837, -0.04728254,  0.01484328,  0.01157081,  0.13854273,
       -0.00243526, -0.02408205,  0.10053414, -0.00762762]), array([-0.02880197,  0.2149866 ,  0.00526456, -0.00119456,  0.01257774,
       -0.01030818,  0.02726558, -0.0295833 ,  0.03998273, -0.0137675 ,
       -0.00175329,  0.26987663, -0.02880197, -0.0449823 ,  0.02495094,
       -0.04202693, -0.05372848,  0.00288625,  0.00172599, -0.02880197,
        0.01287813, -0.01231568,  0.0234736 ,  0.022233  ,  0.00151688,
       -0.00119456, -0.04753529, -0.05150555, -0.00874618,  0.00120351,
       -0.05845784,  0.09554505, -0.04753529, -0.03699015, -0.00175329,
       -0.00094906,  0.02071613,  0.01129988,  0.05268793,  0.01796298,
       -0.06443468,  0.2149866 , -0.04627141, -0.06134516, -0.00874618,
        0.17587645, -0.05740803, -0.02880197,  0.01129988, -0.04753529,
        0.02726558, -0.05150555, -0.22031277, -0.0115396 , -0.03262331,
        0.03300929, -0.05740803,  0.00151688, -0.01665174, -0.05740803,
        0.0330563 ,  0.00133104, -0.06329503, -0.06443468,  0.00454958,
        0.00391213,  0.01796298,  0.03998273, -0.0467932 , -0.00874618,
        0.2149866 , -0.0467932 , -0.0295833 ,  0.00526456, -0.00874618,
       -0.01138718,  0.00454958,  0.01870556,  0.00172599, -0.01178948,
        0.00203698,  0.03490572, -0.00175329,  0.02999115, -0.00652317,
        0.00168273, -0.02445195, -0.00175329, -0.03262331, -0.03262331,
        0.0330563 ,  0.00517786,  0.01257774,  0.00168273, -0.00974789,
       -0.00119456,  0.01870556,  0.00622166, -0.04753529, -0.00119456,
        0.02071613, -0.00874618, -0.04627141, -0.00974789,  0.00526456,
       -0.00484487,  0.03778767, -0.01030818,  0.00172599,  0.00454958,
       -0.22142647, -0.05740803,  0.04913943,  0.02726558,  0.02999115,
        0.01129988,  0.00517786,  0.03438734, -0.08312048,  0.02726558,
        0.2149866 , -0.00191753,  0.02542235, -0.00874618,  0.02495094,
       -0.0137675 , -0.0137675 , -0.00652317,  0.00203698,  0.00172599,
       -0.26766032,  0.00172599,  0.0234736 , -0.00163187, -0.01030818,
       -0.01140257, -0.0295833 ,  0.01129988,  0.02495094, -0.00111487,
        0.00172599, -0.22472479,  0.01829475, -0.05150555,  0.01870556,
        0.03654646, -0.04202693,  0.0330563 ,  0.03998273,  0.01257774,
       -0.00175329,  0.00606496, -0.01112019,  0.0330563 , -0.00175329,
       -0.03699015,  0.00454958, -0.00261807, -0.01665174,  0.1944816 ,
        0.00120351,  0.01349013, -0.0295833 , -0.18402751,  0.13895879,
        0.00846171, -0.05785914,  0.03473904, -0.0137675 ,  0.00133104,
       -0.00175329,  0.02495094,  0.022233  ,  0.00203698, -0.01112019,
       -0.09975924, -0.04202693,  0.0104353 , -0.00191753,  0.03998273,
       -0.00191753,  0.20523206, -0.0115396 ,  0.00168273, -0.01708125,
        0.0104353 , -0.05838527, -0.00841402, -0.0137675 , -0.00652317,
       -0.10671386,  0.02495094,  0.03998273, -0.0295833 ,  0.02999115,
        0.04296404,  0.2149866 ,  0.00606496,  0.08501883,  0.01707154,
       -0.04469034, -0.11430381,  0.0312833 , -0.0295833 , -0.09673132,
       -0.01112019,  0.04296553,  0.00120351,  0.02071613,  0.05489003,
        0.03998273,  0.02071613,  0.0312069 , -0.04627141,  0.03998273,
       -0.01926981, -0.01030818,  0.02999115, -0.07710074,  0.01707154,
       -0.00914683,  0.2149866 , -0.05740803, -0.00874618,  0.10324446,
        0.01257774,  0.0104353 , -0.15096102, -0.03262331]), array([ 3.17478125e-02,  1.90180065e-01,  6.75790870e-03, -7.70167559e-03,
       -3.34799097e-02,  9.23461064e-03, -1.51344333e-02,  2.56907749e-03,
       -4.76833474e-02,  2.23464778e-02,  1.07101056e-03,  1.56736328e-01,
        3.17478125e-02,  9.70365835e-02, -1.55539683e-01, -1.59686910e-03,
        1.10680301e-01, -2.74100373e-02, -5.93843444e-03,  3.17478125e-02,
       -8.04470574e-02,  3.17362495e-02, -1.16113204e-02, -3.92844008e-02,
       -2.33361197e-02, -7.70167559e-03, -1.64157107e-02,  2.32542820e-03,
        1.36437173e-02, -2.22725178e-02,  6.20933660e-02,  1.63828799e-02,
       -1.64157107e-02,  4.11884747e-02,  1.07101056e-03, -1.08903820e-01,
       -1.59482811e-02,  2.18446224e-03, -2.79077609e-02,  1.60610744e-04,
       -2.07853605e-02,  1.90180065e-01, -4.08960126e-02, -9.32019606e-03,
        1.36437173e-02,  2.31162488e-01,  7.50442428e-02,  3.17478125e-02,
        2.18446224e-03, -1.64157107e-02, -1.51344333e-02,  2.32542820e-03,
       -3.06583059e-02, -5.06935065e-03,  4.77115571e-02, -6.21849444e-02,
        7.50442428e-02, -2.33361197e-02,  4.95247262e-02,  7.50442428e-02,
       -3.78221776e-02, -2.43072426e-03, -3.98327448e-03, -2.07853605e-02,
        1.62662857e-02,  2.12970509e-03,  1.60610744e-04, -4.76833474e-02,
       -5.65226849e-03,  1.36437173e-02,  1.90180065e-01, -5.65226849e-03,
        2.56907749e-03,  6.75790870e-03,  1.36437173e-02,  5.62826349e-02,
        1.62662857e-02, -2.27666002e-02, -5.93843444e-03, -6.30870269e-03,
        1.94811152e-03,  2.08436271e-02,  1.07101056e-03, -1.41264000e-02,
       -2.09985918e-04, -5.13751942e-03, -6.84687079e-02,  1.07101056e-03,
        4.77115571e-02,  4.77115571e-02, -3.78221776e-02, -6.12293342e-02,
       -3.34799097e-02, -5.13751942e-03,  1.74685587e-02, -7.70167559e-03,
       -2.27666002e-02, -1.52309043e-04, -1.64157107e-02, -7.70167559e-03,
       -1.59482811e-02,  1.36437173e-02, -4.08960126e-02,  1.74685587e-02,
        6.75790870e-03, -8.96348045e-02, -2.18147844e-02,  9.23461064e-03,
       -5.93843444e-03,  1.62662857e-02, -1.97267415e-02,  7.50442428e-02,
       -1.70551127e-01, -1.51344333e-02, -1.41264000e-02,  2.18446224e-03,
       -6.12293342e-02, -4.02529019e-02, -1.03722216e-01, -1.51344333e-02,
        1.90180065e-01, -9.96152843e-05, -1.27733276e-02,  1.36437173e-02,
       -1.55539683e-01,  2.23464778e-02,  2.23464778e-02, -2.09985918e-04,
        1.94811152e-03, -5.93843444e-03, -8.81196484e-02, -5.93843444e-03,
       -1.16113204e-02,  5.85234956e-02,  9.23461064e-03,  1.54921804e-02,
        2.56907749e-03,  2.18446224e-03, -1.55539683e-01, -7.15251256e-03,
       -5.93843444e-03,  2.04557198e-01, -9.90703529e-03,  2.32542820e-03,
       -2.27666002e-02, -5.72545609e-02, -1.59686910e-03, -3.78221776e-02,
       -4.76833474e-02, -3.34799097e-02,  1.07101056e-03, -4.68829908e-03,
       -6.69572433e-03, -3.78221776e-02,  1.07101056e-03,  4.11884747e-02,
        1.62662857e-02,  1.76765822e-02,  4.95247262e-02, -1.63917197e-01,
       -2.22725178e-02, -2.71131260e-02,  2.56907749e-03,  2.35427412e-01,
       -9.86547071e-04,  1.83959908e-02, -1.18632353e-02, -4.29596971e-02,
        2.23464778e-02, -2.43072426e-03,  1.07101056e-03, -1.55539683e-01,
       -3.92844008e-02,  1.94811152e-03, -6.69572433e-03,  1.55724863e-01,
       -1.59686910e-03,  2.33969101e-03, -9.96152843e-05, -4.76833474e-02,
       -9.96152843e-05, -1.58838005e-01, -5.06935065e-03, -5.13751942e-03,
        5.40774554e-02,  2.33969101e-03,  3.43168900e-02, -5.89261874e-02,
        2.23464778e-02, -2.09985918e-04,  1.47968967e-01, -1.55539683e-01,
       -4.76833474e-02,  2.56907749e-03, -1.41264000e-02,  2.57706686e-02,
        1.90180065e-01, -4.68829908e-03, -2.50212755e-01, -5.86650330e-03,
        3.97468882e-02,  1.28646920e-01, -5.62465099e-02,  2.56907749e-03,
        1.58171920e-02, -6.69572433e-03, -8.30441186e-02, -2.22725178e-02,
       -1.59482811e-02, -3.34437372e-02, -4.76833474e-02, -1.59482811e-02,
       -4.50457391e-03, -4.08960126e-02, -4.76833474e-02,  6.72013084e-02,
        9.23461064e-03, -1.41264000e-02,  1.53322204e-01, -5.86650330e-03,
       -9.79201773e-03,  1.90180065e-01,  7.50442428e-02,  1.36437173e-02,
       -1.50354462e-01, -3.34799097e-02,  2.33969101e-03,  3.65664087e-02,
        4.77115571e-02]), array([-0.04318622, -0.00383472, -0.00411032,  0.00692766,  0.03420987,
       -0.02594373,  0.01198526, -0.01080787,  0.03597422,  0.0593005 ,
        0.02227203, -0.01550959, -0.04318622, -0.1455417 ,  0.07269142,
       -0.03625067, -0.13385615,  0.03668613,  0.03238576, -0.04318622,
        0.03767506,  0.01647671, -0.01829948, -0.01757583,  0.02677671,
        0.00692766, -0.00547626, -0.01583853,  0.01168555,  0.01524989,
        0.14032336, -0.1232895 , -0.00547626,  0.07738861,  0.02227203,
       -0.07538756, -0.04435254, -0.02780832, -0.00532944,  0.00173168,
       -0.03399083, -0.00383472, -0.04383194, -0.08119601,  0.01168555,
       -0.07296467,  0.03825143, -0.04318622, -0.02780832, -0.00547626,
        0.01198526, -0.01583853, -0.03797385, -0.04575693,  0.09485894,
        0.10490564,  0.03825143,  0.02677671, -0.07821454,  0.03825143,
        0.02162958,  0.01623301, -0.00291856, -0.03399083, -0.00599612,
       -0.07060908,  0.00173168,  0.03597422, -0.0120183 ,  0.01168555,
       -0.00383472, -0.0120183 , -0.01080787, -0.00411032,  0.01168555,
       -0.08232486, -0.00599612,  0.03831002,  0.03238576,  0.01291997,
       -0.01247924, -0.11027343,  0.02227203, -0.02482746, -0.03700429,
        0.02143624,  0.00198339,  0.02227203,  0.09485894,  0.09485894,
        0.02162958, -0.01258916,  0.03420987,  0.02143624,  0.01783627,
        0.00692766,  0.03831002,  0.00483167, -0.00547626,  0.00692766,
       -0.04435254,  0.01168555, -0.04383194,  0.01783627, -0.00411032,
       -0.03156938, -0.08758898, -0.02594373,  0.03238576, -0.00599612,
       -0.19401126,  0.03825143, -0.06153646,  0.01198526, -0.02482746,
       -0.02780832, -0.01258916,  0.03786259, -0.09267177,  0.01198526,
       -0.00383472,  0.01948416, -0.0173147 ,  0.01168555,  0.07269142,
        0.0593005 ,  0.0593005 , -0.03700429, -0.01247924,  0.03238576,
       -0.1445194 ,  0.03238576, -0.01829948, -0.20297349, -0.02594373,
       -0.01013873, -0.01080787, -0.02780832,  0.07269142,  0.00640166,
        0.03238576, -0.20256569,  0.00965987, -0.01583853,  0.03831002,
        0.0218094 , -0.03625067,  0.02162958,  0.03597422,  0.03420987,
        0.02227203,  0.00714282, -0.03336354,  0.02162958,  0.02227203,
        0.07738861, -0.00599612, -0.00725291, -0.07821454,  0.09081401,
        0.01524989,  0.02722438, -0.01080787,  0.30799856, -0.38786641,
       -0.0766052 , -0.05619921,  0.04306582,  0.0593005 ,  0.01623301,
        0.02227203,  0.07269142, -0.01757583, -0.01247924, -0.03336354,
       -0.04646737, -0.03625067, -0.03360479,  0.01948416,  0.03597422,
        0.01948416,  0.00545336, -0.04575693,  0.02143624,  0.00505637,
       -0.03360479, -0.05399409,  0.02329614,  0.0593005 , -0.03700429,
        0.13211675,  0.07269142,  0.03597422, -0.01080787, -0.02482746,
       -0.13558698, -0.00383472,  0.00714282,  0.04842739, -0.04323645,
        0.08102286,  0.25327041,  0.07251988, -0.01080787,  0.08876751,
       -0.03336354, -0.10017814,  0.01524989, -0.04435254, -0.01167486,
        0.03597422, -0.04435254, -0.02979348, -0.04383194,  0.03597422,
       -0.08546745, -0.02594373, -0.02482746,  0.24378813, -0.04323645,
        0.05794643, -0.00383472,  0.03825143,  0.01168555,  0.08619119,
        0.03420987, -0.03360479, -0.12347501,  0.09485894]), array([-0.00529318,  0.06944966, -0.00553486, -0.00597442,  0.00639265,
        0.01473813,  0.01171827,  0.01779443,  0.05044326,  0.00306919,
        0.0007699 ,  0.05727946, -0.00529318, -0.10822065, -0.18006795,
       -0.01548464, -0.13259372,  0.08403587,  0.00358575, -0.00529318,
       -0.093399  , -0.02948503,  0.00091499, -0.05289914, -0.01274307,
       -0.00597442,  0.09737225, -0.00710491, -0.02437307,  0.07101417,
        0.0267797 ,  0.00331647,  0.09737225, -0.02493925,  0.0007699 ,
       -0.07625897, -0.04015607, -0.017025  ,  0.01361982,  0.00050861,
        0.08066322,  0.06944966,  0.02996856,  0.01526463, -0.02437307,
        0.07889461, -0.05900095, -0.00529318, -0.017025  ,  0.09737225,
        0.01171827, -0.00710491,  0.39200684,  0.02385956, -0.01148465,
        0.02295292, -0.05900095, -0.01274307, -0.06328466, -0.05900095,
        0.03929607,  0.00409925, -0.019923  ,  0.08066322, -0.00988156,
        0.02325346,  0.00050861,  0.05044326,  0.06574192, -0.02437307,
        0.06944966,  0.06574192,  0.01779443, -0.00553486, -0.02437307,
       -0.06881952, -0.00988156,  0.01297453,  0.00358575, -0.01281809,
       -0.01657834, -0.09606742,  0.0007699 ,  0.00133404,  0.0296452 ,
        0.01302169,  0.13110649,  0.0007699 , -0.01148465, -0.01148465,
        0.03929607, -0.03924308,  0.00639265,  0.01302169, -0.01172441,
       -0.00597442,  0.01297453,  0.00862065,  0.09737225, -0.00597442,
       -0.04015607, -0.02437307,  0.02996856, -0.01172441, -0.00553486,
       -0.02812659, -0.07273143,  0.01473813,  0.00358575, -0.00988156,
        0.12238651, -0.05900095,  0.00117912,  0.01171827,  0.00133404,
       -0.017025  , -0.03924308,  0.04339532, -0.02475916,  0.01171827,
        0.06944966, -0.00322141,  0.00190155, -0.02437307, -0.18006795,
        0.00306919,  0.00306919,  0.0296452 , -0.01657834,  0.00358575,
        0.28288807,  0.00358575,  0.00091499, -0.09653128,  0.01473813,
       -0.00078221,  0.01779443, -0.017025  , -0.18006795, -0.00551049,
        0.00358575, -0.19510433,  0.00793081, -0.00710491,  0.01297453,
        0.09363801, -0.01548464,  0.03929607,  0.05044326,  0.00639265,
        0.0007699 ,  0.02276946,  0.01624704,  0.03929607,  0.0007699 ,
       -0.02493925, -0.00988156, -0.01465281, -0.06328466,  0.09352051,
        0.07101417,  0.10240429,  0.01779443, -0.10119937, -0.11144943,
        0.0133719 ,  0.12220712,  0.05231776,  0.00306919,  0.00409925,
        0.0007699 , -0.18006795, -0.05289914, -0.01657834,  0.01624704,
       -0.11937196, -0.01548464, -0.00639174, -0.00322141,  0.05044326,
       -0.00322141,  0.11702237,  0.02385956,  0.01302169, -0.05892709,
       -0.00639174,  0.01250124,  0.15943778,  0.00306919,  0.0296452 ,
       -0.11342523, -0.18006795,  0.05044326,  0.01779443,  0.00133404,
       -0.08109478,  0.06944966,  0.02276946, -0.23148123, -0.03257536,
        0.02371051, -0.01271339,  0.01936718,  0.01779443, -0.07418366,
        0.01624704, -0.11197451,  0.07101417, -0.04015607, -0.0121702 ,
        0.05044326, -0.04015607,  0.00230936,  0.02996856,  0.05044326,
       -0.07793746,  0.01473813,  0.00133404, -0.004827  , -0.03257536,
       -0.04033756,  0.06944966, -0.05900095, -0.02437307,  0.09178962,
        0.00639265, -0.00639174,  0.10068694, -0.01148465])])
        whiten = False
        explained_variance = np.array([0.42884537957348373, 0.3721189228871174, 0.2538304037085043, 0.23848235716071067, 0.22492764966804693, 0.22028441755445824])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist=[]
    clean.testfile=testfile
    clean.mapping={}
    clean.mapping={'1':0,'2':1}

    def convert(cell):
        value=str(cell)
        try:
            result=int(value)
            return result
        except:
            try:
                result=float(value)
                if (rounding!=-1):
                    result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
                return result
            except:
                result=(binascii.crc32(value.encode('utf8')) % (1<<32))
                return result

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value=str(cell)
        if (value==''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping=={}):
            result=-1
            try:
                result=clean.mapping[cell]
            except:
                raise ValueError("Class label '"+value+"' encountered in input not defined in user-provided mapping.")
            if (not result==int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist=clean.classlist+[str(result)]
            return result
        try:
            result=float(cell)
            if (rounding!=-1):
                result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
            else:
                result=int(int(result*100)/100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist=clean.classlist+[str(result)]
        except:
            result=(binascii.crc32(value.encode('utf8')) % (1<<32))
            if (result in clean.classlist):
                result=clean.classlist.index(result)
            else:
                clean.classlist=clean.classlist+[result]
                result=clean.classlist.index(result)
            if (not result==int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result<0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount=0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f=open(outfile,"w+")
        if (headerless==False):
            next(reader,None)
        outbuf=[]
        for row in reader:
            if (row==[]):  # Skip empty rows
                continue
            rowcount=rowcount+1
            rowlen=num_attr
            if (not testfile):
                rowlen=rowlen+1    
            if (not len(row)==rowlen):
                raise ValueError("Column count must match trained predictor. Row "+str(rowcount)+" differs.")
            i=0
            for elem in row:
                if(i+1<len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid=str(convertclassid(elem))
                    outbuf.append(classid)
                i=i+1
            if (len(outbuf)<IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf=[]
        print(''.join(outbuf),end="", file=f)
        f.close()

        if (testfile==False and not len(clean.classlist)>=2):
            raise ValueError("Number of classes must be at least 2.")



# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)

# Classifier
def classify(row):
    x=row
    o=[0]*num_output_logits
    h_0 = max((((2.931374 * float(x[0]))+ (0.36392817 * float(x[1]))+ (4.350791 * float(x[2]))+ (2.5758283 * float(x[3]))+ (1.4469895 * float(x[4]))+ (0.51504785 * float(x[5]))) + 0.80543554), 0)
    h_1 = max((((0.011981957 * float(x[0]))+ (-0.43376014 * float(x[1]))+ (-0.96925235 * float(x[2]))+ (0.6381285 * float(x[3]))+ (0.29648706 * float(x[4]))+ (1.0051647 * float(x[5]))) + -2.1178916), 0)
    h_2 = max((((3.016368 * float(x[0]))+ (2.3327935 * float(x[1]))+ (0.4515049 * float(x[2]))+ (2.6755962 * float(x[3]))+ (-4.811909 * float(x[4]))+ (0.37899542 * float(x[5]))) + -0.3594816), 0)
    o[0] = (3.748008 * h_0)+ (-0.7358638 * h_1)+ (29.320675 * h_2) + -6.223197

    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)

# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()
    
    if not args.validate: # Then predict
        if not args.cleanfile: # File is not preprocessed
            tempdir=tempfile.gettempdir()
            cleanfile=tempdir+os.sep+"clean.csv"
            clean(args.csvfile,cleanfile, -1, args.headerless, True)
            test_tensor = np.loadtxt(cleanfile,delimiter=',',dtype='float64')
            os.remove(cleanfile)
        else: # File is already preprocessed
            test_tensor = np.loadtxt(args.File,delimiter = ',',dtype = 'float64')               
        test_tensor = Normalize(test_tensor)
        if transform_true:
            test_tensor = transform(test_tensor)
        with open(args.csvfile,'r') as csvinput:
            writer = csv.writer(sys.stdout, lineterminator='\n')
            reader = csv.reader(csvinput)
            if (not args.headerless):
                writer.writerow((next(reader, None)+['Prediction']))
            i=0
            for row in reader:
                if (classify(test_tensor[i])):
                    pred="1"
                else:
                    pred="0"
                row.append(pred)
                writer.writerow(row)
                i=i+1
    elif args.validate: # Then validate this predictor, always clean first.
        if n_classes==2:
            tempdir=tempfile.gettempdir()
            temp_name = next(tempfile._get_candidate_names())
            cleanfile=tempdir+os.sep+temp_name
            clean(args.csvfile,cleanfile, -1, args.headerless)
            val_tensor = np.loadtxt(cleanfile,delimiter = ',',dtype = 'float64')
            os.remove(cleanfile)
            val_tensor = Normalize(val_tensor)
            if transform_true:
                trans = transform(val_tensor[:,:-1])
                val_tensor = np.concatenate((trans,val_tensor[:,-1].reshape(-1,1)),axis = 1)
            count,correct_count,num_TP,num_TN,num_FP,num_FN,num_class_1,num_class_0 = 0,0,0,0,0,0,0,0
            for i,row in enumerate(val_tensor):
                if int(classify(val_tensor[i].tolist())) == int(float(val_tensor[i,-1])):
                    correct_count+=1
                    if int(float(row[-1]))==1:
                        num_class_1+=1
                        num_TP+=1
                    else:
                        num_class_0+=1
                        num_TN+=1
                else:
                    if int(float(row[-1]))==1:
                        num_class_1+=1
                        num_FN+=1
                    else:
                        num_class_0+=1
                        num_FP+=1
                count+=1
        else:
            tempdir=tempfile.gettempdir()
            temp_name = next(tempfile._get_candidate_names())
            cleanvalfile=tempdir+os.sep+temp_name
            clean(args.csvfile,cleanvalfile, -1, args.headerless)
            val_tensor = np.loadtxt(cleanfile,delimiter = ',',dtype = 'float64')
            os.remove(cleanfile)
            val_tensor = Normalize(val_tensor)
            if transform_true:
                trans = transform(val_tensor[:,:-1])
                val_tensor = np.concatenate((trans,val_tensor[:,-1].reshape(-1,1)),axis = 1)
            numeachclass={}
            count,correct_count = 0,0
            for i,row in enumerate(val_tensor):
                if int(classify(val_tensor[i].tolist())) == int(float(val_tensor[i,-1])):
                    correct_count+=1
                    if int(float(val_tensor[i,-1])) in numeachclass.keys():
                        numeachclass[int(float(val_tensor[i,-1]))]+=1
                    else:
                        numeachclass[int(float(val_tensor[i,-1]))]=0
                count+=1

        model_cap=25

        if n_classes==2:

            FN=float(num_FN)*100.0/float(count)
            FP=float(num_FP)*100.0/float(count)
            TN=float(num_TN)*100.0/float(count)
            TP=float(num_TP)*100.0/float(count)
            num_correct=correct_count

            if int(num_TP+num_FN)!=0:
                TPR=num_TP/(num_TP+num_FN) # Sensitivity, Recall
            if int(num_TN+num_FP)!=0:
                TNR=num_TN/(num_TN+num_FP) # Specificity, 
            if int(num_TP+num_FP)!=0:
                PPV=num_TP/(num_TP+num_FP) # Recall
            if int(num_FN+num_TP)!=0:
                FNR=num_FN/(num_FN+num_TP) # Miss rate
            if int(2*num_TP+num_FP+num_FN)!=0:
                FONE=2*num_TP/(2*num_TP+num_FP+num_FN) # F1 Score
            if int(num_TP+num_FN+num_FP)!=0:
                TS=num_TP/(num_TP+num_FN+num_FP) # Critical Success Index

            randguess=int(float(10000.0*max(num_class_1,num_class_0))/count)/100.0
            modelacc=int(float(num_correct*10000)/count)/100.0

            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc)+" ("+str(int(num_correct))+"/"+str(count)+" correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc-randguess)+" (of possible "+str(round(100-randguess,2))+"%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct*100)/model_cap)/100.0)+" bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100*(modelacc-randguess)/model_cap)/100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN)+" ("+str(int(num_TN))+"/"+str(count)+")")
            print("True Positives:                     {:.2f}%".format(TP)+" ("+str(int(num_TP))+"/"+str(count)+")")
            print("False Negatives:                    {:.2f}%".format(FN)+" ("+str(int(num_FN))+"/"+str(count)+")")
            print("False Positives:                    {:.2f}%".format(FP)+" ("+str(int(num_FP))+"/"+str(count)+")")
            if int(num_TP+num_FN)!=0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN+num_FP)!=0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP+num_FP)!=0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2*num_TP+num_FP+num_FN)!=0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP+num_FN)!=0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP+num_FN+num_FP)!=0:    
                print("Critical Success Index:             {:.2f}".format(TS))
        else:
            num_correct=correct_count
            modelacc=int(float(num_correct*10000)/count)/100.0
            randguess=round(max(numeachclass.values())/sum(numeachclass.values())*100,2)
            print("System Type:                        "+str(n_classes)+"-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc)+" ("+str(int(num_correct))+"/"+str(count)+" correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc-randguess)+" (of possible "+str(round(100-randguess,2))+"%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct*100)/model_cap)/100.0)+" bits/bit")






