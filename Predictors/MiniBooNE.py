#!/usr/bin/env python3
#
# This code is was produced by an alpha version of Brainome Daimensions(tm) and is 
# licensed under GNU GPL v2.0 or higher. For details, please see: 
# https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
#
#
# Output of Brainome Daimensions(tm) 0.93 Table Compiler v0.94.
# Invocation: btc https://www.openml.org/data/get_csv/19335523/MiniBooNE.arff -o Predictors/MiniBooNE_NN.py -target signal -cm {'False':0,'True':1} -stopat 98.67 -f NN -e 20 --yes
# Total compiler execution time: 3:36:53.00. Finished on: Apr-22-2020 14:31:40.
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                71.93%
Model accuracy:                     89.61% (116561/130064 correct)
Improvement over best guess:        17.68% (of possible 28.07%)
Model capacity (MEC):               131 bits
Generalization ratio:               889.77 bits/bit
Model efficiency:                   0.13%/parameter
System behavior
True Negatives:                     67.43% (87697/130064)
True Positives:                     22.19% (28864/130064)
False Negatives:                    5.87% (7635/130064)
False Positives:                    4.51% (5868/130064)
True Pos. Rate/Sensitivity/Recall:  0.79
True Neg. Rate/Specificity:         0.94
Precision:                          0.83
F-1 Measure:                        0.81
False Negative Rate/Miss Rate:      0.21
Critical Success Index:             0.68

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "MiniBooNE.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 50
n_classes = 2

mappings = []
list_of_cols_to_normalize = []

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values()))+1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize,mappings):
            if i>=data_arr.shape[1]:
                break
            col = data_arr[:,i]
            normcol = column_norm(col,mapping)
            data_arr[:,i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([1.1119569379690044, -2.241552272445871, 123.39657816124931, -3.450618896404792, -3.723648279282384, -3.5731307974748594, -2.767627263227296, -2.912129280191315, -0.29486972967154845, -3.559654220288467, 0.5701296793886083, 161.29359842641472, -3.2849369238495516, -3.425262207448607, -1.4943033385256388, 929.3226198333048, -3.2812602079861035, 23.215322943950714, -3.4654911272235305, -2.949366158054007, -3.8282075208101003, -3.4252676383134744, 79.81271609436924, 0.10736570472997946, -3.294064465275233, -4.32079080306851, 97.12331313535817, -2.1721502549052825, -3.5596629207970842, -4.040277236303063, 3.8118779328638146, -2.721858131219051, -2.5189482304622453, 361.5327796926138, -3.660164296470284, -3.438338167512925, -3.3437841789037583, -6.500855588779762, -2.4575161232008975, -3.6418956793994894, 140.92904862836727, -25.37450313094079, -3.0051622519650483, 2.2414025955221986, -3.7240049124493204, -3.5548630276947613, -3.884774926605594, -0.8841499442639013, -2.122381604873656, -3.5029392075460133])
        components = np.array([array([0.02945784, 0.0293056 , 0.12384778, 0.03007288, 0.03009024,
       0.03014294, 0.0302454 , 0.03015783, 0.03005461, 0.03009714,
       0.03015292, 0.02743784, 0.03037623, 0.03009742, 0.03099382,
       0.95964456, 0.03065065, 0.03594153, 0.03007878, 0.03010294,
       0.0302265 , 0.03010909, 0.13677324, 0.03018023, 0.0301185 ,
       0.03039777, 0.03366859, 0.03002296, 0.03008215, 0.03027992,
       0.02965452, 0.03021676, 0.02983765, 0.0343046 , 0.03010092,
       0.03008254, 0.03040162, 0.02944362, 0.03023581, 0.03008234,
       0.04618488, 0.03960852, 0.02964749, 0.0291279 , 0.03007919,
       0.03008337, 0.03007291, 0.02929033, 0.02930429, 0.03008979]), array([-0.13788286, -0.13739021, -0.13178458, -0.13677932, -0.13672555,
       -0.13670813, -0.13677347, -0.13681209, -0.13727543, -0.13674466,
       -0.13734843, -0.16091953, -0.13662683, -0.13677246, -0.13656917,
        0.24065723, -0.13646894, -0.13969375, -0.1367699 , -0.13682708,
       -0.13658776, -0.13676   , -0.09169827, -0.13718529, -0.13677414,
       -0.13642406, -0.15845623, -0.13701227, -0.13675554, -0.13656948,
       -0.13824259, -0.13679091, -0.13709119, -0.19430124, -0.13672774,
       -0.13677013, -0.13657808, -0.13671748, -0.13684307, -0.1367441 ,
       -0.15114871, -0.12793071, -0.13706772, -0.13819314, -0.13673179,
       -0.1367601 , -0.13669204, -0.13769935, -0.13745289, -0.13675888]), array([-0.02094128, -0.02135201,  0.89829299, -0.02133303, -0.02135923,
       -0.02171584, -0.02145499, -0.02133378, -0.02153728, -0.02140029,
       -0.0216563 , -0.35899335, -0.02151028, -0.02140955, -0.02081079,
       -0.08641629, -0.02156891,  0.01373658, -0.0213758 , -0.02142233,
       -0.02151601, -0.02133357,  0.01435775, -0.02301023, -0.02138627,
       -0.02060481,  0.07285655, -0.02131133, -0.02134739, -0.02144082,
       -0.02089448, -0.0216219 , -0.02075581,  0.17249449, -0.02136581,
       -0.02142987, -0.02166346, -0.02010349, -0.02150035, -0.02133268,
       -0.04449326, -0.02388159, -0.02074467, -0.0235502 , -0.02136087,
       -0.02127703, -0.02121243, -0.02067948, -0.02075962, -0.02136909]), array([-0.00757884, -0.00872711,  0.3820621 , -0.00842832, -0.00844382,
       -0.0084957 , -0.00889766, -0.00863717, -0.00817318, -0.00846221,
       -0.00797559,  0.67601275, -0.00841183, -0.00808085, -0.010102  ,
       -0.03063774, -0.00847387, -0.03522942, -0.00847212, -0.00824306,
       -0.01037842, -0.00853733, -0.04866051, -0.0077812 , -0.00846606,
       -0.01063305,  0.00406728, -0.0081569 , -0.00851057, -0.00801562,
       -0.00472392, -0.00836845, -0.0087843 , -0.61713014, -0.00850388,
       -0.00834923, -0.0095626 , -0.00942488, -0.00816553, -0.00844638,
        0.0880836 , -0.02981748, -0.01080888, -0.00457068, -0.00843133,
       -0.00847253, -0.00929979, -0.00660447, -0.00892215, -0.0084238 ]), array([-0.02161092, -0.02651743, -0.09422674, -0.02554529, -0.02578781,
       -0.02605309, -0.02706792, -0.02589309, -0.02578734, -0.02594565,
       -0.02661497,  0.01948142, -0.02625397, -0.02544971, -0.02605726,
        0.01559201, -0.02665129,  0.11857583, -0.02589574, -0.02750371,
       -0.02888666, -0.02592013, -0.04812143, -0.03183678, -0.02598829,
       -0.02697982,  0.95917456, -0.02489789, -0.02563436, -0.02635296,
       -0.01534   , -0.02786435, -0.02272552, -0.00190662, -0.02593665,
       -0.0261171 , -0.02867122, -0.02137183, -0.02515029, -0.02570201,
        0.07754577, -0.1443121 , -0.02862072, -0.02705944, -0.0257914 ,
       -0.02540663, -0.02685952, -0.01923568, -0.02281385, -0.02590285]), array([ 0.01279351,  0.00922643, -0.06612098,  0.01122496,  0.01138014,
        0.01125192,  0.01117575,  0.01124495,  0.01014389,  0.01122044,
        0.01151732, -0.47113288,  0.01095184,  0.0116139 ,  0.01084328,
       -0.06503716,  0.0115808 , -0.14137768,  0.01114149,  0.01474305,
        0.00668629,  0.01142117,  0.49342168,  0.00701988,  0.0114897 ,
        0.01869542,  0.08699273,  0.01190782,  0.01083271,  0.01578001,
        0.01306369,  0.01386102,  0.01067613, -0.5716789 ,  0.01121956,
        0.01133373,  0.00660205,  0.00984033,  0.01163969,  0.01171845,
        0.24165993,  0.32779819,  0.00433765, -0.00868074,  0.01125965,
        0.01165797,  0.01285726,  0.01932957,  0.01616885,  0.01167611]), array([-0.03082564, -0.02959637,  0.01600742, -0.03271291, -0.03239278,
       -0.03238134, -0.03272344, -0.03255659, -0.03157991, -0.0325372 ,
       -0.03125816,  0.3595641 , -0.03358739, -0.0302631 , -0.03256791,
       -0.08551117, -0.03249231, -0.16611009, -0.03277081, -0.03101213,
       -0.04912112, -0.03258003,  0.80350399, -0.04484894, -0.03221907,
       -0.0523798 ,  0.02827845, -0.0334947 , -0.03273567, -0.03420925,
       -0.0287259 , -0.03223118, -0.02910696,  0.35579499, -0.03230944,
       -0.03249179, -0.02957687, -0.02676926, -0.03355632, -0.03194386,
       -0.12762116, -0.02322842, -0.02654587,  0.00452308, -0.03224683,
       -0.03177491, -0.03971554, -0.03571554, -0.03858361, -0.03267042]), array([ 0.02775773,  0.02496647, -0.02897883,  0.02106739,  0.02075113,
        0.02018479,  0.02065963,  0.0203616 ,  0.02304604,  0.02065977,
        0.02196975, -0.16531199,  0.01838503,  0.02295617,  0.02011308,
       -0.02287962,  0.01981341,  0.4305406 ,  0.02087952,  0.01815435,
        0.00468728,  0.02082664,  0.26292304,  0.00797005,  0.02033022,
        0.00753503, -0.1270722 ,  0.02196189,  0.02130836,  0.01635375,
        0.04276988,  0.01662569,  0.02705267, -0.23116702,  0.02077649,
        0.02064049,  0.02028388,  0.02958883,  0.02008519,  0.0210255 ,
       -0.09451453, -0.78531228,  0.02550349,  0.05645415,  0.02108555,
        0.02156514,  0.00954845,  0.02226085,  0.02073301,  0.02027781]), array([-0.03401812, -0.04535634, -0.00392534, -0.04248572, -0.04254222,
       -0.04261443, -0.04229016, -0.04269439, -0.04280067, -0.04272306,
       -0.04247161,  0.08885269, -0.0425057 , -0.04329212, -0.04204435,
       -0.02033459, -0.04310364,  0.79936416, -0.04259491, -0.04432021,
       -0.03967931, -0.04267064,  0.0811976 , -0.04048067, -0.04324739,
       -0.03733994, -0.11978922, -0.04088183, -0.04268828, -0.03982494,
       -0.03807704, -0.04393027, -0.04028827,  0.10099151, -0.04284476,
       -0.04304641, -0.04815981, -0.04103308, -0.04104407, -0.04267365,
        0.35739873,  0.34595087, -0.05553529, -0.06307857, -0.04267599,
       -0.04274848, -0.04200204, -0.02747159, -0.02688712, -0.04243646]), array([-8.82617930e-04,  1.53643494e-02, -5.39901795e-03,  1.17351003e-02,
        1.22156444e-02,  1.25678872e-02,  1.54205445e-02,  1.30406904e-02,
        1.36305556e-02,  1.25486598e-02,  1.34488457e-02, -1.19041683e-02,
        1.19987704e-02,  1.06123890e-02,  1.36429939e-02,  3.27939022e-04,
        1.35150352e-02,  3.02420963e-01,  1.32505003e-02,  1.60441293e-02,
        1.10045763e-02,  1.23856139e-02,  3.74794516e-02,  2.57507698e-02,
        1.21742772e-02,  2.87306855e-02,  9.73722327e-02,  7.07504852e-03,
        1.20198878e-02,  3.37421104e-03, -2.71296309e-02,  1.40752392e-02,
        1.07899231e-02, -1.82082174e-01,  1.28679723e-02,  1.30183437e-02,
        2.63938591e-02,  1.06840038e-02,  7.98702646e-03,  1.20618312e-02,
       -8.61928440e-01,  3.33745211e-01,  2.39152141e-02,  1.49151008e-02,
        1.18933644e-02,  1.13231118e-02,  2.70995523e-02, -2.85655828e-02,
        7.72238025e-03,  1.24533265e-02]), array([ 5.70816392e-02, -1.13232293e-03, -8.68048282e-04, -6.66862426e-03,
       -4.80362956e-03, -7.53692173e-03, -6.88776357e-03, -7.15759148e-03,
        1.95722823e-02, -5.69818392e-03,  2.38507733e-02, -2.09975172e-02,
       -3.34979456e-03,  2.47735105e-02, -1.38903151e-02,  4.67699090e-03,
       -8.34660388e-03,  4.56612097e-03, -7.01203977e-03, -4.40924665e-03,
       -2.76503572e-01, -5.88853969e-03, -3.77996084e-02, -1.01612884e-01,
       -5.01420971e-03, -3.71493967e-01,  2.64094495e-03, -1.27794198e-02,
       -6.51326808e-03, -3.57302425e-02,  1.65138483e-02, -1.78374906e-02,
        3.18629442e-02, -1.33407657e-02, -5.50558860e-03, -7.02604513e-03,
        1.79879419e-02,  3.85920718e-02, -1.00375948e-02, -2.70776969e-03,
        1.68655593e-02,  5.16597350e-02, -6.37187454e-03,  8.65852977e-01,
       -2.09069348e-03, -8.25151085e-04, -8.79601469e-02, -3.72146907e-02,
       -2.57676998e-02, -7.50981777e-03]), array([ 1.80000805e-01,  3.27021203e-02, -3.25562125e-04, -2.90188017e-02,
       -3.84264095e-02, -4.86748222e-02, -6.94629736e-02, -4.98135757e-02,
       -2.29400826e-02, -4.23589553e-02, -6.98022147e-02,  3.19981461e-03,
       -9.91315102e-02, -4.77116338e-02, -4.65373699e-02,  3.22590395e-03,
       -5.13236369e-02, -8.89747920e-03, -3.60529764e-02, -5.87302827e-02,
       -8.74118273e-03, -3.72034813e-02, -9.06128740e-03, -3.10785022e-01,
       -4.16587037e-02, -6.09064279e-02, -5.16281258e-03, -2.17370208e-03,
       -3.80994843e-02, -2.05088716e-02,  5.69147407e-01, -8.96685621e-02,
        4.63707910e-02, -9.83487802e-04, -4.38792440e-02, -4.31539861e-02,
       -1.47659087e-01,  6.63108616e-02, -3.66373751e-02, -3.25242344e-02,
       -5.29895256e-02,  2.40764695e-02,  5.52811121e-03, -6.24096901e-02,
       -3.77653180e-02, -2.40501610e-02,  7.60729549e-03,  3.88434934e-01,
        5.40169307e-01, -3.95402875e-02]), array([ 3.25166633e-02, -6.45342050e-03,  5.43202433e-04, -2.93300819e-02,
       -2.78578833e-02, -2.59875075e-02, -3.33528668e-03, -2.88390073e-02,
        2.51461392e-03, -2.37341500e-02,  6.82034489e-03,  6.67920137e-04,
       -1.33183151e-02, -3.95901779e-02, -2.04095033e-03, -7.29144554e-04,
       -2.44747243e-02, -8.49949734e-03, -1.76644935e-02, -7.94384191e-02,
       -3.66909017e-02, -3.00378240e-02,  1.12547198e-02,  5.90361555e-01,
       -3.24315990e-02,  1.61385749e-02,  3.40386360e-03, -3.29426484e-02,
       -2.22157301e-02, -2.41364446e-02, -3.70500412e-01, -3.26967301e-02,
       -1.11251803e-01,  2.42183023e-03, -2.78557901e-02, -2.38418504e-02,
       -1.46104086e-02, -1.48382923e-01, -8.53709433e-03, -3.39580042e-02,
        6.89584695e-03, -2.10295997e-02, -1.46604619e-01,  1.03226755e-01,
       -2.88763581e-02, -4.59985782e-02,  3.81136610e-02,  9.06277817e-02,
        6.40485045e-01, -2.64611592e-02]), array([ 5.75460619e-02, -5.12714249e-02, -3.82643216e-04, -1.96220808e-02,
       -2.46429057e-02, -2.37013834e-02, -2.04961409e-02, -3.17177214e-02,
       -2.13654665e-02, -2.14916953e-02, -5.11803703e-02, -4.52262571e-03,
       -4.53963890e-02, -5.45810208e-02, -4.72527853e-02, -6.65743401e-04,
       -2.94911241e-02,  5.77428743e-03, -2.28589724e-02, -3.84102598e-02,
        4.72199768e-01, -2.01283221e-02,  7.25389319e-03,  5.04157370e-01,
       -2.26123433e-02, -1.69769754e-01,  3.62865927e-03,  3.33903574e-02,
       -2.14683765e-02,  7.99554665e-02,  4.70204564e-01, -1.43348924e-02,
       -1.97456417e-01, -4.12129394e-03, -2.60926819e-02, -2.22629091e-02,
       -1.22750224e-01, -2.24047300e-01,  3.02960330e-03, -2.62288944e-02,
       -1.12214186e-02,  1.13874751e-02,  1.54627198e-01,  1.20701192e-01,
       -1.86462115e-02, -3.19754440e-02, -1.84045224e-01, -3.29896512e-02,
       -2.43682128e-01, -2.32111942e-02]), array([ 1.19889814e-01, -4.52024804e-03, -2.93930319e-04,  2.55144646e-03,
        6.26960506e-05, -2.56450966e-03, -1.64363323e-02, -7.41290027e-03,
       -7.83203265e-03,  2.88691995e-03,  5.18952343e-03,  5.83646992e-04,
        1.67248877e-03,  4.14378326e-02,  2.31803857e-03,  1.21917292e-03,
       -6.06878789e-03,  8.32257930e-04, -4.16683288e-03,  5.82152580e-02,
       -6.24284659e-01, -3.24158067e-03, -5.59890544e-03,  3.99467138e-01,
       -2.25907581e-05, -1.86053817e-01, -1.52049264e-03,  1.14311053e-02,
       -9.15048909e-03, -4.12469960e-02,  9.99969339e-02,  4.66812182e-02,
        7.49315302e-02,  1.08474440e-03, -5.41894587e-04,  1.27495000e-02,
       -6.60045842e-02,  6.21307851e-02, -7.00454369e-02,  7.42217648e-03,
       -1.78293075e-02,  5.35816956e-03,  1.20683762e-01, -2.29541773e-01,
       -9.84133897e-04,  9.56484972e-03,  8.86886696e-02,  4.35516459e-01,
       -3.12614400e-01,  4.71586830e-03]), array([ 2.35146392e-01,  7.32324921e-02,  2.02888526e-05,  9.38578432e-03,
        1.89871619e-02,  1.91010775e-02,  4.35504191e-02,  2.15516918e-02,
        2.43934315e-02,  1.76962708e-02,  3.48676246e-02, -2.91251806e-04,
        2.83145709e-02,  7.33508163e-02,  9.01044186e-02,  3.68940866e-04,
        1.24217167e-02, -1.83396316e-03,  7.90325752e-03,  7.26534283e-02,
       -2.18594220e-01,  1.39578719e-02, -5.79178658e-03,  1.74863594e-02,
        1.63209214e-02, -3.96127424e-01, -5.25911869e-05,  4.51964858e-02,
        1.33581768e-02,  1.08730746e-01,  4.79418787e-02,  2.69197051e-02,
        6.09628954e-02, -3.45712284e-04,  1.91420361e-02,  2.17688206e-02,
       -2.39980176e-02,  3.26529719e-02,  2.06460604e-02,  1.83979116e-02,
        9.14243012e-03,  2.48966167e-03,  2.78382615e-01, -3.00665030e-01,
        2.52340941e-02,  1.91015267e-02, -3.37313928e-01, -5.76334316e-01,
        2.63680875e-01,  1.60115367e-02]), array([-1.07167351e-01,  7.43417864e-04, -2.23325427e-04, -2.76449805e-02,
       -2.41890395e-02, -2.66132032e-02, -7.06335695e-03, -3.63908500e-02,
       -6.27531870e-03, -2.23039557e-02, -8.59116962e-03, -1.69080644e-03,
       -2.78626661e-02, -2.39372052e-02, -2.87935797e-02,  1.82941774e-04,
       -9.91605372e-03,  6.79971589e-03, -2.28035480e-02,  7.86391597e-02,
        2.33182546e-01, -3.50697780e-02,  2.19626809e-04, -1.60712166e-01,
       -2.06841912e-02, -2.14792735e-01,  2.50483637e-03, -5.84497235e-02,
       -3.12033818e-02,  3.14362363e-03, -3.55570104e-01,  4.82509291e-02,
       -5.61899726e-02, -5.28971215e-03, -2.15370019e-02, -7.94570156e-03,
        2.13740348e-01, -5.14859256e-02, -8.71167060e-02, -2.64217710e-02,
        7.64952453e-03, -8.82231624e-04,  6.94779713e-01, -1.48047332e-02,
       -2.16096081e-02, -3.04332993e-02, -1.25735381e-01,  3.88020737e-01,
        5.26528588e-02, -2.03418371e-02]), array([-8.74041674e-02,  1.67100365e-01, -7.40539709e-05, -2.45654557e-02,
       -3.26940364e-02, -4.33923735e-02,  5.47383175e-03, -4.49632113e-02,
        2.61102596e-02, -2.90707848e-02, -3.40244795e-02, -1.39618404e-03,
       -1.12977164e-01, -2.76313036e-02,  9.55108235e-03,  5.37594792e-04,
       -1.71391195e-02, -3.88898483e-03, -2.11358882e-02, -6.47048276e-02,
        2.27646120e-01, -4.46775076e-02, -8.84574969e-04,  1.80646135e-01,
       -3.43552835e-02, -1.24665249e-01, -3.95420698e-04, -5.16715508e-02,
       -3.33054912e-02,  2.51143756e-02, -6.85787512e-02, -1.08604645e-01,
        4.81935067e-01, -3.59286726e-03, -3.69135981e-02, -4.48147979e-02,
        1.71906653e-01,  5.44937037e-01,  1.01545895e-02, -3.10135654e-02,
       -8.66608245e-03,  6.46777240e-03, -2.50884727e-01, -4.17350129e-02,
       -2.83407763e-02, -2.88048926e-02, -4.01853938e-01,  1.32254460e-01,
       -5.76213955e-02, -3.93647285e-02]), array([ 1.65046400e-01,  2.05006499e-01, -1.08047259e-06,  4.26003787e-02,
        4.37725586e-02,  5.31512685e-02,  5.50892246e-02,  3.29657018e-02,
        3.69979409e-02,  4.32856192e-02,  3.57328276e-02, -4.64862264e-05,
        6.30465683e-02,  4.44247071e-02,  5.23799932e-02,  4.82490532e-05,
        5.80019692e-02,  3.63706543e-03,  4.18119941e-02,  1.15024740e-01,
        1.33013206e-01,  3.61860943e-02, -1.38277705e-04, -1.92619380e-01,
        4.35228354e-02, -4.51169901e-01,  1.46786162e-03,  4.31271857e-02,
        4.04838363e-02,  2.07845570e-01, -1.82645507e-01,  4.07274374e-02,
       -2.29927654e-01, -1.72873936e-03,  3.90433699e-02,  4.27031533e-02,
       -1.18029575e-02, -3.04511991e-01,  9.12766497e-02,  4.23211173e-02,
       -2.79934062e-02, -2.33214406e-03, -4.91026878e-01, -1.52114309e-01,
        4.87124987e-02,  4.01296852e-02, -1.50929842e-01,  2.90334153e-01,
       -8.36816446e-02,  5.39741955e-02]), array([ 6.96388473e-01,  4.28532306e-01, -5.07162350e-05, -1.85198478e-02,
       -2.43839588e-02, -3.96015254e-02, -9.02794067e-02, -3.74552647e-02,
        1.92721689e-02, -3.18764769e-02, -5.05513150e-02,  2.23939070e-04,
       -1.20463488e-01, -2.73614553e-02, -4.69255300e-03,  8.81040204e-04,
       -2.36610845e-02, -3.46887423e-03, -1.72668655e-02, -7.27747243e-02,
        9.64261593e-02, -2.17861271e-02,  3.09241310e-04, -3.20019680e-02,
       -3.44801805e-02,  2.55032605e-01, -2.60660897e-04,  1.54448561e-02,
       -1.68978103e-02, -1.54901751e-01, -2.36581165e-01, -4.96435683e-02,
       -1.16551258e-02, -3.85242965e-04, -3.15871966e-02, -2.89958838e-02,
       -2.77947080e-01,  5.40987894e-02, -2.87094135e-02, -3.23257406e-02,
        4.38211925e-03,  7.70161977e-04,  1.19712284e-01,  8.42699317e-02,
       -2.48644583e-02, -4.01640543e-02, -5.05480850e-03, -1.04163281e-02,
       -1.60784772e-01, -1.66242245e-02]), array([-3.80694801e-01,  1.72302720e-01, -5.63971833e-05, -2.60483131e-02,
       -8.58450611e-03, -7.01647800e-03,  4.00298185e-02, -2.07328466e-03,
       -7.05508673e-02, -1.20193197e-02,  8.00970090e-02,  1.23681245e-03,
       -5.94235611e-02, -2.03224276e-02,  1.46419455e-01,  2.76258988e-04,
       -2.41159615e-02,  2.88849587e-03, -1.99631453e-02,  1.87602664e-01,
        1.52153905e-02,  4.64841222e-03, -9.23265155e-04, -9.68575544e-03,
       -6.60872185e-03, -2.32306823e-03,  4.17848393e-04,  9.61138510e-02,
       -1.90603575e-02,  3.07243917e-01, -1.09928489e-01,  1.03465829e-01,
        1.05735375e-01,  6.10761360e-04, -1.30667751e-02,  4.12661322e-03,
       -7.67764378e-01,  8.95745940e-02,  1.19333255e-03, -5.00428924e-03,
       -3.84651839e-03, -1.67859568e-03,  8.86368118e-02,  6.10845517e-02,
       -6.86293787e-03, -5.60139975e-03,  1.17195142e-02,  3.20419969e-02,
        3.08905254e-02,  1.05027201e-03]), array([-3.98965867e-01,  7.69370387e-01,  4.67660722e-05,  7.04444620e-03,
        1.09461521e-02,  5.86424496e-03, -3.47841125e-02,  9.26799722e-03,
        3.59449838e-03,  7.22341040e-03,  8.68916862e-02, -1.46664091e-03,
       -1.79786669e-01,  9.66370119e-03, -5.08316062e-02,  3.53089840e-04,
        2.75701798e-02,  1.63982100e-03,  6.74212238e-03, -4.62695908e-02,
       -1.22594204e-01,  2.18809447e-03, -1.65894875e-03,  4.39278993e-02,
        3.79062810e-03,  5.83595313e-03,  8.84597950e-04, -1.98786189e-02,
        1.62711489e-02, -2.48764458e-01,  1.69223096e-01, -4.61111154e-02,
       -1.53977769e-01,  5.33759268e-04,  7.68319752e-03,  7.32819287e-03,
        1.96128487e-01, -1.42298448e-01,  2.19059723e-03,  1.01834134e-02,
        1.40650403e-02,  3.77987447e-03,  4.06781442e-02, -1.56911299e-02,
        5.74785729e-03,  1.03462873e-02,  1.75337402e-02, -7.44126998e-02,
        2.75231485e-02,  2.29919073e-03]), array([-1.26443930e-02, -3.89588256e-02,  1.27236704e-04, -3.16609030e-02,
       -1.47667276e-02, -2.26002966e-02,  5.50250081e-02, -3.29772530e-02,
        1.01037525e-01, -2.11948566e-02,  1.81015701e-01,  1.92412362e-03,
        4.58539856e-02,  7.74559634e-02,  1.23225678e-02,  4.55357862e-04,
       -2.07758223e-02, -4.60860770e-04, -7.18843392e-03,  1.34267739e-01,
       -2.39691725e-01, -2.08376996e-02, -4.49850084e-03, -3.70661021e-02,
       -1.66587533e-02,  5.15709520e-01,  8.56092116e-04,  1.46419674e-02,
       -3.10930910e-02,  1.99185959e-01,  5.88745877e-02, -2.16449604e-02,
       -1.04266430e-01,  2.75868418e-03, -1.97147731e-02, -1.47813712e-02,
        7.10364367e-02, -2.09171838e-01, -1.70371982e-02, -1.73329768e-02,
       -8.83928531e-03,  5.93566131e-03, -1.33085094e-03,  8.55310192e-02,
       -1.22848830e-02, -2.02012856e-02, -6.81609655e-01,  1.34886333e-01,
       -2.38900544e-03, -1.78553874e-02]), array([-6.91965775e-02,  1.48959243e-01, -7.75865564e-05, -1.25506995e-02,
       -2.02917195e-02, -2.64987036e-02, -5.46001170e-02, -2.09665460e-02,
       -1.41117229e-02, -2.19773681e-02, -1.73046094e-02, -6.49385266e-04,
        9.24327145e-01, -2.94752270e-02, -5.15079241e-02, -1.00603012e-04,
        2.52119100e-03,  5.74170423e-06, -1.61331003e-02, -1.47155742e-01,
        3.23350094e-02, -1.76937711e-02,  1.25227655e-03, -3.23350768e-04,
       -2.37629291e-02, -2.47778819e-02, -1.21829951e-04, -9.29754535e-03,
       -1.20050846e-02, -2.05719783e-01,  1.98980513e-02, -8.64436669e-02,
        1.94359895e-02, -1.30465370e-03, -1.91717044e-02, -3.22497649e-02,
       -1.36222814e-01,  3.71322108e-02, -3.28993778e-03, -2.58186015e-02,
       -3.05614302e-03,  2.75602858e-03,  3.20702764e-02, -1.14870319e-02,
       -1.99011983e-02, -2.70783011e-02, -8.84636290e-02,  4.02435082e-02,
        9.20654555e-03, -1.81506037e-02])])
        whiten = False
        explained_variance = np.array([447631.9933482972, 175863.49940418144, 31019.69948352794, 18921.332635232415, 5065.787458562951, 2113.3326311393366, 1498.7809756818247, 956.9578252057233, 690.5140734935146, 566.7305492211574, 19.53558562919184, 4.837184127636046, 2.702047234866083, 2.17396270297376, 1.1237087051059202, 0.9251692382598125, 0.8390030311097125, 0.609705639993158, 0.48577393230082505, 0.409786778678677, 0.31735663927975555, 0.28992655645054216, 0.2560040266702474, 0.1948544165673684])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    il=[]
    
    ignorelabels=[]
    ignorecolumns=[]
    target="signal"


    if (testfile):
        target=''
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless==False):
                header=next(reader, None)
                try:
                    if (target!=''): 
                        hc=header.index(target)
                    else:
                        hc=len(header)-1
                        target=header[hc]
                except:
                    raise NameError("Target '"+target+"' not found! Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=header.index(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute '"+ignorecolumns[i]+"' is the target. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '"+ignorecolumns[i]+"' not found in header. Header must be same as in file passed to btc.")
                for i in range(0,len(header)):      
                    if (i==hc):
                        continue
                    if (i in il):
                        continue
                    print(header[i]+",", end = '', file=outputfile)
                print(header[hc],file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if (row[target] in ignorelabels):
                        continue
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name==target):
                            continue
                        if (',' in row[name]):
                            print ('"'+row[name]+'"'+",",end = '', file=outputfile)
                        else:
                            print (row[name]+",",end = '', file=outputfile)
                    print (row[target], file=outputfile)

            else:
                try:
                    if (target!=""): 
                        hc=int(target)
                    else:
                        hc=-1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=int(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute "+str(col)+" is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    if (hc==-1):
                        hc=len(row)-1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0,len(row)):
                        if (i in il):
                            continue
                        if (i==hc):
                            continue
                        if (',' in row[i]):
                            print ('"'+row[i]+'"'+",",end = '', file=outputfile)
                        else:
                            print(row[i]+",",end = '', file=outputfile)
                    print (row[hc], file=outputfile)

def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'False':0,'True':1}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    # function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping

# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)
# Classifier
def classify(row):
    #inits
    x=row
    o=[0]*num_output_logits


    #Nueron Equations
    h_0 = max((((180.2156 * float(x[0]))+ (106.98701 * float(x[1]))+ (-78.85662 * float(x[2]))+ (-32.06165 * float(x[3]))+ (19.304361 * float(x[4]))+ (22.540401 * float(x[5]))+ (8.518422 * float(x[6]))+ (5.0355244 * float(x[7]))+ (5.868034 * float(x[8]))+ (44.43667 * float(x[9]))+ (-1.160374 * float(x[10]))+ (2.9928129 * float(x[11]))+ (-10.079935 * float(x[12]))+ (-4.7851005 * float(x[13]))+ (0.10665709 * float(x[14]))+ (-7.7985663 * float(x[15]))+ (9.574092 * float(x[16]))+ (2.8633385 * float(x[17]))+ (7.7571387 * float(x[18]))+ (-4.874145 * float(x[19]))+ (2.7142196 * float(x[20]))+ (5.67177 * float(x[21]))+ (8.682265 * float(x[22]))+ (-11.983085 * float(x[23]))) + -3.1594756), 0)
    h_1 = max((((-153.65953 * float(x[0]))+ (-11.015225 * float(x[1]))+ (25.367006 * float(x[2]))+ (3.9290676 * float(x[3]))+ (-6.062605 * float(x[4]))+ (-16.327593 * float(x[5]))+ (32.56129 * float(x[6]))+ (20.251064 * float(x[7]))+ (-9.679777 * float(x[8]))+ (-1.4256659 * float(x[9]))+ (-1.1917366 * float(x[10]))+ (1.3865347 * float(x[11]))+ (-1.838105 * float(x[12]))+ (0.10578909 * float(x[13]))+ (-1.0542428 * float(x[14]))+ (-0.47427258 * float(x[15]))+ (1.7148814 * float(x[16]))+ (-0.089562364 * float(x[17]))+ (-1.5231521 * float(x[18]))+ (1.0303365 * float(x[19]))+ (0.25072938 * float(x[20]))+ (0.29745477 * float(x[21]))+ (-2.4048915 * float(x[22]))+ (-0.30325598 * float(x[23]))) + -2.5000196), 0)
    h_2 = max((((-1.6867397 * float(x[0]))+ (-15.445243 * float(x[1]))+ (-5.903711 * float(x[2]))+ (-34.81486 * float(x[3]))+ (-3.7615387 * float(x[4]))+ (4.942888 * float(x[5]))+ (9.951024 * float(x[6]))+ (-28.004068 * float(x[7]))+ (12.86398 * float(x[8]))+ (-1.8206726 * float(x[9]))+ (0.31825474 * float(x[10]))+ (-3.1335926 * float(x[11]))+ (-4.4325876 * float(x[12]))+ (-5.2651744 * float(x[13]))+ (-1.1620029 * float(x[14]))+ (-5.880775 * float(x[15]))+ (3.602057 * float(x[16]))+ (5.03276 * float(x[17]))+ (1.6179581 * float(x[18]))+ (-4.0235386 * float(x[19]))+ (-0.7553149 * float(x[20]))+ (2.9564433 * float(x[21]))+ (2.4009109 * float(x[22]))+ (-4.6156116 * float(x[23]))) + 0.48927408), 0)
    h_3 = max((((2.796388 * float(x[0]))+ (1.6279974 * float(x[1]))+ (-1.2635323 * float(x[2]))+ (-0.46340513 * float(x[3]))+ (1.2907455 * float(x[4]))+ (-0.46094525 * float(x[5]))+ (-0.13708957 * float(x[6]))+ (0.700805 * float(x[7]))+ (1.1185724 * float(x[8]))+ (-2.9143734 * float(x[9]))+ (-0.32257622 * float(x[10]))+ (6.2138996 * float(x[11]))+ (14.853516 * float(x[12]))+ (12.75428 * float(x[13]))+ (5.413511 * float(x[14]))+ (10.84658 * float(x[15]))+ (-14.7705765 * float(x[16]))+ (-5.277673 * float(x[17]))+ (-13.129992 * float(x[18]))+ (10.730352 * float(x[19]))+ (-6.4812517 * float(x[20]))+ (-12.5209675 * float(x[21]))+ (-15.010866 * float(x[22]))+ (16.954443 * float(x[23]))) + 0.7244804), 0)
    h_4 = max((((-1.1623408 * float(x[0]))+ (-0.08241829 * float(x[1]))+ (0.18647546 * float(x[2]))+ (0.026340418 * float(x[3]))+ (-0.061779957 * float(x[4]))+ (-0.10847169 * float(x[5]))+ (0.24898626 * float(x[6]))+ (0.13440461 * float(x[7]))+ (-0.10159475 * float(x[8]))+ (-0.0006976286 * float(x[9]))+ (-0.17161404 * float(x[10]))+ (-0.25165984 * float(x[11]))+ (-0.02102483 * float(x[12]))+ (-0.0069044097 * float(x[13]))+ (-0.2738707 * float(x[14]))+ (-0.3136023 * float(x[15]))+ (0.37091383 * float(x[16]))+ (-0.004619205 * float(x[17]))+ (0.07117632 * float(x[18]))+ (-0.5883723 * float(x[19]))+ (0.44191775 * float(x[20]))+ (0.3165693 * float(x[21]))+ (-0.32947728 * float(x[22]))+ (-1.4624585 * float(x[23]))) + -1.2566587), 0)
    o[0] = (0.00021477518 * h_0)+ (-0.012138342 * h_1)+ (2.5081259e-05 * h_2)+ (-0.013017722 * h_3)+ (1.6015258 * h_4) + -1.1609972

    

    #Output Decision Rule
    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)


def Predict(arr,headerless,csvfile, get_key, classmapping):
    with open(csvfile, 'r') as csvinput:
        #readers and writers
        writer = csv.writer(sys.stdout, lineterminator=os.linesep)
        reader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            writer.writerow(','.join(next(reader, None) + ["Prediction"]))
        
        
        for i, row in enumerate(reader):
            #use the transformed array as input to predictor
            pred = str(get_key(int(classify(arr[i])), classmapping))
            #use original untransformed line to write out
            row.append(pred)
            writer.writerow(row)


def Validate(arr):
    if n_classes == 2:
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        outputs=[]
        for i, row in enumerate(arr):
            outputs.append(int(classify(arr[i, :-1].tolist())))
        outputs=np.array(outputs)
        correct_count = int(np.sum(outputs.reshape(-1) == arr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(arr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(arr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0
    else:
        numeachclass = {}
        count, correct_count = 0, 0
        preds = []
        for i, row in enumerate(arr):
            pred = int(classify(arr[i].tolist()))
            preds.append(pred)
            if pred == int(float(arr[i, -1])):
                correct_count += 1
                if int(float(arr[i, -1])) in numeachclass.keys():
                    numeachclass[int(float(arr[i, -1]))] += 1
                else:
                    numeachclass[int(float(arr[i, -1]))] = 0
            count += 1
        return count, correct_count, numeachclass, preds
    


# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()


    #clean if not already clean
    if not args.cleanfile:
        tempdir = tempfile.gettempdir()
        cleanfile = tempdir + os.sep + "clean.csv"
        preprocessedfile = tempdir + os.sep + "prep.csv"
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x,y: x
        classmapping = {}


    #load file
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')


    #Normalize
    cleanarr = Normalize(cleanarr)


    #Transform
    if transform_true:
        if args.validate:
            trans = transform(cleanarr[:, :-1])
            cleanarr = np.concatenate((trans, cleanarr[:, -1].reshape(-1, 1)), axis = 1)
        else:
            cleanarr = transform(cleanarr)


    #Predict
    if not args.validate:
        Predict(cleanarr, args.headerless, preprocessedfile, get_key, classmapping)


    #Validate
    else: 
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = Validate(cleanarr)
        else:
            count, correct_count, numeachclass, preds = Validate(cleanarr)
            #Correct Labels
            true_labels = cleanarr[:, -1]


        #Report Metrics
        model_cap=131
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
            print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
            print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
            print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
            if int(num_TP + num_FN) != 0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN + num_FP) != 0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP + num_FP) != 0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2 * num_TP + num_FP + num_FN) != 0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP + num_FN) != 0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP + num_FN + num_FP) != 0:
                print("Critical Success Index:             {:.2f}".format(TS))

        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            print("System Type:                        " + str(n_classes) + "-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")





            def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
                #check for numpy/scipy is imported
                try:
                    from scipy.sparse import coo_matrix #required for multiclass metrics
                    try:
                        np.array
                    except:
                        import numpy as np
                except:
                    raise ValueError("Scipy and Numpy Required for Multiclass Metrics")
                # Compute confusion matrix to evaluate the accuracy of a classification.
                # By definition a confusion matrix :math:C is such that :math:C_{i, j}
                # is equal to the number of observations known to be in group :math:i and
                # predicted to be in group :math:j.
                # Thus in binary classification, the count of true negatives is
                # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
                # :math:C_{1,1} and false positives is :math:C_{0,1}.
                # Read more in the :ref:User Guide <confusion_matrix>.
                # Parameters
                # ----------
                # y_true : array-like of shape (n_samples,)
                # Ground truth (correct) target values.
                # y_pred : array-like of shape (n_samples,)
                # Estimated targets as returned by a classifier.
                # labels : array-like of shape (n_classes), default=None
                # List of labels to index the matrix. This may be used to reorder
                # or select a subset of labels.
                # If None is given, those that appear at least once
                # in y_true or y_pred are used in sorted order.
                # sample_weight : array-like of shape (n_samples,), default=None
                # Sample weights.
                # normalize : {'true', 'pred', 'all'}, default=None
                # Normalizes confusion matrix over the true (rows), predicted (columns)
                # conditions or all the population. If None, confusion matrix will not be
                # normalized.
                # Returns
                # -------
                # C : ndarray of shape (n_classes, n_classes)
                # Confusion matrix.
                # References
                # ----------
                if labels is None:
                    labels = np.array(list(set(list(y_true.astype('int')))))
                else:
                    labels = np.asarray(labels)
                    if np.all([l not in y_true for l in labels]):
                        raise ValueError("At least one label specified must be in y_true")


                if sample_weight is None:
                    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
                else:
                    sample_weight = np.asarray(sample_weight)
                if y_true.shape[0]!=y_pred.shape[0]:
                    raise ValueError("y_true and y_pred must be of the same length")

                if normalize not in ['true', 'pred', 'all', None]:
                    raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


                n_labels = labels.size
                label_to_ind = {y: x for x, y in enumerate(labels)}
                # convert yt, yp into index
                y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
                y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
                # intersect y_pred, y_true with labels, eliminate items not in labels
                ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
                y_pred = y_pred[ind]
                y_true = y_true[ind]
                # also eliminate weights of eliminated items
                sample_weight = sample_weight[ind]
                # Choose the accumulator dtype to always have high precision
                if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                    dtype = np.int64
                else:
                    dtype = np.float64
                cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


                with np.errstate(all='ignore'):
                    if normalize == 'true':
                        cm = cm / cm.sum(axis=1, keepdims=True)
                    elif normalize == 'pred':
                        cm = cm / cm.sum(axis=0, keepdims=True)
                    elif normalize == 'all':
                        cm = cm / cm.sum()
                    cm = np.nan_to_num(cm)
                return cm


            print("Confusion Matrix:")
            mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #Clean Up
    if not args.cleanfile:
        os.remove(cleanfile)
        os.remove(preprocessedfile)
