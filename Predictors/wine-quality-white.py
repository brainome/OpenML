#!/usr/bin/env python3
#
# This code is was produced by an alpha version of Brainome Daimensions(tm) and is
# licensed under GNU GPL v2.0 or higher. For details, please see:
# https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
#
#
# Output of Brainome Daimensions(tm) 0.96 Table Compiler v0.96.
# Invocation: btc https://www.openml.org/data/get_csv/4535759/phpTJRsqa -o Predictors/wine-quality-white_QC.py -target Class -stopat 87.42 -f QC -e 100 --yes --runlocalonly
# Total compiler execution time: 0:00:03.67. Finished on: May-27-2020 22:17:11.
# This source code requires Python 3.
#
"""
Classifier Type: Quick Clustering
System Type:                        7-way classifier
Best-guess accuracy:                44.92%
Model accuracy:                     100.00% (4898/4898 correct)
Improvement over best guess:        55.08% (of possible 55.08%)
Model capacity (MEC):               2599 bits
Generalization ratio:               1.88 bits/bit
Confusion Matrix:
 [44.88% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 29.75% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 17.97% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 3.57% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 3.33% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.41% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.10%]

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "phpTJRsqa.csv"


#Number of attributes
num_attr = 11
n_classes = 7


# Preprocessor for CSV files
def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    il=[]
    
    ignorelabels=[]
    ignorecolumns=[]
    target="Class"


    if (testfile):
        target = ''
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if (target != ''): 
                        hc = header.index(target)
                    else:
                        hc = len(header) - 1
                        target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                for i in range(0, len(header)):      
                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    print(header[i] + ",", end='', file=outputfile)
                print(header[hc], file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if (row[target] in ignorelabels):
                        continue
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name==target):
                            continue
                        if (',' in row[name]):
                            print ('"' + row[name] + '"' + ",", end='', file=outputfile)
                        else:
                            print (row[name] + ",", end='', file=outputfile)
                    print (row[target], file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc =- 1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    if (hc == -1):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if (',' in row[i]):
                            print ('"' + row[i] + '"'+",", end='', file=outputfile)
                        else:
                            print(row[i]+",", end = '', file=outputfile)
                    print (row[hc], file=outputfile)

def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'4': 0, '3': 1, '5': 2, '6': 3, '2': 4, '1': 5, '7': 6}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    # function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping

# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([38.78134, 42.666079999999994, 48.61118999999999, 52.164590000000004, 53.89685, 54.4404, 54.6862, 54.909325, 56.012475, 57.663355, 61.9755, 63.10574999999999, 65.32624999999999, 67.14541, 71.06397, 74.99017999999998, 76.19733, 77.72200000000001, 78.369825, 79.02672000000001, 79.17274, 79.31642, 79.53925000000001, 79.81427000000001, 80.0276, 80.54909, 80.98289, 82.57988, 84.63390000000001, 84.7475, 84.842825, 85.6155, 85.86587, 86.64117, 86.79563, 86.9837, 87.30749000000002, 87.76352000000001, 88.03833, 88.48552000000001, 89.07682499999999, 89.52083499999999, 89.85786, 90.30850000000001, 92.21764999999999, 93.02710000000002, 93.80942999999999, 94.29360000000001, 94.56015, 95.72099999999999, 96.163375, 96.484175, 97.1651, 97.709015, 97.856115, 98.36226999999998, 98.59165999999999, 99.56513999999999, 99.878035, 100.4145, 101.00210000000001, 101.32930000000002, 101.4837, 101.663275, 101.93984, 102.195965, 102.379245, 102.44834499999999, 102.47527500000001, 102.483125, 102.5556, 102.74284499999999, 103.24601, 103.51035000000002, 103.783145, 103.82695, 103.93305000000001, 104.10321000000002, 104.17443, 104.41345000000001, 104.6968, 104.84699999999998, 105.11525, 105.36965000000001, 106.071045, 106.18702, 106.25202, 106.34549999999999, 106.40090000000001, 106.47710000000001, 106.638465, 106.86716499999999, 107.11554999999998, 107.28175999999999, 107.398045, 107.95064250000001, 108.48797000000002, 108.74243, 108.87052999999999, 109.13538, 109.45899, 109.67147500000002, 109.790805, 109.90077500000001, 110.4263, 110.77533, 110.88234, 110.96684000000002, 110.98075, 111.03544, 111.15018, 111.260705, 111.34072, 111.41875000000002, 111.5507, 111.68700000000001, 111.7853, 111.93170500000001, 111.967, 112.08687, 112.46065, 112.87024500000001, 113.17147, 113.18638, 113.21571, 113.384175, 113.72585000000001, 114.01820000000001, 114.25627, 114.42872, 114.53174999999999, 114.5539665, 114.57691650000001, 114.63432, 114.672, 114.68408, 114.78424999999999, 114.9142, 114.974615, 115.053315, 115.16845, 115.27029, 115.44615, 115.613335, 115.68295, 115.752225, 116.21600000000001, 116.35633999999999, 116.50046, 116.599065, 116.66845, 116.77079, 116.89075500000001, 116.9374, 117.05844999999998, 117.15991999999999, 117.24242, 117.33315, 117.408345, 117.56991, 117.69501, 117.82916499999999, 117.88958, 118.51740000000001, 118.60477000000002, 118.67357000000001, 118.752475, 118.87127500000001, 118.96340000000001, 119.0223, 119.189425, 119.25252, 119.47833, 119.661495, 119.68328500000001, 119.81428, 119.89898500000001, 120.05318500000001, 120.2793, 120.414985, 120.478655, 120.50975, 120.62752, 120.76705, 120.99432, 121.13900000000001, 121.209855, 121.265365, 121.27866, 121.32866, 121.39595, 121.46649, 121.52835999999999, 121.65636, 121.882985, 122.037585, 122.08769999999998, 122.1098, 122.2827, 122.46616499999999, 122.49846500000001, 122.54866650000001, 122.70088000000001, 122.88587000000001, 123.01079, 123.04106, 123.078675, 123.26295499999999, 123.367105, 123.44697, 123.73811, 123.85815999999998, 123.90700499999997, 124.06833, 124.30655, 124.376175, 124.447405, 124.66108, 124.84315000000001, 124.91550000000001, 125.03200000000001, 125.35148999999998, 125.3737, 125.54146999999998, 125.84619999999998, 126.07282000000001, 126.19951, 126.4281, 126.51646, 126.54518, 126.74539000000001, 126.77682, 126.96632499999998, 127.14779499999999, 127.18915, 127.31751500000001, 127.488565, 127.60879499999999, 127.70645999999999, 127.72739999999999, 127.8268, 127.912665, 127.925285, 128.05493, 128.178775, 128.22126500000002, 128.35852, 128.40612, 128.482525, 128.8868, 128.93295, 128.97383000000002, 129.05822999999998, 129.11744, 129.25498, 129.38645, 129.88844, 130.004885, 130.074545, 130.1855, 130.42465, 130.937265, 131.26885499999997, 131.31025999999997, 131.4091, 131.56784, 131.67246999999998, 131.73739, 131.996185, 132.10648, 132.13909999999998, 132.34398500000003, 132.444825, 132.54375, 132.653275, 132.75303, 132.92552999999998, 132.96581, 132.993525, 133.024405, 133.050515, 133.11039999999997, 133.17728, 133.23384, 133.2776, 133.30530000000002, 133.59920000000002, 133.71089, 133.78187, 133.8773665, 133.97414, 134.0571, 134.23063499999998, 134.25457999999998, 134.32285, 134.45005, 134.48289999999997, 134.56962499999997, 134.63816, 134.642055, 134.65573, 134.67822999999999, 134.75642, 134.92237, 134.99373, 135.2018, 135.25436000000002, 135.273875, 135.340625, 135.41805, 135.47266, 135.53019, 135.62544, 135.68811, 135.7437, 135.886215, 136.01899999999998, 136.05706999999998, 136.0923, 136.144215, 136.22848499999998, 136.40841999999998, 136.45714, 136.48466000000002, 136.53415999999999, 136.56675, 136.59024, 136.68104, 136.77505000000002, 136.79569000000004, 136.80706000000004, 136.83852000000002, 136.86509, 136.91554, 136.97002, 137.01655499999998, 137.061825, 137.08995, 137.11090000000002, 137.13134, 137.19969, 137.2613, 137.29531500000002, 137.403975, 137.552545, 137.61892500000002, 137.76391999999998, 137.93577, 137.99551000000002, 138.05774, 138.12373000000002, 138.13562000000002, 138.14812, 138.15776, 138.18241, 138.269925, 138.36625999999998, 138.41125499999998, 138.613155, 138.63078499999997, 138.65447999999998, 138.68628999999999, 138.70119, 138.96341999999999, 139.03277, 139.13835999999998, 139.26351, 139.310585, 139.35691500000001, 139.39424, 139.4277, 139.48269, 139.60303, 139.70595, 139.80372, 139.911835, 139.99138, 140.034995, 140.10855, 140.19213000000002, 140.22039999999998, 140.252545, 140.289215, 140.30157, 140.37695, 140.55360000000002, 140.62855, 140.67298999999997, 140.97131499999998, 140.99693, 141.0134, 141.03727000000003, 141.10125000000002, 141.15595000000002, 141.19855, 141.2458, 141.40474999999998, 141.47703, 141.53885000000002, 141.600885, 141.71211499999998, 141.86645, 141.90319, 141.94708999999997, 141.95909999999998, 141.98127499999998, 142.00262499999997, 142.09691, 142.23829999999998, 142.25375000000003, 142.26375000000002, 142.32317, 142.43645, 142.46435000000002, 142.542715, 142.67986000000002, 142.729215, 142.74276, 142.7722, 142.962075, 142.990675, 143.004325, 143.03044999999997, 143.152735, 143.41211499999997, 143.56746, 143.62942500000003, 143.678865, 143.743395, 143.82865499999997, 143.82979999999998, 143.85395, 143.86585, 143.983845, 144.02359, 144.07272, 144.12091999999998, 144.22619999999998, 144.36939999999998, 144.39655, 144.40215, 144.4103, 144.431525, 144.501925, 144.5669, 144.63368, 144.70808500000004, 144.74189, 144.8078, 144.81479000000002, 144.84516, 144.89436999999998, 144.93079999999998, 145.13247, 145.15980000000002, 145.18409, 145.28289999999998, 145.79998, 145.88395500000001, 145.97063, 145.99108, 146.0002, 146.00234, 146.00671, 146.02273999999997, 146.219275, 146.354755, 146.46027, 146.51077, 146.570685, 146.61391, 146.73895, 146.75132000000002, 146.949955, 147.01445499999997, 147.04363, 147.1557, 147.265825, 147.293075, 147.31969999999998, 147.33615, 147.39954, 147.75189, 147.79658, 147.83489500000002, 147.85296499999998, 148.284595, 148.36095, 148.63279, 148.70845, 148.79647999999997, 148.81237, 148.81932, 148.87158499999998, 148.899995, 148.92371500000002, 148.95732, 149.00115, 149.0335, 149.11977000000002, 149.202345, 149.2661, 149.46983, 149.51282, 149.53749, 149.55650000000003, 149.627225, 149.691635, 149.77694000000002, 149.84586000000002, 149.87968, 149.89861000000002, 149.9203, 149.99314499999997, 150.07234999999997, 150.08867500000002, 150.231615, 150.26909, 150.29291999999998, 150.30847, 150.32119999999998, 150.33238999999998, 150.40803999999997, 150.54383, 150.56303000000003, 150.59980000000002, 150.65327000000002, 150.71857, 150.73834999999997, 150.75965000000002, 150.78035, 150.8034, 150.86214, 150.9061, 151.02594, 151.05541499999998, 151.08281500000004, 151.102325, 151.12539499999997, 151.147085, 151.27405, 151.42068, 151.58769999999998, 151.66507000000001, 151.73280999999997, 151.74914, 151.7554, 151.92661499999997, 151.9442, 151.95960000000002, 151.979475, 152.04422, 152.116795, 152.16575, 152.2198, 152.26510000000002, 152.41752000000002, 152.43490000000003, 152.461485, 152.525805, 152.57571000000002, 152.60016000000002, 152.6473, 152.72441750000002, 152.79489999999998, 152.84952999999996, 152.86268, 152.95434, 153.0491, 153.28746, 153.30053000000004, 153.31338, 153.33314000000001, 153.37376, 153.468365, 153.48903, 153.53333, 153.55231999999998, 153.56292, 153.65926, 153.68505, 153.6947, 153.85039999999998, 153.90432, 153.98897, 154.02495, 154.07290999999998, 154.09814, 154.14513, 154.2532, 154.38490000000002, 154.41015000000002, 154.557635, 154.57034499999997, 154.58496, 154.60437, 154.62775, 154.63746500000002, 154.65739000000002, 154.72348, 154.737585, 154.76473, 154.81452, 154.86330500000003, 154.94227999999998, 155.053225, 155.132305, 155.17725000000002, 155.25758499999998, 155.384985, 155.38958499999998, 155.39531, 155.42876, 155.51601, 155.5836, 155.64086, 155.71320500000002, 155.72905, 155.75162500000002, 155.779675, 155.79849, 155.93657000000002, 155.95751, 156.02869, 156.09534000000002, 156.10836999999998, 156.11483999999996, 156.11979, 156.12321000000003, 156.12767, 156.18685499999998, 156.28184500000003, 156.32386000000002, 156.34783500000003, 156.38234500000002, 156.4267, 156.45731, 156.54585, 156.65812499999998, 156.67693, 156.71756999999997, 156.75417999999996, 156.78911499999998, 156.821235, 156.84628000000004, 156.89015, 156.96967, 156.98167, 157.05132999999998, 157.11665, 157.13079499999998, 157.152975, 157.16707499999998, 157.2117, 157.25741, 157.28201, 157.336775, 157.39243500000003, 157.41945, 157.46904, 157.54990499999997, 157.5621, 157.59692, 157.635745, 157.6626, 157.72390000000001, 157.79489500000003, 157.849995, 158.05289999999997, 158.05690499999997, 158.067605, 158.10829, 158.17439000000002, 158.2141, 158.225805, 158.28505, 158.34035, 158.36894, 158.40739, 158.517425, 158.53399000000002, 158.56067, 158.610315, 158.76785, 159.01111000000003, 159.01271, 159.04354999999998, 159.12074, 159.134825, 159.22502500000002, 159.312105, 159.35332999999997, 159.36314, 159.41773999999998, 159.45517999999998, 159.48592499999998, 159.51576, 159.5405, 159.55034999999998, 159.57637, 159.60769, 159.63260000000002, 159.71583, 159.78943, 159.94182, 159.960215, 159.99244499999998, 160.06461000000002, 160.07941, 160.09078999999997, 160.09774999999996, 160.11548, 160.19728, 160.241355, 160.2541775, 160.34915, 160.3608, 160.3741, 160.53924999999998, 160.601875, 160.637545, 160.662995, 160.71668, 160.7277, 160.93345, 160.94465, 161.049045, 161.139195, 161.1732, 161.20433, 161.23262499999998, 161.24259, 161.27093000000002, 161.30126, 161.3391, 161.41456, 161.52212500000002, 161.56796500000002, 161.5865, 161.62774000000002, 161.77781999999996, 161.80642999999998, 161.895465, 161.90877, 161.946285, 161.979535, 161.98670499999997, 162.01626999999996, 162.02759999999998, 162.046685, 162.073005, 162.09057, 162.278785, 162.3754, 162.43775, 162.46867999999998, 162.51833, 162.548095, 162.559305, 162.72746, 162.75300999999996, 162.85135999999997, 163.01447000000002, 163.04245, 163.30106, 163.329015, 163.37226499999997, 163.4184, 163.43815, 163.46880000000002, 163.527725, 163.576775, 163.61288000000002, 163.63064000000003, 163.67540000000002, 163.72089, 163.72395, 163.84320000000002, 163.96309, 163.96801499999998, 164.02863499999998, 164.06186999999997, 164.08982, 164.09990000000005, 164.13465000000002, 164.17483000000001, 164.18102, 164.22476999999998, 164.30865, 164.3535, 164.3619, 164.4172, 164.46705000000003, 164.52995000000004, 164.59244, 164.63399000000004, 164.66496, 164.71963, 164.74832500000002, 164.783935, 164.828685, 164.90845000000002, 165.172635, 165.22176000000002, 165.28380000000004, 165.37874, 165.56629999999998, 165.62307, 165.67234000000002, 165.7019, 165.7579, 165.86105, 165.91675, 166.13832000000002, 166.16782, 166.20577500000002, 166.234235, 166.2676, 166.50423, 166.58427, 166.62923999999998, 166.65096999999997, 166.68086, 166.691265, 166.69307500000002, 166.72815000000003, 166.78655000000003, 166.872475, 166.90943500000003, 166.95302000000004, 166.99632, 167.07413, 167.161135, 167.170265, 167.18607, 167.202895, 167.220975, 167.265665, 167.46336999999997, 167.48359999999997, 167.72426000000002, 167.77911, 167.845125, 167.93613000000005, 167.96933, 167.9875, 168.05025999999998, 168.17600000000002, 168.41875749999997, 168.44799999999998, 168.49017999999998, 168.53473, 168.55042650000001, 168.610865, 168.7576, 168.776015, 168.792705, 168.80012000000002, 168.81503000000004, 168.84888, 168.87058000000002, 168.87745, 168.883765, 168.899315, 168.9161, 169.0383, 169.15789999999998, 169.17864000000003, 169.24439, 169.33051999999998, 169.508595, 169.51130500000005, 169.520235, 169.52775, 169.54166999999998, 169.57658, 169.76887, 169.80246999999997, 169.89551, 169.969355, 170.04829999999998, 170.10819499999997, 170.13107499999998, 170.26645000000002, 170.30326000000002, 170.346995, 170.34925, 170.35807500000004, 170.41182500000002, 170.49235000000002, 170.54535, 170.565, 170.5795, 170.601605, 170.62125500000002, 170.69371, 170.70239999999998, 170.71009999999998, 170.72841, 170.74671, 170.86183999999997, 170.89079999999996, 170.90109999999999, 170.91264999999999, 170.95767, 171.00187, 171.02087999999998, 171.03422999999998, 171.0576, 171.10526, 171.1162, 171.16331499999998, 171.20137, 171.21841, 171.24546000000004, 171.27617000000004, 171.30067000000003, 171.39723500000002, 171.60448000000002, 171.60601, 171.62448, 171.67046000000002, 171.75459999999998, 171.76724000000002, 171.77709, 171.86397999999997, 171.89914999999996, 171.92301999999998, 171.977725, 171.981945, 172.12487000000002, 172.15697, 172.18752, 172.217805, 172.24894, 172.29158, 172.32906, 172.47104000000002, 172.52635, 172.55644999999998, 172.60150000000002, 172.65470000000002, 172.75417149999998, 172.792435, 172.87051000000002, 172.87624, 172.90748000000002, 172.95175, 173.04963, 173.07004, 173.09449, 173.10165, 173.149815, 173.21114, 173.24063, 173.4709, 173.48383500000003, 173.495075, 173.59341, 173.66334999999998, 173.7491, 173.75969999999998, 173.9041, 173.90832, 173.92972, 173.96534000000003, 173.98521, 173.991775, 174.04109499999998, 174.131075, 174.25708500000002, 174.31349999999998, 174.54020000000003, 174.55477000000002, 174.60087, 174.64937, 174.68312000000003, 174.71914999999998, 174.72794999999996, 174.74804, 174.80993999999998, 174.87304999999998, 174.90393999999998, 174.91254000000004, 174.93490000000003, 174.97590000000002, 175.027675, 175.09470500000003, 175.14058, 175.21928999999997, 175.24729, 175.26828, 175.387165, 175.412325, 175.4263, 175.47857, 175.51620000000003, 175.620125, 175.63359000000003, 175.64446500000003, 175.66415999999998, 175.66616, 175.67650500000002, 175.704455, 175.74384999999998, 175.81199999999995, 175.878535, 175.913535, 175.9568, 175.98729999999998, 176.01428999999996, 176.04654, 176.06331, 176.07354, 176.1525, 176.16700999999998, 176.29023, 176.340255, 176.411545, 176.48081, 176.50341, 176.524425, 176.54976500000004, 176.56977999999998, 176.58398499999998, 176.6189, 176.65680500000002, 176.819655, 176.96619, 177.02005000000003, 177.07716, 177.136715, 177.237015, 177.2833, 177.30746, 177.34825999999998, 177.39285999999998, 177.4207, 177.46042, 177.51623999999998, 177.56515000000002, 177.657745, 177.96432999999996, 178.012, 178.01982500000003, 178.050925, 178.07813, 178.09976, 178.12302999999997, 178.15157, 178.18552, 178.20175, 178.21861650000002, 178.25186, 178.27460000000002, 178.31623000000002, 178.339215, 178.34313, 178.39021000000002, 178.43887500000002, 178.58641, 178.71131000000003, 178.98499999999999, 179.05499, 179.14624, 179.2156, 179.23639999999997, 179.27022, 179.39286500000003, 179.46754500000003, 179.54933499999999, 179.58615500000002, 179.66062, 179.81926000000004, 179.94222000000002, 179.96907, 179.98059999999998, 179.99113500000004, 180.02903500000002, 180.16127, 180.26135, 180.32389999999998, 180.37113499999998, 180.421575, 180.440065, 180.48710000000003, 180.496315, 180.53054500000002, 180.58230500000002, 180.625295, 180.66861, 180.70738999999998, 180.72645, 180.75414999999998, 180.8204, 180.86725, 180.88777, 180.91091999999998, 180.91685, 181.03877, 181.0625, 181.09125, 181.117925, 181.131455, 181.20042999999998, 181.27365, 181.30995000000001, 181.35465, 181.39737000000002, 181.42407000000003, 181.46937, 181.494775, 181.51332499999998, 181.54116, 181.57296499999995, 181.610505, 181.66126, 181.699685, 181.843755, 181.988585, 182.021355, 182.09045, 182.14936500000002, 182.17542, 182.254955, 182.2625, 182.41935, 182.42969, 182.449385, 182.478045, 182.535, 182.59841, 182.62219, 182.63063, 182.641145, 182.66352, 182.71869499999997, 182.76576999999997, 182.798735, 182.85956499999998, 182.938465, 182.97555, 183.14792, 183.193265, 183.234825, 183.37847999999997, 183.512515, 183.52173499999998, 183.58700499999998, 183.648595, 183.89376, 183.921, 183.92639000000003, 183.95849, 184.1028, 184.12169, 184.13615, 184.15991, 184.18525, 184.19430499999999, 184.20587999999998, 184.3258, 184.34445, 184.35270000000003, 184.3647, 184.37775649999998, 184.42915, 184.46572, 184.56443000000002, 184.5722, 184.58643999999998, 184.71763000000004, 184.769, 184.848045, 184.94867, 185.03390000000002, 185.05699000000004, 185.06577, 185.11789999999996, 185.15785499999998, 185.16965499999998, 185.22274999999996, 185.34502500000002, 185.37144, 185.39095, 185.39849, 185.477065, 185.585875, 185.67053, 185.92060000000004, 185.95126500000003, 185.97081500000002, 186.02035, 186.09624999999997, 186.36917000000005, 186.40649000000002, 186.43685, 186.46293, 186.49138, 186.50903999999997, 186.669175, 186.6811, 186.69490000000002, 186.83315000000002, 186.8624865, 186.91754, 186.978745, 187.0313, 187.1002, 187.1375, 187.30309999999997, 187.319175, 187.38, 187.41154999999998, 187.45099, 187.49809, 187.516105, 187.58729999999997, 187.591295, 187.64667, 187.66592, 187.68, 187.71171500000003, 187.740235, 187.75122, 187.76782999999998, 187.80883999999998, 187.83043, 187.88112999999998, 187.92183, 187.96731499999999, 188.00126, 188.03318000000002, 188.09124000000003, 188.27747, 188.29658999999998, 188.68740000000003, 188.72718, 188.74492999999998, 188.76094999999998, 188.802935, 188.835335, 188.84372, 188.87527, 189.0333, 189.20211, 189.34385500000002, 189.390795, 189.44637999999998, 189.49453999999997, 189.57093999999995, 189.58406, 189.78080999999997, 189.82919999999996, 189.90741, 189.92825, 189.98853000000003, 190.06875499999998, 190.188285, 190.2334, 190.26259, 190.27733999999998, 190.320765, 190.430435, 190.4882, 190.53123, 190.57347999999996, 190.58441499999998, 190.7054, 190.72535, 190.74557, 190.75603, 190.78965999999997, 190.81889, 190.87582000000003, 190.9964, 191.0059, 191.06672500000002, 191.147445, 191.19248, 191.20863, 191.29219, 191.40696000000003, 191.45057, 191.52330999999998, 191.5307, 191.58529, 191.62153999999998, 191.684125, 191.74325499999998, 191.75948999999997, 191.76364999999998, 191.76859000000002, 191.7895, 191.81781, 191.84056850000002, 191.8545585, 191.90435, 191.9454, 192.044605, 192.07669500000003, 192.13021500000002, 192.2013265, 192.2031665, 192.27416, 192.5005, 192.61426, 192.67451, 192.72792000000004, 192.7409, 192.74509999999998, 192.80392, 192.87788999999998, 192.992745, 193.031965, 193.12142, 193.16187000000002, 193.266285, 193.314165, 193.37702000000002, 193.41303, 193.49057999999997, 193.5836, 193.6132, 193.762395, 193.819425, 193.972195, 194.011825, 194.063465, 194.1514, 194.22147999999999, 194.27168999999998, 194.33912000000004, 194.49579000000003, 194.62467500000002, 194.721325, 194.83375, 194.95408000000003, 194.98125, 195.011205, 195.044355, 195.11270000000002, 195.17004999999997, 195.185305, 195.19987500000002, 195.22197, 195.28003, 195.30155, 195.32397500000002, 195.438845, 195.48765, 195.56242500000002, 195.62772, 195.66905500000001, 195.72762, 195.74682, 195.76017, 195.81106, 195.90113000000002, 195.95754000000005, 195.99210000000005, 196.12581, 196.18489999999997, 196.20092, 196.21411999999998, 196.23594999999997, 196.25108, 196.27587, 196.30043999999998, 196.34289, 196.38569, 196.4059, 196.41035, 196.43455, 196.476765, 196.56964, 196.73817499999998, 196.86303, 196.92589, 196.98672499999998, 197.03961499999997, 197.105, 197.14875999999998, 197.201945, 197.3975, 197.42838, 197.44982, 197.59278, 197.61720000000003, 197.6268, 197.64390000000003, 197.676575, 197.87207, 197.89729, 197.92192, 198.08473, 198.11055, 198.37989499999998, 198.43829499999998, 198.50709999999998, 198.63464, 198.9841, 199.02965, 199.0582, 199.17949999999996, 199.20174999999998, 199.22390000000001, 199.25285000000002, 199.29083500000002, 199.34248, 199.37244499999997, 199.47525000000002, 199.55016, 199.59083499999997, 199.63962500000002, 199.69995, 199.75000999999997, 199.77086, 199.80516, 199.91336, 200.04360000000003, 200.13057500000002, 200.36748, 200.45994000000002, 200.49993999999998, 200.51197000000002, 200.54527000000002, 200.5718, 200.5919, 200.70773, 200.82343, 200.85354, 200.85593, 200.8881, 200.928115, 201.03962, 201.08272999999997, 201.09484999999998, 201.10973, 201.13112999999998, 201.1472, 201.15930500000002, 201.165815, 201.38339, 201.40806, 201.46323, 201.51709, 201.56501000000003, 201.65418, 201.86376, 201.91566, 202.01384000000002, 202.03483000000003, 202.07743349999998, 202.0940235, 202.11068999999998, 202.12878, 202.13956000000002, 202.15247, 202.30649999999997, 202.3295, 202.44098000000002, 202.54704000000004, 202.57796000000002, 202.691395, 202.774995, 202.82441, 202.88761, 202.94475, 202.97019999999998, 203.02294999999998, 203.10739, 203.16199999999998, 203.2115, 203.26809, 203.29131999999998, 203.34812999999997, 203.51772, 203.55017, 203.61524, 203.675995, 203.77095, 203.91860000000003, 203.92464, 203.93044, 203.94655999999998, 203.98055, 204.03454, 204.1261, 204.227, 204.23655000000002, 204.26513, 204.30146, 204.35406, 204.40332, 204.41573999999997, 204.46169999999998, 204.48413, 204.514065, 204.54005999999998, 204.67683999999997, 204.70528000000002, 204.76859000000002, 204.970675, 205.01521, 205.04724, 205.117565, 205.166855, 205.202995, 205.20569, 205.23051000000004, 205.23741, 205.299, 205.40321, 205.5691435, 205.74495499999998, 205.85657499999996, 205.9869, 206.02769999999998, 206.0597, 206.08218, 206.09852999999998, 206.23546000000002, 206.30897000000004, 206.42275, 206.51615, 206.68425, 206.71465, 206.75830000000002, 206.86839000000003, 207.21185000000003, 207.36581, 207.44048499999997, 207.50587000000002, 207.52675000000002, 207.54694999999998, 207.64279999999997, 207.82048, 207.85116, 207.86594499999998, 207.913965, 208.11692000000002, 208.18380000000002, 208.24797999999998, 208.34328500000004, 208.39363500000002, 208.437435, 208.45864999999998, 208.46507, 208.47019, 208.47632, 208.49229499999998, 208.57111499999996, 208.740205, 208.804905, 208.83401500000002, 208.885985, 208.98636000000002, 209.06465000000003, 209.13904000000002, 209.2195, 209.317, 209.365465, 209.39772499999998, 209.40591999999998, 209.44354999999996, 209.45535, 209.49075, 209.5686, 209.61548000000002, 209.63358, 209.6696, 209.76835, 209.88420000000002, 209.98129999999998, 210.03085, 210.12925, 210.20935, 210.25262500000002, 210.26851500000004, 210.29859, 210.36761, 210.45601, 210.50806999999998, 210.52478, 210.56074, 210.6008, 210.63977, 210.68456999999998, 210.95540499999998, 211.05343, 211.14439999999996, 211.1812, 211.18785000000003, 211.29317, 211.31792, 211.40542500000004, 211.47630000000004, 211.65004499999998, 211.74631, 211.80568, 211.81068, 211.90825, 211.96237000000002, 211.999455, 212.03116, 212.06242000000003, 212.21675, 212.28985, 212.39034999999996, 212.65861499999997, 212.68784999999997, 212.71747, 212.85212, 212.91512, 212.9654, 212.9876, 213.44845999999998, 213.60155, 213.6301565, 213.64608650000002, 213.68598, 213.90890000000002, 213.9833, 214.05199, 214.05624, 214.1157, 214.169375, 214.332755, 214.3838, 214.452235, 214.528575, 214.58651, 214.61095999999998, 214.645515, 214.75254, 214.843075, 214.8492, 214.87075000000002, 214.91575, 214.96385, 214.98547, 215.01092999999997, 215.08726000000001, 215.143535, 215.203435, 215.23216, 215.259475, 215.33377000000002, 215.79295000000002, 215.94925, 216.06322999999998, 216.09977499999997, 216.18009999999998, 216.24935, 216.27505000000002, 216.3306, 216.377725, 216.40209499999997, 216.53307999999998, 216.66844, 216.85877, 216.88267000000002, 216.9719, 217.05106, 217.12506, 217.19089000000002, 217.19111000000004, 217.21924, 217.2766, 217.31040000000002, 217.35110000000003, 217.36165500000004, 217.376055, 217.4069, 217.46555, 217.52573, 217.69511, 217.74314999999999, 217.79915, 217.85862500000002, 217.98984000000002, 218.04999, 218.08174000000002, 218.14578, 218.19371499999997, 218.21303149999997, 218.2399665, 218.26317500000002, 218.37204000000003, 218.419695, 218.45395499999998, 218.53244999999998, 218.57984999999996, 218.62365, 218.718545, 218.783475, 218.81793, 218.88156, 218.92791, 218.93625, 218.9536, 218.96946499999999, 218.972265, 219.32468500000002, 219.36138, 219.38866000000002, 219.46551999999997, 219.56204, 219.59973000000002, 219.65433000000002, 219.79488, 219.91740000000004, 220.01302, 220.10662499999998, 220.18564, 220.20972, 220.22225000000003, 220.45492000000002, 220.54740999999999, 220.55969499999998, 220.578455, 220.64215, 220.699825, 220.744525, 220.86249999999998, 220.87309999999997, 220.88387, 220.986185, 221.0804, 221.1086, 221.18739999999997, 221.21859999999998, 221.23485, 221.306305, 221.38450500000002, 221.4605, 221.51251, 221.59834, 221.64543, 221.6533, 221.67149999999998, 221.70260000000002, 221.86645, 221.94684999999998, 222.067295, 222.08068999999998, 222.12156, 222.33817, 222.49982, 222.592525, 222.6737, 222.77421, 222.83036, 222.98891999999995, 223.031455, 223.0774, 223.12174, 223.182645, 223.30346150000003, 223.473045, 223.54464000000002, 223.58587999999997, 223.64424000000002, 223.7908, 223.85341, 223.97542999999996, 224.00202000000002, 224.14463, 224.22458, 224.27967999999998, 224.2851, 224.28893, 224.31126, 224.416085, 224.43364499999998, 224.47125999999997, 224.5813, 224.68205, 224.7074, 224.747595, 224.786345, 224.845135, 224.87384, 224.88209, 224.89707499999997, 224.906425, 224.93721000000002, 224.99891, 225.11993, 225.20112999999998, 225.23781000000002, 225.4257, 225.49275, 225.54500000000002, 225.571, 225.608155, 225.628655, 225.69450999999998, 225.85385000000002, 226.08612, 226.15296999999998, 226.26185999999998, 226.30773, 226.31929000000002, 226.337965, 226.366255, 226.41583999999997, 226.59959999999998, 226.6264, 226.63189999999997, 226.74904500000002, 226.831, 226.91215, 226.96820000000002, 227.01344999999998, 227.05525, 227.1113, 227.19349999999997, 227.25560000000002, 227.26318, 227.27810499999998, 227.35067500000002, 227.43170000000003, 227.57735, 227.7095, 227.75536, 227.81887, 227.8843, 227.9784, 228.2556, 228.28267999999997, 228.31967, 228.35079000000002, 228.52486999999996, 228.6543, 228.674485, 228.69475, 228.74197499999997, 228.796405, 228.84312999999997, 228.91615000000002, 228.99445, 229.03134999999997, 229.04789999999997, 229.066325, 229.110475, 229.17067, 229.2439, 229.25687, 229.46390000000002, 229.48302, 229.700675, 229.79678, 229.866065, 229.8972, 229.92886000000001, 230.01945999999998, 230.10665, 230.1232, 230.13145, 230.187715, 230.24181500000003, 230.279, 230.321575, 230.331875, 230.3629, 230.40685000000002, 230.45050000000003, 230.48334, 230.49374, 230.5255, 230.56821, 230.68074, 230.87794000000002, 231.04322000000002, 231.06919499999998, 231.10727500000002, 231.13726000000003, 231.16215, 231.18507499999998, 231.35713499999997, 231.44556999999998, 231.4685, 231.52429999999998, 231.6239, 231.7435, 231.84855000000005, 231.86899500000004, 231.898365, 231.91391, 231.92444, 231.9851, 232.1583, 232.22595, 232.28623999999996, 232.29259000000002, 232.30294, 232.4102, 232.48025, 232.5306, 232.59925, 232.6304, 232.92598, 233.01515999999998, 233.07027999999997, 233.22668, 233.39065, 233.51285, 233.566455, 233.58073, 233.59543, 233.61890499999998, 233.64106999999998, 233.65861999999998, 233.817055, 233.894205, 233.9708, 233.99697999999998, 234.02237999999997, 234.06469999999996, 234.12825000000004, 234.23899999999998, 234.284805, 234.29360000000003, 234.30722000000003, 234.39787, 234.47904999999997, 234.4956, 234.58785, 234.67907499999998, 234.71887500000003, 234.832135, 234.902945, 234.96196, 234.98645, 235.09149000000002, 235.10287, 235.18534999999997, 235.49280000000005, 235.50279500000002, 235.54290500000002, 235.57751000000002, 235.61435, 235.651475, 235.65670500000002, 235.66658, 235.67475, 235.78075, 235.91098999999997, 235.97664999999998, 236.03425, 236.08929999999998, 236.093475, 236.12194, 236.143805, 236.174105, 236.20441, 236.24071, 236.37474, 236.46544500000005, 236.54736, 236.56897999999995, 236.59681999999998, 236.6262, 236.690455, 236.771265, 236.9777, 237.0479, 237.07801, 237.1394435, 237.19428349999998, 237.21095, 237.22431, 237.23325999999997, 237.25374999999997, 237.34350999999998, 237.40991, 237.6441, 237.76394, 237.92818499999998, 238.004935, 238.07666, 238.11283500000002, 238.28390000000002, 238.37330000000003, 238.44310000000002, 238.5233, 238.93130000000002, 239.03795000000002, 239.22247, 239.27927, 239.35174, 239.40093499999998, 239.503175, 239.59008, 239.80401999999998, 239.83339999999998, 239.86323, 240.014555, 240.07903, 240.13010500000001, 240.19443, 240.21724999999998, 240.23739999999998, 240.25844999999998, 240.460315, 240.48113, 240.60492, 240.823335, 240.84675, 241.00707, 241.04295000000002, 241.07150000000001, 241.1146, 241.13701999999998, 241.16406999999998, 241.269625, 241.28884999999997, 241.336295, 241.402945, 241.47242, 241.71187, 241.72386999999998, 241.76040000000003, 241.78325, 241.80327499999999, 241.90817500000003, 242.0412, 242.24885, 242.26245, 242.320935, 242.48874999999998, 242.57026000000002, 242.61672, 242.6651, 242.70228, 242.75481, 242.80930999999998, 242.83518, 242.95526999999998, 243.13887, 243.42015, 243.4736, 243.52373, 243.57003, 243.61344, 243.64524, 243.66824499999996, 243.67929499999997, 243.71886, 244.0688215, 244.14993, 244.21366999999998, 244.40947000000003, 244.45810000000003, 244.50829000000004, 244.56232, 244.73369499999998, 244.76928500000002, 244.82000000000002, 244.8749, 244.9081, 244.96595, 245.05056, 245.06629999999998, 245.1227, 245.18724999999995, 245.23577, 245.26657, 245.410635, 245.56679, 245.59897, 245.65178, 245.91039999999998, 245.9715, 246.15839999999997, 246.28874, 246.3779, 246.4433, 246.4761, 246.4993, 246.5305, 246.55831, 246.587955, 246.61324000000002, 246.74694999999997, 246.875565, 247.026315, 247.11631500000004, 247.15053, 247.19694499999997, 247.369405, 247.59640000000002, 247.71976, 247.75984999999997, 247.77114999999998, 247.79859, 247.88668999999996, 247.95552999999995, 247.96103, 248.01620000000003, 248.03286000000003, 248.11697000000004, 248.20597000000004, 248.371595, 248.523865, 248.70025, 248.72465, 248.83229500000002, 248.932185, 249.03594, 249.08605, 249.110285, 249.2174, 249.26543, 249.520955, 249.562455, 249.60674999999998, 249.72854999999998, 249.88920000000002, 249.98530000000005, 250.1003, 250.22052000000002, 250.24292000000003, 250.35095, 250.64990999999998, 250.65492, 250.68002, 250.74953, 250.85793, 251.106525, 251.12726, 251.23784999999998, 251.39010000000002, 251.4909, 251.6173865, 251.68055149999998, 251.69770499999998, 251.7495, 251.78250500000001, 251.89101, 252.05701, 252.26285000000001, 252.33693, 252.387745, 252.442115, 252.568035, 252.62547, 252.712535, 252.953615, 253.175675, 253.33163, 253.65176499999998, 253.69662999999997, 253.83588000000003, 253.95228000000003, 253.98408, 254.02044999999998, 254.04299999999995, 254.288265, 254.53605000000002, 254.60703, 254.6464, 254.702685, 254.78533500000003, 254.91877, 254.95509000000004, 255.08242, 255.25145, 255.34544999999997, 255.39845499999996, 255.438755, 255.47525000000002, 255.5319, 255.699135, 255.72457000000003, 255.7332, 255.78879999999998, 255.84734999999998, 255.86319999999998, 255.93779999999998, 256.04909999999995, 256.07874, 256.185665, 256.28690500000005, 256.434355, 256.6249, 256.70498999999995, 256.806805, 256.9827, 257.0986, 257.21119, 257.29645, 257.34625, 257.39525000000003, 257.406475, 257.424305, 257.43798000000004, 257.6231, 257.6706, 257.83259999999996, 257.9563, 257.97994, 258.00150499999995, 258.04866499999997, 258.10454999999996, 258.12725, 258.161, 258.2017, 258.25789, 258.272805, 258.348515, 258.50399000000004, 258.59556499999997, 259.10549349999997, 259.145415, 259.1858, 259.26577, 259.27363, 259.35699999999997, 259.37885, 259.44855, 259.58119999999997, 259.71047, 259.79987000000006, 259.90128500000003, 259.963035, 259.98763499999995, 260.02488999999997, 260.19494, 260.23902, 260.38661, 260.5095, 260.6463, 260.82307499999996, 260.89955, 261.05192, 261.47245999999996, 261.55625, 261.686395, 261.76839, 261.89449, 262.12159999999994, 262.15062, 262.291995, 262.30971, 262.45293, 262.63919, 263.1597, 263.29457, 263.39605, 263.49398, 263.53767000000005, 263.58892000000003, 263.63647000000003, 263.669775, 263.80170999999996, 263.81714999999997, 263.84154, 264.0416, 264.10668499999997, 264.252685, 264.58144500000003, 264.66324999999995, 264.77598, 264.82778, 264.8347, 264.84271, 264.84833, 264.88131, 264.91184, 264.941335, 265.0919, 265.2671, 265.42362, 265.45056999999997, 265.48762, 265.59979, 265.684, 265.776725, 265.80647, 265.88922, 265.97892, 266.02566, 266.11689, 266.25104999999996, 266.53585, 266.75421, 266.80235, 267.17032500000005, 267.62247, 267.65754, 267.6631365, 267.79380000000003, 267.95867499999997, 268.091325, 268.16155000000003, 268.68168000000003, 268.93479, 269.0756, 269.37537999999995, 269.4873, 269.5448, 269.63563, 269.65753, 269.69050000000004, 269.73292499999997, 269.79328499999997, 269.84636, 269.9063, 269.96130000000005, 270.01645, 270.11310999999995, 270.16815999999994, 270.33187, 270.6417, 270.77689499999997, 270.9321, 271.050565, 271.59576500000003, 271.67556, 271.7193, 271.75866, 271.9076299999999, 272.0689500000001, 272.110135, 272.1454349999999, 272.3556, 272.7516800000001, 272.816585, 272.86405, 273.01056, 273.26095999999995, 273.310835, 273.379585, 273.53472999999997, 273.70110500000004, 273.78915500000005, 273.9778500000001, 274.09691000000004, 274.18006, 274.37649999999996, 274.54503, 274.8918, 274.9906, 275.04970000000003, 275.13267500000006, 275.175245, 275.20998, 275.4299, 275.79909999999995, 275.85839999999996, 275.91933, 275.974225, 276.10894499999995, 276.29247, 276.47591, 276.56025, 276.59775, 276.61275, 276.63059499999997, 276.673085, 276.767805, 276.84709999999995, 276.93719, 276.99618999999996, 277.00115, 277.01175, 277.04155000000003, 277.12928999999997, 277.27833, 277.29449, 277.452775, 277.52360999999996, 277.5472, 277.59375, 277.68665, 277.71308, 278.087095, 278.100495, 278.13817, 278.906755, 278.996135, 279.00973, 279.023875, 279.07230000000004, 279.33331999999996, 279.36206999999996, 279.37188, 279.38001, 279.5685, 280.36749999999995, 280.41900000000004, 280.45879, 280.50859, 280.6111000000001, 280.71764999999994, 281.18085499999995, 281.34273999999994, 281.43588000000005, 282.231715, 282.29941499999995, 282.3825, 282.41826000000003, 282.55691, 282.75350000000003, 282.8366, 282.930655, 283.262375, 283.37555000000003, 283.45193000000006, 283.78152, 283.9257, 283.973, 284.0077, 284.0421, 284.04388, 284.0897, 284.41035, 284.543005, 284.69268999999997, 284.9575, 285.78423, 285.97121, 286.0833, 286.3317800000001, 286.82649000000004, 286.85908000000006, 286.8842, 286.96495, 287.04189999999994, 287.20673999999997, 287.36569, 287.694365, 287.88023499999997, 288.12795, 288.4919, 288.6675, 288.815875, 288.888685, 289.06565, 289.3932, 289.60548, 290.1293, 290.30920000000003, 290.44096, 290.60168000000004, 290.91822, 291.1476, 291.22365, 291.226975, 291.48134500000003, 291.810355, 292.072325, 292.277285, 292.32257500000003, 292.476785, 292.63119, 292.940205, 293.05493000000007, 293.09832500000005, 293.15899999999993, 293.21304, 293.29569000000004, 293.4512000000001, 293.610965, 293.72659, 293.881975, 294.15975499999996, 294.30889, 294.7022, 294.94845, 294.96359, 294.97089000000005, 294.97654, 295.093, 295.32129, 295.432685, 295.57680000000005, 295.68215, 295.70608000000004, 295.76683, 295.89776500000005, 296.16206500000004, 296.3444, 296.66115, 296.68487, 296.78557, 297.38577499999997, 297.76446, 298.03281, 298.07424000000003, 298.69230000000005, 299.28648499999997, 299.367595, 299.40013, 299.484525, 300.07817, 300.43585, 300.65930000000003, 301.32325, 301.42904999999996, 301.642375, 301.97150999999997, 302.21031000000005, 302.40105000000005, 302.45709999999997, 302.50641999999993, 302.77266, 303.13679, 303.5562, 303.58540000000005, 303.58939999999996, 303.66810999999996, 303.89045, 304.70214999999996, 304.94285499999995, 305.120305, 305.34438, 306.8619500000001, 307.4909, 307.71799999999996, 307.97909999999996, 309.0473, 309.15205, 309.2967, 309.568305, 309.90757999999994, 310.166475, 310.47265, 310.95543999999995, 311.383345, 311.92739, 312.05955000000006, 312.18318, 312.35162499999996, 312.75665000000004, 313.2534, 314.98755, 315.38255000000004, 315.42068, 315.73643500000003, 316.30565, 316.39430000000004, 316.82925, 317.917325, 318.033825, 318.13734999999997, 318.49170000000004, 318.670615, 318.86185, 320.21254999999996, 320.73985, 321.47471, 322.10936000000004, 324.57944999999995, 324.90355, 325.00384999999994, 325.4924, 325.973, 327.07079999999996, 327.26211, 327.43003999999996, 327.94965, 329.02757, 329.35395, 329.7056, 330.014825, 330.52601500000003, 331.40403000000003, 332.41261000000003, 333.70404999999994, 335.7131350000001, 335.84280000000007, 335.9533, 336.15650000000005, 336.95740500000005, 339.065925, 339.866665, 340.53380000000004, 341.25477, 342.640575, 343.18577, 346.682685, 351.608675, 355.95134999999993, 359.64099999999996, 363.573525, 367.76942499999996, 370.076, 375.68958, 379.663505, 380.00530499999996, 393.60912499999995, 426.97816, 438.18281, 457.98935, 476.33050000000003])
labels = array([4.0, 1.0, 0.0, 5.0, 4.0, 1.0, 4.0, 1.0, 0.0, 1.0, 4.0, 5.0, 4.0, 0.0, 2.0, 4.0, 0.0, 4.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 4.0, 1.0, 4.0, 1.0, 0.0, 4.0, 0.0, 5.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 4.0, 1.0, 2.0, 1.0, 4.0, 1.0, 3.0, 0.0, 4.0, 0.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 4.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 0.0, 2.0, 0.0, 1.0, 4.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 2.0, 1.0, 3.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 4.0, 0.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 4.0, 2.0, 0.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 4.0, 0.0, 1.0, 0.0, 4.0, 2.0, 1.0, 2.0, 5.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 3.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 4.0, 0.0, 1.0, 0.0, 4.0, 2.0, 4.0, 2.0, 0.0, 2.0, 4.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 4.0, 1.0, 0.0, 2.0, 1.0, 3.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 3.0, 4.0, 2.0, 0.0, 2.0, 0.0, 1.0, 3.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 4.0, 0.0, 1.0, 0.0, 2.0, 0.0, 3.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 6.0, 0.0, 2.0, 0.0, 2.0, 3.0, 2.0, 0.0, 3.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 2.0, 1.0, 0.0, 1.0, 4.0, 2.0, 1.0, 0.0, 4.0, 2.0, 0.0, 2.0, 0.0, 4.0, 2.0, 0.0, 1.0, 4.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 3.0, 4.0, 3.0, 4.0, 0.0, 4.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 4.0, 0.0, 3.0, 0.0, 1.0, 0.0, 3.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 4.0, 0.0, 4.0, 1.0, 0.0, 5.0, 0.0, 1.0, 0.0, 2.0, 0.0, 5.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 4.0, 0.0, 1.0, 2.0, 0.0, 3.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 4.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 3.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 4.0, 3.0, 0.0, 1.0, 2.0, 0.0, 2.0, 4.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 4.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 4.0, 2.0, 0.0, 1.0, 4.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 4.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 3.0, 2.0, 1.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 4.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 2.0, 0.0, 3.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 4.0, 0.0, 4.0, 1.0, 2.0, 4.0, 0.0, 3.0, 0.0, 1.0, 0.0, 1.0, 2.0, 3.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 4.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 0.0, 2.0, 1.0, 2.0, 4.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 5.0, 1.0, 0.0, 1.0, 0.0, 3.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 4.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 4.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 3.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 4.0, 1.0, 0.0, 1.0, 0.0, 3.0, 1.0, 2.0, 0.0, 4.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 3.0, 1.0, 3.0, 4.0, 0.0, 2.0, 0.0, 4.0, 0.0, 2.0, 1.0, 0.0, 3.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, 0.0, 3.0, 2.0, 0.0, 4.0, 0.0, 2.0, 0.0, 3.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 3.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 5.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 3.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 3.0, 1.0, 0.0, 4.0, 0.0, 3.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 6.0, 1.0, 0.0, 2.0, 0.0, 1.0, 3.0, 2.0, 0.0, 1.0, 0.0, 3.0, 0.0, 3.0, 0.0, 4.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 3.0, 1.0, 0.0, 2.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 3.0, 0.0, 2.0, 3.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 3.0, 0.0, 5.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 4.0, 2.0, 0.0, 3.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 3.0, 0.0, 1.0, 0.0, 2.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 3.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 3.0, 4.0, 0.0, 1.0, 2.0, 0.0, 3.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 4.0, 2.0, 1.0, 0.0, 2.0, 0.0, 4.0, 0.0, 1.0, 0.0, 4.0, 0.0, 3.0, 0.0, 3.0, 0.0, 1.0, 0.0, 3.0, 1.0, 2.0, 0.0, 2.0, 3.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 4.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 3.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 3.0, 4.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 4.0, 0.0, 2.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.0, 0.0, 2.0, 0.0, 3.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 6.0, 0.0, 3.0, 2.0, 1.0, 3.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 3.0, 1.0, 0.0, 2.0, 0.0, 2.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 1.0, 0.0, 3.0, 0.0, 3.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 4.0, 0.0, 1.0, 0.0, 3.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 3.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 6.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 4.0, 0.0, 1.0, 3.0, 0.0, 2.0, 0.0, 3.0, 0.0, 2.0, 4.0, 2.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 3.0, 1.0, 0.0, 2.0, 0.0, 3.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 4.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 4.0, 1.0, 2.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 6.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 3.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 4.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 3.0, 2.0, 0.0, 1.0, 0.0, 3.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 3.0, 0.0, 2.0, 0.0, 3.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 3.0, 4.0, 0.0, 3.0, 0.0, 2.0, 3.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 3.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 3.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 1.0, 3.0, 4.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 4.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 3.0, 1.0, 4.0, 3.0, 1.0, 2.0, 0.0, 2.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 4.0, 0.0, 2.0, 1.0, 4.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 3.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 4.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 4.0, 3.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 4.0, 0.0, 1.0, 2.0, 3.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 2.0, 0.0, 3.0, 2.0, 4.0, 3.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 4.0, 0.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 4.0, 0.0, 1.0, 5.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 5.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 3.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 5.0, 2.0, 1.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 4.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 0.0, 1.0, 4.0, 1.0, 0.0, 4.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 2.0, 3.0, 0.0, 1.0, 0.0, 5.0, 1.0, 0.0, 4.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 4.0, 3.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 4.0, 1.0, 0.0, 1.0, 2.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 5.0, 1.0, 2.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 1.0, 3.0, 4.0, 1.0, 0.0, 4.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 4.0, 2.0, 0.0, 1.0, 5.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 3.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 1.0, 2.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 5.0, 1.0, 5.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 4.0, 5.0, 1.0, 5.0])
def eqenergy(rows):
    return np.sum(rows, axis=1)
def classify(rows):
    energys = eqenergy(rows)

    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = labels[numers[indys]]
        outputs[defaultindys] = 1.0
        return outputs
    return thresh_search(energys)

numthresholds = 2599



# Main method
model_cap = numthresholds


def Validate(file):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 0
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:, -1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    with open(preprocessedfile, 'r') as csvinput:
        dirtyreader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(dirtyreader, None) + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            print(str(','.join(str(j) for j in ([i for i in row]))) + ',' + str(get_key(int(outputs[k]), classmapping)))



#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()

    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile, classmapping)


    else:
        print("Classifier Type: Quick Clustering")
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
            print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
            print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
            print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
            if int(num_TP + num_FN) != 0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN + num_FP) != 0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP + num_FP) != 0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2 * num_TP + num_FP + num_FN) != 0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP + num_FN) != 0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP + num_FN + num_FP) != 0:
                print("Critical Success Index:             {:.2f}".format(TS))

        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            print("System Type:                        " + str(n_classes) + "-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            try:
                import numpy as np # For numpy see: http://numpy.org
                from numpy import array
            except:
                print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

            def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
                #check for numpy/scipy is imported
                try:
                    from scipy.sparse import coo_matrix #required for multiclass metrics
                except:
                    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                    sys.exit()
                # Compute confusion matrix to evaluate the accuracy of a classification.
                # By definition a confusion matrix :math:C is such that :math:C_{i, j}
                # is equal to the number of observations known to be in group :math:i and
                # predicted to be in group :math:j.
                # Thus in binary classification, the count of true negatives is
                # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
                # :math:C_{1,1} and false positives is :math:C_{0,1}.
                # Read more in the :ref:User Guide <confusion_matrix>.
                # Parameters
                # ----------
                # y_true : array-like of shape (n_samples,)
                # Ground truth (correct) target values.
                # y_pred : array-like of shape (n_samples,)
                # Estimated targets as returned by a classifier.
                # labels : array-like of shape (n_classes), default=None
                # List of labels to index the matrix. This may be used to reorder
                # or select a subset of labels.
                # If None is given, those that appear at least once
                # in y_true or y_pred are used in sorted order.
                # sample_weight : array-like of shape (n_samples,), default=None
                # Sample weights.
                # normalize : {'true', 'pred', 'all'}, default=None
                # Normalizes confusion matrix over the true (rows), predicted (columns)
                # conditions or all the population. If None, confusion matrix will not be
                # normalized.
                # Returns
                # -------
                # C : ndarray of shape (n_classes, n_classes)
                # Confusion matrix.
                # References
                # ----------
                if labels is None:
                    labels = np.array(list(set(list(y_true.astype('int')))))
                else:
                    labels = np.asarray(labels)
                    if np.all([l not in y_true for l in labels]):
                        raise ValueError("At least one label specified must be in y_true")


                if sample_weight is None:
                    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
                else:
                    sample_weight = np.asarray(sample_weight)
                if y_true.shape[0]!=y_pred.shape[0]:
                    raise ValueError("y_true and y_pred must be of the same length")

                if normalize not in ['true', 'pred', 'all', None]:
                    raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


                n_labels = labels.size
                label_to_ind = {y: x for x, y in enumerate(labels)}
                # convert yt, yp into index
                y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
                y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
                # intersect y_pred, y_true with labels, eliminate items not in labels
                ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
                y_pred = y_pred[ind]
                y_true = y_true[ind]
                # also eliminate weights of eliminated items
                sample_weight = sample_weight[ind]
                # Choose the accumulator dtype to always have high precision
                if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                    dtype = np.int64
                else:
                    dtype = np.float64
                cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


                with np.errstate(all='ignore'):
                    if normalize == 'true':
                        cm = cm / cm.sum(axis=1, keepdims=True)
                    elif normalize == 'pred':
                        cm = cm / cm.sum(axis=0, keepdims=True)
                    elif normalize == 'all':
                        cm = cm / cm.sum()
                    cm = np.nan_to_num(cm)
                return cm


            print("Confusion Matrix:")
            mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])



    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        os.remove(preprocessedfile)


