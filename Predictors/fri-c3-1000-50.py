#!/usr/bin/env python3
#
# This code is was produced by an alpha version of Brainome Daimensions(tm) and is 
# licensed under GNU GPL v2.0 or higher. For details, please see: 
# https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
#
#
# Output of Brainome Daimensions(tm) 0.93 Table Compiler v0.94.
# Invocation: btc https://www.openml.org/data/get_csv/53340/fri_c3_1000_50.arff -o Predictors/fri-c3-1000-50_NN.py -target binaryClass -stopat 91.30 -f NN -e 20 --yes
# Total compiler execution time: 0:34:18.77. Finished on: Apr-21-2020 21:29:24.
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                55.50%
Model accuracy:                     87.20% (872/1000 correct)
Improvement over best guess:        31.70% (of possible 44.5%)
Model capacity (MEC):               151 bits
Generalization ratio:               5.77 bits/bit
Model efficiency:                   0.20%/parameter
System behavior
True Negatives:                     50.00% (500/1000)
True Positives:                     37.20% (372/1000)
False Negatives:                    7.30% (73/1000)
False Positives:                    5.50% (55/1000)
True Pos. Rate/Sensitivity/Recall:  0.84
True Neg. Rate/Specificity:         0.90
Precision:                          0.87
F-1 Measure:                        0.85
False Negative Rate/Miss Rate:      0.16
Critical Success Index:             0.74

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "fri_c3_1000_50.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 50
n_classes = 2

mappings = []
list_of_cols_to_normalize = []

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values()))+1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize,mappings):
            if i>=data_arr.shape[1]:
                break
            col = data_arr[:,i]
            normcol = column_norm(col,mapping)
            data_arr[:,i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([0.022175622857142883, 0.0036852499999999503, -0.005283180000000001, -0.0019249528571428619, -0.001995547142857153, 0.0037802400000000996, 0.007525802857142856, -0.008515908571428542, -0.01142454428571429, -0.0010426585714285567, 0.016358760000000003, -0.027446664285714278, -0.010668357142857143, 0.00775760000000003, 0.0018326499999999856, -0.021154884285714298, -0.012006634285714333, -0.01661488142857144, 0.013651088571428573, 0.02583893285714289, -0.024322067142857177, -0.009777309999999996, 0.04119341714285715, 0.026224071428571392, 0.011600288571428585, 0.004649447142857141, 0.0381685785714286, -0.017457435714285725, -0.008749764285714287, -0.004558222857142798, -0.030300160000000003, 0.019716147142857205, 0.015430989999999988, -0.02001175428571429, -0.0015601299999999819, -0.013055274285714246, 0.03999874999999999, 0.009065452857142843, -0.028575758571428547, 0.033177969999999966, 0.011154088571428605, 0.00574312142857144, 0.025600442857142854, 0.02209588142857145, 0.031462518571428594, -0.0074560428571428744, -0.010529412857142878, 0.012369581428571426, 0.015114980000000014, 0.030198281428571436])
        components = np.array([array([ 0.44313315,  0.45870679,  0.49453476,  0.20734491,  0.5238623 ,
       -0.00063753,  0.05106885, -0.03747362, -0.0267655 , -0.02549984,
       -0.00794604, -0.02931921,  0.04193652, -0.02557265,  0.00958926,
       -0.00570367, -0.00147189, -0.01145874,  0.0194623 , -0.00691199,
        0.01055482, -0.02595283, -0.01264299,  0.0073141 ,  0.04361369,
        0.01260475,  0.03017907, -0.00079231, -0.04118939, -0.03381138,
       -0.01804917, -0.02413514, -0.0104153 , -0.00675706,  0.04455423,
        0.01505633,  0.04523125,  0.06658923,  0.02271859,  0.0392235 ,
       -0.02504732,  0.03780575, -0.01001907, -0.00309351,  0.00235512,
        0.02366426,  0.00772735,  0.0121469 , -0.01640845,  0.00237963]), array([ 0.3770507 ,  0.33926622, -0.23759194, -0.6040849 , -0.12817426,
        0.08261658, -0.08514498,  0.08385346, -0.068286  , -0.02969804,
       -0.16879888, -0.08255008, -0.05587029,  0.03757571,  0.07876796,
        0.14316031,  0.09500616,  0.1003088 , -0.05993008,  0.0449342 ,
       -0.03578273, -0.00337905,  0.14149751,  0.08803751, -0.01365639,
        0.01247724,  0.02707841,  0.10314248,  0.05047323, -0.06528294,
       -0.00891955, -0.00080583,  0.02744716,  0.16946441,  0.00309752,
       -0.0881677 , -0.09808555, -0.05046202, -0.01490469,  0.00174864,
        0.13207918,  0.12681952, -0.01114792,  0.03292038,  0.02987554,
        0.04270765,  0.16590565, -0.01200275, -0.12834668,  0.09019352]), array([-3.76571264e-02, -4.39187136e-02,  3.95625774e-02,  5.96184136e-02,
        2.37909610e-02,  1.00808976e-01, -2.48498393e-02,  1.16299427e-04,
       -2.93516519e-01,  3.50982210e-02,  1.86902863e-01, -1.41333182e-01,
       -1.86487950e-01,  1.70128168e-01,  1.13658215e-01, -8.45062155e-03,
       -2.07353156e-03,  3.69852040e-01, -1.62964696e-01, -2.09656735e-02,
        1.35200238e-01, -3.84420481e-02,  8.16521133e-02,  1.25749201e-01,
       -1.83829677e-01,  5.60226380e-02,  8.32311237e-02, -7.90120909e-02,
       -1.14379817e-01, -1.37033205e-01,  2.03144961e-01,  1.83767180e-01,
       -2.35012058e-01,  6.34618020e-02,  1.76275196e-01,  1.58770243e-01,
        2.41292149e-02, -2.55469228e-01,  1.12925617e-01,  2.17553701e-01,
       -2.43727926e-03,  1.94826339e-02,  1.90651733e-01, -8.69615328e-02,
       -8.77647250e-03,  9.62662292e-02,  7.95644033e-02, -9.00836845e-02,
        2.94768957e-01,  4.13886644e-02]), array([ 1.95838583e-02,  1.93021574e-02, -2.27545576e-02, -8.46456827e-02,
       -1.74463253e-02, -2.23624809e-01,  4.68304135e-02, -6.62091406e-02,
        2.81619378e-02, -9.07811112e-02, -3.53287814e-02, -8.58840119e-02,
       -2.49074802e-01,  8.34358525e-02,  2.60154817e-01, -1.20780111e-01,
       -2.09719246e-01, -2.03461863e-01,  1.74493760e-01, -2.27859513e-01,
       -5.15506830e-02, -9.80405720e-02,  5.22876837e-02, -3.25398593e-02,
       -4.29184369e-02,  9.89288161e-02,  3.68789137e-01,  1.35424544e-01,
       -4.84432379e-02,  1.47469875e-01, -3.63107764e-01, -3.57480392e-02,
       -4.10865244e-02, -1.91642641e-01,  9.89590063e-02, -1.00917231e-02,
        1.34917506e-02, -8.47695013e-02,  9.06504579e-02, -9.95738273e-02,
        6.13976207e-02,  1.98289801e-01,  4.93853938e-05, -9.93332788e-02,
        5.16199388e-02, -1.53237902e-01,  7.45880842e-02, -4.67118526e-02,
        2.83938981e-01, -1.32151097e-01]), array([ 2.55818426e-02,  2.94169241e-02, -1.09056352e-02, -7.24239636e-02,
        7.16300727e-03,  6.36605891e-03,  1.16803302e-01,  1.01148404e-01,
       -1.00360751e-01, -2.63332356e-02, -1.80259732e-01,  3.05247776e-02,
       -7.70851942e-02,  2.86992925e-02,  9.35706887e-02, -4.22723728e-03,
        1.83157609e-01,  1.47400707e-01,  3.66428714e-03,  8.85261827e-02,
       -2.19945067e-01,  2.38797638e-01, -1.71127552e-01, -1.97711375e-01,
       -6.38028777e-02, -2.82295709e-01,  1.17606764e-01, -9.54475554e-02,
        9.80468870e-02,  6.93468362e-02, -3.19450341e-02, -1.11717599e-01,
        6.37322749e-02, -1.32603914e-01,  3.73950598e-01,  1.12553013e-01,
        1.71184159e-01, -1.79039390e-01, -2.76692978e-01,  2.84180037e-04,
       -5.17645382e-02, -1.56328103e-01,  2.41096545e-02,  1.72189848e-01,
       -7.99901998e-02,  6.98234978e-02, -8.58483885e-02,  2.50362600e-01,
        5.88157121e-02, -2.89293294e-01]), array([ 0.06600744,  0.0868876 , -0.03778387, -0.15008811, -0.03073614,
       -0.23562134, -0.14569429, -0.16113572,  0.03331674, -0.14624704,
        0.02693949,  0.30484741, -0.05361884, -0.07426008, -0.18100733,
       -0.01828566, -0.17737353, -0.01822229,  0.06985472, -0.08685419,
        0.19765652, -0.09701378,  0.24696545, -0.02163678, -0.04277925,
        0.03181646, -0.02076557, -0.03925652,  0.07147334, -0.12180399,
        0.13972451,  0.34223153,  0.2357918 , -0.30309423,  0.10713704,
        0.03604   ,  0.26108809, -0.17764045, -0.06811371,  0.08462402,
       -0.27705413, -0.08838195,  0.00307411, -0.0423328 ,  0.05398394,
        0.05083871, -0.04054294,  0.05210815, -0.13861351, -0.00853068]), array([-0.07344829, -0.06301402,  0.04644828,  0.11254764,  0.02586578,
       -0.27021839,  0.24557959,  0.11121739, -0.19292311, -0.01634909,
       -0.23214288, -0.04175773,  0.16006952,  0.14436068, -0.16051454,
       -0.14212756,  0.02373368, -0.04553835, -0.28445942, -0.01644545,
       -0.00432044, -0.10841145,  0.01499112,  0.05949995, -0.0883362 ,
       -0.10133237, -0.09676753,  0.14837491,  0.02617008, -0.02484673,
       -0.07023915,  0.10334467,  0.28357529,  0.11339206,  0.1222384 ,
        0.10492204, -0.01063972, -0.14263779, -0.08268281, -0.01460303,
        0.05149262,  0.16319373, -0.22027868,  0.20166634,  0.02619134,
       -0.04227881,  0.2078363 , -0.40738364, -0.03621826,  0.00427955]), array([-0.02030758, -0.01528846,  0.00374647, -0.01016852,  0.00928368,
       -0.07952788,  0.10244508, -0.34833395, -0.06525077,  0.01892709,
        0.11740108, -0.0607607 ,  0.1018985 ,  0.09846052,  0.17383123,
       -0.24870605,  0.00812646,  0.10016664, -0.25443294, -0.05743988,
        0.07973039, -0.05294031, -0.09041848, -0.27621893, -0.16880856,
       -0.00121756,  0.01135226, -0.01594507,  0.25223712,  0.1256873 ,
        0.07806287, -0.03195146, -0.07230356, -0.06886166,  0.06443005,
       -0.42054211, -0.04159047,  0.00626533,  0.11122225, -0.08538604,
       -0.04845778,  0.09156964, -0.15059681,  0.01010894, -0.0897672 ,
        0.16988732,  0.16663772,  0.25561493, -0.15146873,  0.20168983]), array([ 0.03655161,  0.03004805, -0.01128139, -0.03391303, -0.02001696,
        0.00158795, -0.12039448,  0.22111693, -0.20414954,  0.04895938,
       -0.08114856, -0.10214736,  0.06561356, -0.12034111,  0.00576864,
       -0.26487243, -0.32616659, -0.0577285 , -0.16187251,  0.08972615,
        0.06918018, -0.03339894,  0.23970415, -0.04571146, -0.27428424,
        0.02904102,  0.1593408 , -0.14662107, -0.1036853 ,  0.29977308,
        0.13052044, -0.15902775,  0.02831448,  0.03780901,  0.04139571,
        0.11015591, -0.16203451,  0.22602996, -0.07595721, -0.00640708,
       -0.00043579, -0.18311459, -0.09801822, -0.19283394,  0.16036117,
        0.16139711, -0.18435386, -0.02519214, -0.13868428, -0.14873712]), array([ 4.76211463e-02,  4.37289287e-02, -1.45387564e-02, -3.48632067e-02,
       -8.56135697e-03,  1.43710258e-01,  1.32073445e-01, -3.15468803e-02,
       -3.47263300e-02,  8.29491358e-02,  1.06347478e-01,  1.03339474e-01,
       -1.85183872e-01,  8.65047820e-02,  1.33022112e-01, -6.78841785e-03,
       -1.06276281e-01, -2.14227259e-01, -2.09511665e-01,  6.75941823e-02,
       -2.17473067e-01,  1.74972948e-01, -9.10043553e-02, -5.66453721e-02,
        2.39174913e-01, -1.09705061e-01,  1.64086185e-01, -3.54661144e-02,
       -3.96130297e-02, -7.94957023e-05,  2.00319965e-01,  8.05992040e-02,
        1.79570425e-01, -8.54596999e-02, -1.74955973e-01,  8.08660398e-02,
        2.17550385e-01, -1.79437558e-01,  2.55271185e-01, -2.12585916e-01,
        2.00492367e-01, -1.34621517e-01, -1.27695195e-01, -5.49671498e-02,
        1.16878874e-01,  2.43711417e-01, -1.66025459e-01, -1.94725881e-01,
        8.52846178e-03,  2.05254185e-01]), array([ 0.0480449 ,  0.01198076, -0.01036646,  0.02697764, -0.01603895,
       -0.00269985, -0.13589501,  0.0629409 ,  0.17946496,  0.30317783,
        0.03897897, -0.28631316,  0.03251526, -0.0708464 ,  0.01980099,
       -0.07762037,  0.05090265, -0.08406629, -0.12122168,  0.10980299,
        0.13173803,  0.0822309 ,  0.25669779,  0.27224472,  0.04977021,
       -0.02833173, -0.04033704, -0.42090629,  0.115703  ,  0.01587303,
       -0.14057237, -0.08465544,  0.00951706, -0.22204787,  0.09577452,
        0.03001119,  0.12314825, -0.05657678, -0.01618346, -0.33312091,
       -0.12128188,  0.02684176,  0.09303162,  0.23300966, -0.06257507,
       -0.04646699,  0.13731087, -0.03343271,  0.09667965,  0.16701841]), array([-0.01221509, -0.01705154,  0.04872849,  0.10085946,  0.03075593,
       -0.10346477, -0.2481445 ,  0.25414534, -0.02505971, -0.00353295,
        0.1755041 ,  0.10662929, -0.09581763, -0.13808557, -0.095101  ,
        0.28583861,  0.01063782,  0.03041939, -0.06059904,  0.02623272,
        0.16309755,  0.09901201, -0.0814017 ,  0.10117795, -0.09748607,
       -0.05345551,  0.10384868,  0.09072092,  0.30293055,  0.25134715,
       -0.02568499, -0.05382727,  0.04516506,  0.01972531,  0.02848008,
        0.070059  ,  0.13490103, -0.09146079, -0.08386883,  0.03352426,
        0.24453227,  0.1187189 , -0.32212713, -0.29351205, -0.28956511,
       -0.0592297 ,  0.07536869,  0.0804115 ,  0.03801337,  0.17670273]), array([ 0.01815331,  0.04171914, -0.02126161, -0.05049761, -0.04460317,
        0.15066494, -0.08922689, -0.15274076,  0.04890793,  0.12982387,
       -0.01869112, -0.00606241, -0.04842904, -0.09758628, -0.05671715,
       -0.28251349,  0.13291171,  0.11441904,  0.02115711,  0.1117891 ,
        0.10884045, -0.06513323,  0.03533648, -0.00673949,  0.29046968,
       -0.14830636,  0.07790661,  0.36107062, -0.10280033,  0.05234456,
        0.2763245 , -0.01507835, -0.00042293, -0.1027142 , -0.04209888,
        0.15896733, -0.02875664,  0.16815702, -0.05498338, -0.16836391,
       -0.1970415 , -0.00328887, -0.21407269, -0.11904129, -0.22130962,
        0.01562563,  0.28643674, -0.05945043,  0.22712404, -0.2175861 ]), array([-0.02571226, -0.03385822,  0.00395785,  0.06653965, -0.00497767,
        0.07987139,  0.0006287 ,  0.02403176,  0.06008496,  0.23081287,
       -0.13907948, -0.24321213, -0.07480885,  0.21007951,  0.08343354,
        0.06173716, -0.03801543, -0.08771023, -0.04150405,  0.32282024,
        0.03079383, -0.26956047, -0.13849578, -0.01283504,  0.0621488 ,
        0.11903257, -0.08288339,  0.14323884,  0.31257418, -0.06325476,
       -0.04970654, -0.0562185 ,  0.25507615, -0.11251769, -0.04155141,
        0.12794048,  0.13027645,  0.02752835, -0.01683755,  0.25475775,
       -0.04873637,  0.00370221,  0.1227476 , -0.25524967,  0.32471275,
        0.04595604,  0.12852305,  0.2073476 , -0.01209722, -0.03937596]), array([-0.05644653, -0.06000721,  0.04018449,  0.0784334 ,  0.02690027,
       -0.15938463,  0.00631505,  0.07410186, -0.10125891,  0.16973802,
       -0.06257079, -0.18531109, -0.23380502, -0.26261777,  0.20246398,
        0.06248591,  0.01975554, -0.03016408,  0.00953793, -0.26339045,
        0.12177993, -0.03413498,  0.17584207, -0.13328323,  0.06093631,
       -0.25017718,  0.01066548,  0.14682406, -0.0476598 , -0.31946682,
        0.24023871, -0.20258013,  0.04277353,  0.01251534,  0.05419266,
       -0.09159162,  0.0185158 , -0.01133533,  0.03277188,  0.03968104,
        0.13647468, -0.23135301, -0.02168887,  0.03927538,  0.1063033 ,
       -0.35007018,  0.05306321,  0.07869993, -0.19025942,  0.10855741]), array([ 0.01448263,  0.01954101, -0.01791446, -0.05757532, -0.01638162,
       -0.14543517,  0.29519719, -0.06661556, -0.18064093, -0.0870922 ,
        0.03643173, -0.13996596, -0.09950634, -0.38310823,  0.06598983,
       -0.06500707, -0.41361477,  0.03598139,  0.09953622,  0.25113318,
        0.0054698 ,  0.10646852, -0.24871713,  0.17035599,  0.08819044,
        0.06385899, -0.06921141,  0.03204975,  0.18996793, -0.18754642,
       -0.02493406,  0.11224437, -0.01360161,  0.17025609, -0.01399312,
        0.07719732, -0.19259958, -0.07230013, -0.15880289, -0.1809797 ,
       -0.16399858, -0.01862818,  0.03095602, -0.03973021, -0.10876537,
       -0.00902049, -0.05409143,  0.08545312,  0.0610336 ,  0.12398317]), array([ 0.0296294 ,  0.02630926, -0.01947607,  0.00898847, -0.01892931,
        0.01507377,  0.04592934,  0.00201217, -0.001796  , -0.20125253,
       -0.05251071, -0.10583368, -0.1162357 , -0.26751795, -0.18423185,
       -0.08324606,  0.11403912,  0.08636695, -0.05532359,  0.12111867,
        0.04094911, -0.10100038, -0.01395091, -0.27162397, -0.04693108,
        0.12028334, -0.09511524,  0.00648736,  0.06457889,  0.30915474,
        0.03303283,  0.24780249,  0.03763755,  0.13628472,  0.05738951,
        0.17645045,  0.21747183,  0.10475894,  0.35065977, -0.21224114,
        0.24006839, -0.04041576,  0.28014684,  0.11339932,  0.0446834 ,
       -0.23345375,  0.07604151,  0.08841433,  0.00207674, -0.03746886]), array([-0.01919591, -0.02039971,  0.00570724, -0.01351512,  0.01572844,
        0.13519022, -0.29293959,  0.15223097, -0.20419446, -0.10356358,
       -0.11729976,  0.03020239,  0.15285651,  0.09892404,  0.14328805,
       -0.14129623, -0.1958428 ,  0.07339688,  0.24149665, -0.0326093 ,
       -0.10538271, -0.14018059, -0.17339455, -0.01780249, -0.1025391 ,
       -0.03252921, -0.03441674,  0.20450744,  0.06874979, -0.02480014,
        0.02764706,  0.02141875, -0.11910537, -0.24103198, -0.02409082,
        0.10531348,  0.19918089,  0.22572688, -0.12166356, -0.04254304,
        0.06900726, -0.08799109,  0.0474258 ,  0.31350255, -0.02513884,
        0.01995368,  0.02768289, -0.08048815,  0.05494495,  0.43568033]), array([ 0.01890702,  0.01610526, -0.02589611, -0.00520603, -0.03123317,
        0.07193175,  0.12109397,  0.13565047, -0.27873195,  0.1438857 ,
        0.15924952,  0.00265563, -0.16596156, -0.03511554, -0.29229604,
        0.12592715,  0.02445514,  0.06250669,  0.03871304, -0.03861895,
       -0.00580786,  0.17583586, -0.07111979, -0.06973563, -0.29919328,
        0.29783499,  0.03631984,  0.11886349, -0.2231983 , -0.12845441,
       -0.07128584, -0.23887102,  0.20562519, -0.05416671, -0.17522287,
       -0.15126576,  0.24060315,  0.10800235,  0.05263175, -0.16718141,
       -0.26089751,  0.06751926, -0.07979648,  0.11136357,  0.09387415,
        0.03684926,  0.12950872,  0.0993976 ,  0.08613169, -0.02072137]), array([ 0.02228211,  0.0784179 , -0.00938491,  0.0220992 , -0.00517961,
       -0.28500409, -0.27291869, -0.08359837, -0.25594541,  0.05040986,
       -0.10294292, -0.07290133, -0.12357112,  0.12818338, -0.17164417,
       -0.12940867,  0.09922288, -0.30498149, -0.0574748 ,  0.07306759,
       -0.13152853,  0.18786933, -0.08504496,  0.04839833,  0.04868294,
        0.25511653, -0.31806257,  0.10609142, -0.00943777, -0.00807223,
        0.06230331, -0.03917502, -0.3490488 , -0.13223566, -0.01913648,
       -0.09923621,  0.00489496, -0.18154589, -0.06018481, -0.06454995,
        0.14770445, -0.20218381,  0.07772159, -0.11667863, -0.07734966,
        0.00687394,  0.05333967, -0.03245178, -0.06528894, -0.14905463]), array([-3.14613522e-02, -3.97418406e-03,  3.14630978e-02,  4.63915144e-02,
        1.69465773e-02,  1.19289215e-01,  5.26851715e-02,  5.80555191e-02,
       -7.34176175e-02, -3.05716586e-01,  1.61585780e-01, -6.71433899e-02,
       -1.52463756e-01, -1.10396847e-02,  2.70633987e-01, -1.56800141e-01,
        2.01675579e-01,  6.45063551e-02,  1.90917003e-01, -5.98122207e-03,
       -2.42022483e-01,  3.83560529e-04,  1.56517857e-01,  2.65845319e-01,
       -5.55120231e-02,  4.92272495e-02, -2.33806237e-01, -1.59506646e-01,
        1.22260802e-01,  1.32790992e-01, -9.42872465e-03,  1.30145894e-01,
        4.64929401e-02,  3.12837988e-02, -1.50697267e-01, -7.00386147e-02,
        7.49479135e-02, -6.53428822e-03, -1.86221862e-02,  5.42414276e-02,
       -9.30755304e-02, -1.90877333e-01, -4.08764857e-01, -1.23607080e-02,
        2.13134339e-01, -1.29703790e-01,  1.71795625e-01,  1.13329683e-02,
       -2.69325623e-02, -9.78559724e-02]), array([ 0.08025941,  0.07741626, -0.03648342, -0.10635337,  0.00622192,
       -0.06697924, -0.03997627,  0.12155157,  0.06385904, -0.14819511,
        0.23720042,  0.07955427, -0.01331355,  0.12728883, -0.22311725,
       -0.01039596, -0.04130882, -0.07858695, -0.31919916,  0.15219793,
       -0.23333055,  0.0055652 ,  0.00135076, -0.24923187, -0.05501346,
       -0.2321706 ,  0.01133136, -0.0730999 ,  0.1361429 , -0.15963638,
       -0.01420833, -0.02837873, -0.17230159, -0.10933045, -0.04153242,
        0.03427246, -0.18793615,  0.13940684,  0.01042618,  0.0612487 ,
       -0.22014412, -0.04125822, -0.03148797, -0.08403606,  0.14754951,
       -0.41353965,  0.00658978, -0.03782042,  0.26317305,  0.13143415]), array([-0.03613725, -0.02414411, -0.04462245, -0.02715358, -0.04702204,
       -0.00651904,  0.03068373,  0.01350066, -0.14339795, -0.19739035,
        0.30488855, -0.09504129,  0.00444317, -0.19342336, -0.10874052,
        0.19440512,  0.0055784 , -0.13678697, -0.08860838,  0.00354015,
        0.00154837, -0.31735418, -0.00087756,  0.00686057,  0.35083685,
       -0.01372159, -0.07821283,  0.01219978, -0.10746117,  0.04730469,
        0.01531054, -0.1616641 , -0.23622402, -0.05572994,  0.34234938,
       -0.01238222,  0.12039671,  0.0564386 , -0.12613508, -0.00797034,
        0.04273656,  0.203384  , -0.13296444,  0.14090817,  0.29309311,
        0.25902349, -0.00984532,  0.03973661, -0.01241633, -0.03003736]), array([ 0.04557   ,  0.05112757, -0.06576258, -0.13465767, -0.05402881,
       -0.14932704,  0.11424098, -0.27377231, -0.07424112, -0.03674655,
        0.10898281, -0.21882528,  0.28085055,  0.06506966,  0.14351143,
        0.15112279,  0.08033547, -0.084081  , -0.14259468,  0.1982039 ,
        0.12426668, -0.16772698,  0.00683485,  0.04521119, -0.0585871 ,
        0.06519034,  0.07462124,  0.03274168, -0.26898592,  0.04397159,
       -0.10074218, -0.0277855 ,  0.02057495,  0.03444352, -0.15068858,
        0.09174036,  0.29605403,  0.02859162, -0.08396749,  0.1435202 ,
        0.02954105, -0.2932382 , -0.16697934,  0.05972126, -0.25616309,
       -0.21116403, -0.23333238,  0.06672509,  0.10600933,  0.0250015 ]), array([-0.04456324, -0.05262697,  0.00734843,  0.06182877,  0.0008515 ,
       -0.14609391, -0.15962035, -0.05091422,  0.1584912 , -0.36015682,
       -0.21407012,  0.05321637, -0.03367623, -0.1171242 ,  0.01391158,
       -0.04268528,  0.06238386,  0.04151814, -0.04409458,  0.2028478 ,
        0.05073267,  0.13689945, -0.11894392, -0.00257732,  0.12252091,
        0.13774801,  0.21962941, -0.08817187, -0.23303548,  0.07458113,
       -0.00279676, -0.33616095,  0.07493272,  0.08662643,  0.0488398 ,
        0.01374688, -0.0432584 , -0.16409335,  0.11296853,  0.20928128,
       -0.18170862, -0.17086318,  0.03539365,  0.01370128,  0.08860501,
        0.08237375,  0.31625108, -0.05643177, -0.01197612,  0.30307964]), array([-0.03800439, -0.03986602,  0.01892931,  0.03800349,  0.01162208,
        0.17743301, -0.18836672, -0.15660435,  0.17887141,  0.0316863 ,
       -0.06824111, -0.30210309, -0.00761523, -0.03100768, -0.20683454,
        0.07931746, -0.22556195,  0.01001218, -0.16099579, -0.24329179,
       -0.44132693, -0.0893169 , -0.1434858 ,  0.13771557, -0.07927923,
       -0.04872786,  0.19754571,  0.01563006, -0.09005795,  0.00581196,
        0.0187244 ,  0.15282554, -0.07226876,  0.21003164,  0.08889262,
       -0.01794749,  0.22456877, -0.04993933, -0.09784576, -0.04806569,
       -0.17792527, -0.01955228, -0.02266205, -0.11488442, -0.07515535,
       -0.08978091,  0.12271555,  0.02341991, -0.26139876, -0.0243055 ]), array([ 0.03757778,  0.04680631, -0.02484189, -0.03311773, -0.00924187,
        0.25806397, -0.02440365,  0.18366647,  0.14133223, -0.06024444,
       -0.09920994, -0.01785261,  0.08761389,  0.10920678, -0.1035054 ,
       -0.08219281, -0.08981535, -0.20885905, -0.05926554, -0.29319508,
        0.18747972, -0.08712151, -0.07916152,  0.00293959,  0.0374235 ,
        0.22868749, -0.10879639,  0.07227578,  0.12960167, -0.11916975,
        0.01244504, -0.0444838 ,  0.04179731,  0.18858394,  0.25377791,
        0.03944077, -0.11601128, -0.18746159,  0.21901178, -0.05626943,
       -0.06473251, -0.30278098, -0.260491  ,  0.11948803, -0.06429793,
        0.0209906 , -0.10970433,  0.23103671,  0.27887315, -0.00233878]), array([-0.04538359, -0.02921995,  0.04402435,  0.11591108,  0.02351491,
        0.10669927, -0.10946942,  0.21035766,  0.08617656, -0.22620857,
       -0.22353202,  0.03002222, -0.2661484 , -0.15140787,  0.07635352,
       -0.09566296,  0.12709403,  0.07125126, -0.34439492,  0.15001305,
       -0.02432972, -0.30590801,  0.0022741 ,  0.06609395, -0.02676667,
        0.06253134,  0.01257639,  0.02665712, -0.15462585, -0.20635089,
       -0.08859988,  0.11017381,  0.09633754, -0.15182093, -0.11400264,
       -0.36822361, -0.02011149, -0.01075817, -0.15875305, -0.11347977,
        0.06844256,  0.07538759,  0.03401605, -0.03944079, -0.1757599 ,
        0.06204579, -0.24940421,  0.06143659,  0.09285867, -0.02321182])])
        whiten = False
        explained_variance = np.array([3.286225238442112, 1.5468320953717563, 1.4833496068249756, 1.4524045172342572, 1.4030456885030997, 1.387968628466133, 1.3320671723679383, 1.3195740923400254, 1.2824261437462272, 1.2582962685897843, 1.2430977274661088, 1.2070850976666194, 1.1826233534214783, 1.1709487109086119, 1.1675804022997611, 1.1532194009826482, 1.1158001619492541, 1.0860172871739964, 1.0731846981154367, 1.0576920756143329, 1.0437215799335038, 1.032570936006519, 1.0081861012771598, 0.9950005079552806, 0.9769594636027978, 0.9521894017098396, 0.9453645386572771, 0.9259112495928908])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    il=[]
    
    ignorelabels=[]
    ignorecolumns=[]
    target="binaryClass"


    if (testfile):
        target=''
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless==False):
                header=next(reader, None)
                try:
                    if (target!=''): 
                        hc=header.index(target)
                    else:
                        hc=len(header)-1
                        target=header[hc]
                except:
                    raise NameError("Target '"+target+"' not found! Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=header.index(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute '"+ignorecolumns[i]+"' is the target. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '"+ignorecolumns[i]+"' not found in header. Header must be same as in file passed to btc.")
                for i in range(0,len(header)):      
                    if (i==hc):
                        continue
                    if (i in il):
                        continue
                    print(header[i]+",", end = '', file=outputfile)
                print(header[hc],file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if (row[target] in ignorelabels):
                        continue
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name==target):
                            continue
                        if (',' in row[name]):
                            print ('"'+row[name]+'"'+",",end = '', file=outputfile)
                        else:
                            print (row[name]+",",end = '', file=outputfile)
                    print (row[target], file=outputfile)

            else:
                try:
                    if (target!=""): 
                        hc=int(target)
                    else:
                        hc=-1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=int(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute "+str(col)+" is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    if (hc==-1):
                        hc=len(row)-1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0,len(row)):
                        if (i in il):
                            continue
                        if (i==hc):
                            continue
                        if (',' in row[i]):
                            print ('"'+row[i]+'"'+",",end = '', file=outputfile)
                        else:
                            print(row[i]+",",end = '', file=outputfile)
                    print (row[hc], file=outputfile)

def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'N': 0, 'P': 1}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    # function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping

# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)
# Classifier
def classify(row):
    #inits
    x=row
    o=[0]*num_output_logits


    #Nueron Equations
    h_0 = max((((19.21986 * float(x[0]))+ (14.219352 * float(x[1]))+ (-2.9013531 * float(x[2]))+ (-0.54644305 * float(x[3]))+ (1.1743423 * float(x[4]))+ (2.5896468 * float(x[5]))+ (1.0691513 * float(x[6]))+ (-0.9331146 * float(x[7]))+ (2.0563312 * float(x[8]))+ (2.4018562 * float(x[9]))+ (-1.4720101 * float(x[10]))+ (-1.6815165 * float(x[11]))+ (4.510259 * float(x[12]))+ (1.0570924 * float(x[13]))+ (-4.8561244 * float(x[14]))+ (-1.2770844 * float(x[15]))+ (1.5209087 * float(x[16]))+ (-0.7824413 * float(x[17]))+ (-0.39346832 * float(x[18]))+ (0.11610445 * float(x[19]))+ (1.2096568 * float(x[20]))+ (5.7996554 * float(x[21]))+ (-4.8334527 * float(x[22]))+ (2.5830238 * float(x[23]))+ (2.0085378 * float(x[24]))+ (-3.2747955 * float(x[25]))+ (-0.2760634 * float(x[26]))+ (-3.6316605 * float(x[27]))) + -0.5039454), 0)
    h_1 = max((((-8.740117 * float(x[0]))+ (-3.7393055 * float(x[1]))+ (-0.54074955 * float(x[2]))+ (0.5676439 * float(x[3]))+ (-1.3962 * float(x[4]))+ (-2.5414753 * float(x[5]))+ (2.2050958 * float(x[6]))+ (0.9022519 * float(x[7]))+ (0.03085769 * float(x[8]))+ (-0.85178524 * float(x[9]))+ (1.7763734 * float(x[10]))+ (0.49192286 * float(x[11]))+ (0.13907668 * float(x[12]))+ (1.5389342 * float(x[13]))+ (0.50981224 * float(x[14]))+ (-2.8490582 * float(x[15]))+ (-0.1864246 * float(x[16]))+ (-0.9300186 * float(x[17]))+ (0.44948643 * float(x[18]))+ (-0.40605342 * float(x[19]))+ (-0.6627512 * float(x[20]))+ (-0.030643847 * float(x[21]))+ (-1.1468444 * float(x[22]))+ (1.7919719 * float(x[23]))+ (2.1871266 * float(x[24]))+ (0.3019653 * float(x[25]))+ (-0.6955502 * float(x[26]))+ (0.47857377 * float(x[27]))) + -12.872892), 0)
    h_2 = max((((5.123717 * float(x[0]))+ (-1.5755394 * float(x[1]))+ (-0.097406164 * float(x[2]))+ (-2.8193839 * float(x[3]))+ (-3.0318193 * float(x[4]))+ (-0.09808052 * float(x[5]))+ (3.2971897 * float(x[6]))+ (-0.5313062 * float(x[7]))+ (0.23009153 * float(x[8]))+ (2.4585192 * float(x[9]))+ (0.14793251 * float(x[10]))+ (0.74783814 * float(x[11]))+ (1.2904792 * float(x[12]))+ (4.1421022 * float(x[13]))+ (-0.77193326 * float(x[14]))+ (-0.23686524 * float(x[15]))+ (2.8906257 * float(x[16]))+ (-1.6432644 * float(x[17]))+ (-0.15469596 * float(x[18]))+ (0.047592152 * float(x[19]))+ (0.16899852 * float(x[20]))+ (2.187051 * float(x[21]))+ (-1.3730632 * float(x[22]))+ (-0.26938748 * float(x[23]))+ (3.372527 * float(x[24]))+ (-2.7216105 * float(x[25]))+ (-2.2028666 * float(x[26]))+ (-2.2481074 * float(x[27]))) + 1.1242664), 0)
    h_3 = max((((-0.31389523 * float(x[0]))+ (-1.2406926 * float(x[1]))+ (0.15660377 * float(x[2]))+ (-0.3927383 * float(x[3]))+ (0.25751862 * float(x[4]))+ (-1.5929892 * float(x[5]))+ (-0.27656972 * float(x[6]))+ (-0.6027345 * float(x[7]))+ (-1.3145039 * float(x[8]))+ (-1.2513925 * float(x[9]))+ (0.6636132 * float(x[10]))+ (0.7563617 * float(x[11]))+ (-0.32772195 * float(x[12]))+ (-0.021108929 * float(x[13]))+ (0.73052514 * float(x[14]))+ (-0.82356036 * float(x[15]))+ (-1.1629754 * float(x[16]))+ (-0.28338087 * float(x[17]))+ (0.17144312 * float(x[18]))+ (1.001156 * float(x[19]))+ (0.04501097 * float(x[20]))+ (-0.5907438 * float(x[21]))+ (-0.33974668 * float(x[22]))+ (-0.05725275 * float(x[23]))+ (-0.18893968 * float(x[24]))+ (0.3910212 * float(x[25]))+ (-0.37001032 * float(x[26]))+ (0.8324538 * float(x[27]))) + -3.934609), 0)
    h_4 = max((((5.988609 * float(x[0]))+ (4.941669 * float(x[1]))+ (-1.0006386 * float(x[2]))+ (0.14453386 * float(x[3]))+ (0.8382823 * float(x[4]))+ (1.1287084 * float(x[5]))+ (-0.3192735 * float(x[6]))+ (-0.25243944 * float(x[7]))+ (0.6869671 * float(x[8]))+ (0.3238027 * float(x[9]))+ (-0.18365243 * float(x[10]))+ (-0.8505877 * float(x[11]))+ (0.788621 * float(x[12]))+ (-0.2872961 * float(x[13]))+ (-1.2250081 * float(x[14]))+ (0.13113907 * float(x[15]))+ (0.048564196 * float(x[16]))+ (0.24218519 * float(x[17]))+ (0.08873537 * float(x[18]))+ (0.35259578 * float(x[19]))+ (0.22122528 * float(x[20]))+ (1.2030863 * float(x[21]))+ (-1.4072615 * float(x[22]))+ (1.0048227 * float(x[23]))+ (-0.24083725 * float(x[24]))+ (-1.01907 * float(x[25]))+ (0.54108495 * float(x[26]))+ (-0.54232633 * float(x[27]))) + -6.0283117), 0)
    o[0] = (1.7094812 * h_0)+ (0.438322 * h_1)+ (-1.2065505 * h_2)+ (-19.099524 * h_3)+ (-6.807128 * h_4) + -1.392565

    

    #Output Decision Rule
    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)


def Predict(arr,headerless,csvfile, get_key, classmapping):
    with open(csvfile, 'r') as csvinput:
        #readers and writers
        writer = csv.writer(sys.stdout, lineterminator=os.linesep)
        reader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            writer.writerow(','.join(next(reader, None) + ["Prediction"]))
        
        
        for i, row in enumerate(reader):
            #use the transformed array as input to predictor
            pred = str(get_key(int(classify(arr[i])), classmapping))
            #use original untransformed line to write out
            row.append(pred)
            writer.writerow(row)


def Validate(arr):
    if n_classes == 2:
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        outputs=[]
        for i, row in enumerate(arr):
            outputs.append(int(classify(arr[i, :-1].tolist())))
        outputs=np.array(outputs)
        correct_count = int(np.sum(outputs.reshape(-1) == arr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(arr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(arr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0
    else:
        numeachclass = {}
        count, correct_count = 0, 0
        preds = []
        for i, row in enumerate(arr):
            pred = int(classify(arr[i].tolist()))
            preds.append(pred)
            if pred == int(float(arr[i, -1])):
                correct_count += 1
                if int(float(arr[i, -1])) in numeachclass.keys():
                    numeachclass[int(float(arr[i, -1]))] += 1
                else:
                    numeachclass[int(float(arr[i, -1]))] = 0
            count += 1
        return count, correct_count, numeachclass, preds
    


# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()


    #clean if not already clean
    if not args.cleanfile:
        tempdir = tempfile.gettempdir()
        cleanfile = tempdir + os.sep + "clean.csv"
        preprocessedfile = tempdir + os.sep + "prep.csv"
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x,y: x
        classmapping = {}


    #load file
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')


    #Normalize
    cleanarr = Normalize(cleanarr)


    #Transform
    if transform_true:
        if args.validate:
            trans = transform(cleanarr[:, :-1])
            cleanarr = np.concatenate((trans, cleanarr[:, -1].reshape(-1, 1)), axis = 1)
        else:
            cleanarr = transform(cleanarr)


    #Predict
    if not args.validate:
        Predict(cleanarr, args.headerless, preprocessedfile, get_key, classmapping)


    #Validate
    else: 
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = Validate(cleanarr)
        else:
            count, correct_count, numeachclass, preds = Validate(cleanarr)
            #Correct Labels
            true_labels = cleanarr[:, -1]


        #Report Metrics
        model_cap=151
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
            print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
            print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
            print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
            if int(num_TP + num_FN) != 0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN + num_FP) != 0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP + num_FP) != 0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2 * num_TP + num_FP + num_FN) != 0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP + num_FN) != 0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP + num_FN + num_FP) != 0:
                print("Critical Success Index:             {:.2f}".format(TS))

        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            print("System Type:                        " + str(n_classes) + "-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")





            def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
                #check for numpy/scipy is imported
                try:
                    from scipy.sparse import coo_matrix #required for multiclass metrics
                    try:
                        np.array
                    except:
                        import numpy as np
                except:
                    raise ValueError("Scipy and Numpy Required for Multiclass Metrics")
                # Compute confusion matrix to evaluate the accuracy of a classification.
                # By definition a confusion matrix :math:C is such that :math:C_{i, j}
                # is equal to the number of observations known to be in group :math:i and
                # predicted to be in group :math:j.
                # Thus in binary classification, the count of true negatives is
                # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
                # :math:C_{1,1} and false positives is :math:C_{0,1}.
                # Read more in the :ref:User Guide <confusion_matrix>.
                # Parameters
                # ----------
                # y_true : array-like of shape (n_samples,)
                # Ground truth (correct) target values.
                # y_pred : array-like of shape (n_samples,)
                # Estimated targets as returned by a classifier.
                # labels : array-like of shape (n_classes), default=None
                # List of labels to index the matrix. This may be used to reorder
                # or select a subset of labels.
                # If None is given, those that appear at least once
                # in y_true or y_pred are used in sorted order.
                # sample_weight : array-like of shape (n_samples,), default=None
                # Sample weights.
                # normalize : {'true', 'pred', 'all'}, default=None
                # Normalizes confusion matrix over the true (rows), predicted (columns)
                # conditions or all the population. If None, confusion matrix will not be
                # normalized.
                # Returns
                # -------
                # C : ndarray of shape (n_classes, n_classes)
                # Confusion matrix.
                # References
                # ----------
                if labels is None:
                    labels = np.array(list(set(list(y_true.astype('int')))))
                else:
                    labels = np.asarray(labels)
                    if np.all([l not in y_true for l in labels]):
                        raise ValueError("At least one label specified must be in y_true")


                if sample_weight is None:
                    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
                else:
                    sample_weight = np.asarray(sample_weight)
                if y_true.shape[0]!=y_pred.shape[0]:
                    raise ValueError("y_true and y_pred must be of the same length")

                if normalize not in ['true', 'pred', 'all', None]:
                    raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


                n_labels = labels.size
                label_to_ind = {y: x for x, y in enumerate(labels)}
                # convert yt, yp into index
                y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
                y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
                # intersect y_pred, y_true with labels, eliminate items not in labels
                ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
                y_pred = y_pred[ind]
                y_true = y_true[ind]
                # also eliminate weights of eliminated items
                sample_weight = sample_weight[ind]
                # Choose the accumulator dtype to always have high precision
                if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                    dtype = np.int64
                else:
                    dtype = np.float64
                cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


                with np.errstate(all='ignore'):
                    if normalize == 'true':
                        cm = cm / cm.sum(axis=1, keepdims=True)
                    elif normalize == 'pred':
                        cm = cm / cm.sum(axis=0, keepdims=True)
                    elif normalize == 'all':
                        cm = cm / cm.sum()
                    cm = np.nan_to_num(cm)
                return cm


            print("Confusion Matrix:")
            mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #Clean Up
    if not args.cleanfile:
        os.remove(cleanfile)
        os.remove(preprocessedfile)
