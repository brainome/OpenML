#!/usr/bin/env python3
#
# This code has been produced by an evaluation version of Daimensions(tm).
# Portions of this code copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.
# Distribution of this code in binary form or commercial use of any kind is forbidden.
# For a detailed license agreement see: http://brainome.ai/license
# Use of predictions results at your own risk.
#
# Output of Brainome Daimensions(tm) 0.98 Table Compiler v0.98.
# Invocation: btc -f QC -target class dilbert.csv -o dilbert.py -nsamples 0 --yes -nsamples 0 -e 100
# Total compiler execution time: 1:12:58.28. Finished on: Sep-03-2020 21:28:44.
# This source code requires Python 3.
#
"""
Classifier Type:                     Decision Tree
System Type:                         5-way classifier
Training/Validation Split:           50:50%
Best-guess accuracy:                 20.49%
Training accuracy:                   100.00% (5000/5000 correct)
Validation accuracy:                 26.26% (1313/5000 correct)
Overall Model accuracy:              63.13% (6313/10000 correct)
Overall Improvement over best guess: 42.64% (of possible 79.51%)
Model capacity (MEC):                3743 bits
Generalization ratio:                1.68 bits/bit
Model efficiency:                    0.01%/parameter
Confusion Matrix:
 [12.25% 2.15% 1.78% 2.00% 1.70%]
 [2.25% 13.43% 1.68% 1.34% 1.79%]
 [1.83% 1.48% 11.73% 1.88% 2.21%]
 [1.96% 1.24% 1.78% 13.24% 2.24%]
 [1.97% 1.81% 1.85% 1.93% 12.48%]
Overfitting:                         No
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "dilbert.csv"


#Number of attributes
num_attr = 2000
n_classes = 5


# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target="class"


def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target="class"
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return
    if (testfile):
        target = ''
        hc = -1
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([179.07098059999998, 180.13073115, 181.39811145000002, 187.11766420000004, 189.86515079999998, 191.78200025, 192.59159870000002, 198.45796934999998, 199.2586939, 200.7929615, 201.16396645, 201.6115587, 203.11478695, 204.2230717, 204.45950475, 207.05278760000002, 207.70122275, 210.95441005, 211.13854760000004, 213.60550465, 214.12156929999998, 214.16714815, 214.36857025, 216.09760065, 216.3649958, 216.50930835, 216.84724295, 217.5643556, 218.1066776, 218.53854615, 218.71235604999998, 220.28846484999997, 220.66397354999998, 222.3636695, 223.1010454, 226.23597875000002, 226.6964076, 227.00242980000002, 227.08107825000002, 227.8047639, 227.95470505000003, 228.09994825, 228.13163455, 228.2036099, 228.31995815, 230.01269315000002, 230.19818275, 233.2016584, 233.5390857, 233.9072369, 234.1852942, 235.03791040000002, 235.33137995, 235.9176137, 236.3987529, 238.4896354, 238.72335579999998, 239.30510759999999, 239.40501329999998, 239.82415065, 240.0078402, 240.2784701, 240.33365405, 240.4198323, 240.67281735, 240.78790370000002, 241.55765995000002, 242.00989085, 242.25473075, 242.5267778, 242.84171809999998, 243.52689715, 243.74237225000002, 243.94726960000003, 244.80232160000003, 245.37392625, 245.786883, 246.2503277, 246.60384795, 246.96308175000001, 248.07416145000002, 248.57942075, 248.8607465, 250.09057095, 251.2689454, 251.9857976, 252.94565045000002, 253.613966, 254.6878506, 254.71396555, 254.8114943, 255.30411480000004, 255.53648465000003, 255.77664650000003, 255.934663, 256.00376289999997, 256.2034593, 256.29593435, 257.76491315, 257.97369074999995, 258.48027994999995, 258.5164559, 258.57324589999996, 258.87833205000004, 258.96891105, 259.24199269999997, 260.0297101, 260.23912695, 260.4089772, 261.18879205, 261.4837307, 261.63910545, 262.54697235000003, 263.04003685000004, 263.36395780000004, 263.63977445, 263.7081419, 263.89033625, 264.09322695000003, 264.41073435, 264.991087, 265.25331485000004, 265.829356, 265.98334495, 266.21120835, 266.4951735, 266.62635975, 267.01553845, 267.3689605, 267.38606025, 267.39585925, 267.56469475, 267.8848888, 268.22583135, 268.34096120000004, 268.63228445, 268.68319809999997, 268.95423725, 269.24605760000003, 271.16033204999997, 271.38265950000005, 271.821205, 272.6799009, 273.15844845, 273.3596976, 273.47364415, 273.672578, 273.88889205, 274.05415204999997, 274.3183543, 275.02069915000004, 275.89873205000004, 276.16119675, 276.45133015, 276.52506070000004, 276.79094185, 277.74756535, 278.32532545000004, 278.79169445, 278.98014880000005, 279.04126629999996, 279.36350919999995, 279.6068575, 279.81273225, 280.0608383, 280.31232950000003, 280.33232845, 281.0393182, 281.4632365, 283.52506715, 284.2220658, 284.90934115, 285.1466765, 286.18848280000003, 286.78840195, 286.90268365, 287.1282073, 287.36091220000003, 287.79654425, 287.8488433, 288.00180795, 288.32732695, 289.08448905, 289.21095760000003, 289.42188075, 289.65039485, 289.7058465, 289.76751935000004, 290.5419776, 290.83770735, 291.16055875, 291.43886455, 291.6372976, 291.75791530000004, 292.0267875, 292.3439553, 293.12284345, 293.34582645, 293.34904215, 293.52933205, 293.63926599999996, 293.85028124999997, 293.92975775, 294.03776450000004, 294.13005975, 294.25890495, 294.3789755, 294.46486115000005, 294.7540406, 295.07850084999995, 295.2474287, 295.9434504, 296.317242, 296.3974953, 296.45760475, 296.54353894999997, 296.64745030000006, 296.8230704, 296.9703876, 297.01317635000004, 297.39258944999995, 297.58455815, 297.87860945, 298.1014478, 298.2455167, 298.41708029999995, 298.61221524999996, 298.8705923, 299.05323795000004, 299.1810136, 299.3140801, 299.37418045, 299.41883889999997, 299.47149365, 299.6584634, 299.9368094, 300.88684025, 300.96831755, 301.16257955, 301.51240505, 301.7576779, 301.89665130000003, 302.07312175, 302.2095877, 302.250389, 302.30049245, 302.49252675, 302.7149875, 302.89932875, 303.10880305, 303.32961865, 303.53547165, 303.67852185, 303.92163135, 304.0497167, 304.43503714999997, 304.56092915, 304.728901, 304.95669169999996, 305.18149370000003, 305.36968905000003, 305.47171335, 305.5305095, 305.5643013, 305.62433745, 305.6792285, 305.8797962, 306.2656411, 306.53564159999996, 306.65104415, 307.3062721, 307.46969875, 307.64367864999997, 307.71543985, 307.98098935, 308.61658194999995, 308.65249235, 308.7284585, 308.81094944999995, 308.8415489, 308.99158935, 309.12473115, 309.1621872, 309.29291135, 309.6620788, 309.9192324, 310.0861834, 310.11714409999996, 310.2929685, 310.47697445, 310.55597495, 310.6018642, 310.6864849, 310.9131867, 311.13230754999995, 311.27997555, 311.6927453, 311.7407143, 311.7832875, 311.86330075, 311.94507280000005, 311.98794755, 312.04823905, 312.13737345000004, 312.17340585, 312.2293338, 312.24018025, 312.2923161, 312.45869365, 312.5905579, 313.0267677, 313.08698085000003, 313.2973103, 313.45533815, 313.6211438, 313.77979175, 313.79041405, 313.8153631, 313.88247839999997, 314.03968805, 314.25160135, 314.4059238, 314.50877525000004, 314.69212260000006, 314.90382784999997, 315.36438865, 315.7921329, 316.07583180000006, 316.3775699, 316.58918800000004, 317.05502695, 317.4676473, 317.61447515, 317.7443387, 317.860506, 317.9534331, 318.00709919999997, 318.11148315, 318.27779505, 318.42072815, 318.89754895, 319.03175835, 319.1002443, 319.26438525, 319.4159304, 319.50766965, 319.60504475, 319.68901105, 319.72123865, 319.75757434999997, 319.98130455, 320.5758171, 320.89906199999996, 320.99200279999997, 321.19223385, 321.35579565, 321.4077378, 321.45637325, 321.56954375000004, 321.69382830000006, 321.8120834, 321.91797675, 322.6120777, 323.2572458, 323.3246972, 323.34378735, 323.36303495000004, 323.42466379999996, 323.5068783, 323.5447845, 323.61793825, 323.6867176, 323.714618, 323.75318500000003, 323.83037375000004, 323.9443896, 324.02592025, 324.0639956, 324.10650154999996, 324.13790309999996, 324.18392969999996, 324.409457, 324.60371895000003, 324.84809244999997, 325.4184116, 325.47318145, 325.51865545, 325.65206365, 325.79465095, 325.9928218, 326.1476278, 326.44645455, 326.7694696, 326.96872229999997, 327.11408604999997, 327.29374490000004, 327.42097075, 327.6954896, 327.8556352, 327.9698231, 328.2077183, 328.37581120000004, 328.43794560000003, 328.65030390000004, 328.66787315, 328.75545645, 328.88510795, 328.9996041, 329.09407665000003, 329.1298152, 329.17621779999996, 329.26063584999997, 329.373487, 329.50757680000004, 329.87346564999996, 330.10321395000005, 330.38314125, 330.67566445, 330.91460854999997, 331.12243635, 331.24136865, 331.3043804, 331.35560139999995, 331.46232315, 331.48945355, 331.61721804999996, 331.71385304999995, 331.97574035, 332.06631975000005, 332.19537725, 332.4431487, 332.71079695000003, 332.90489585, 332.9868487, 333.07392489999995, 333.15341809999995, 333.27408579999997, 333.49483084999997, 333.5873434, 333.64378805, 333.71274665, 333.83863840000004, 333.9447794, 333.9993068, 334.07819345, 334.2140784, 334.32748745000004, 334.3728145, 334.3808633, 334.4115311, 334.45041905000005, 334.94928215, 335.04043624999997, 335.12971585, 335.3009975, 335.39143905000003, 335.4687432, 335.70295780000004, 336.01055695, 336.107977, 336.30862974999997, 336.5880812, 336.83357594999995, 336.93689084999994, 337.0356424, 337.07170245, 337.15456085, 337.23674589999996, 337.3207678, 337.4457632, 337.5309254, 337.6178128, 337.696902, 337.75102595, 337.93296145, 338.04899324999997, 338.14565804999995, 338.20648529999994, 338.27967264999995, 338.34560969999995, 338.37716445, 338.4144135, 338.45967069999995, 338.5052914, 338.60492605, 338.783453, 338.8719011, 339.441344, 339.6848585, 339.92072455000005, 339.94185985, 339.97998835, 340.0206737, 340.08874249999997, 340.18475109999997, 340.22932845, 340.2539838, 340.3760061, 340.41592645000003, 340.45709405, 340.7138195, 340.76629675, 340.8184406, 340.90761605, 340.99753495000004, 341.1080432, 341.24900764999995, 341.37979615, 341.463066, 341.51625965000005, 341.55654555, 341.6028109, 341.69461325, 341.8278181, 341.97063735, 342.12803994999996, 342.215919, 342.28379195, 342.3136169, 342.39898585000003, 342.6344456, 342.7998473, 342.870898, 343.08906195000003, 343.19343050000003, 343.2438192, 343.470309, 343.6751098, 343.71362535000003, 343.7441101, 343.76603205000004, 343.82559155, 343.92195585, 344.02420355000004, 344.11460845, 344.2540792, 344.4076028, 344.4764556, 344.55811745, 344.6133228, 344.66077484999994, 344.72342985, 344.76059525, 344.77909195, 344.95634785000004, 345.00335515000006, 345.12497915, 345.2121958, 345.3978502, 345.62131305, 345.9750633, 346.0063727, 346.01729209999996, 346.060034, 346.11884545, 346.1540125, 346.26704270000005, 346.36958315000004, 346.66145535, 346.73617155, 346.7550566, 346.77087200000005, 346.7880736, 346.88176195000005, 347.02322485, 347.23444015, 347.43105225, 347.47188025, 347.5196824, 347.5426565, 347.5766324, 347.60823915000003, 347.72409625, 347.76844995, 347.85130275, 347.9701907, 348.08006115, 348.33435875, 348.47903145, 348.495738, 348.5478853, 348.62657605, 348.65829069999995, 348.68196905, 348.8470587, 349.01646775, 349.14750415000003, 349.1958351, 349.28520145, 349.37165650000003, 349.4802536, 349.58691775, 349.7265604, 349.74580255, 349.80535084999997, 349.8827972, 349.9108801, 349.94062675, 350.03540069999997, 350.124467, 350.16909705, 350.22032615, 350.28515745000004, 350.36621334999995, 350.45823674999997, 350.5672609, 350.64945785, 350.70211875, 350.79462509999996, 350.88969675, 350.93513155000005, 350.9680853, 350.99522190000005, 351.0087682, 351.0205197, 351.07749079999996, 351.14840519999996, 351.19660350000004, 351.24635445, 351.28201455, 351.3287696, 351.429159, 351.52432610000005, 351.6421947, 351.7204101, 351.8037815, 351.96288605, 352.00764925, 352.08994635, 352.18722255, 352.2494724, 352.84292085000004, 352.98460485, 353.0348748, 353.14838335, 353.23089355, 353.32649349999997, 353.3537009, 353.51982625, 353.71016629999997, 353.76528110000004, 353.85183625, 353.92104, 353.92564345000005, 353.93694755, 353.96072690000005, 354.09358585, 354.1316506, 354.36256844999997, 354.51137755, 354.9128843, 355.04200375000005, 355.2131302, 355.39146965, 355.46346725, 355.49351735000005, 355.50013255, 355.62186985000005, 355.7197947, 355.90644065, 356.05021845, 356.20866854999997, 356.37037515, 356.74136415, 356.83486755, 356.970147, 357.03460115, 357.1003257, 357.18935385, 357.29107899999997, 357.32835475, 357.44129845, 357.56846895, 357.60093715000005, 357.67864405, 357.8433275, 357.98958845, 358.05667955, 358.19243205, 358.34305365, 358.4318963, 358.47861485, 358.50150830000007, 358.54032225000003, 358.56131595, 358.57218804999997, 358.6023745, 358.69659005000005, 358.77041235, 358.88714985, 358.9503046, 359.01427105, 359.07581145, 359.15874145, 359.1927137, 359.25872604999995, 359.32879175, 359.344035, 359.37364515, 359.3963175, 359.5961539, 359.7460713, 360.1590937, 360.3452346, 360.5748875, 360.68477054999994, 360.92348995, 361.1098474, 361.21010344999996, 361.2725368, 361.30465285, 361.33358510000005, 361.47347035, 361.5326715, 361.54272349999997, 361.6137912, 361.67371985, 361.71345635, 361.7971937, 361.84749775, 361.9130302, 361.970406, 362.08180355, 362.4045019, 362.6420109, 362.87165494999994, 362.97838964999994, 363.0216352, 363.0621547, 363.10371375, 363.2252271, 363.39399099999997, 363.48380525, 363.64126120000003, 363.7811491, 363.86159875, 363.95149145, 363.9863985, 364.2364718, 364.30875199999997, 364.34714755000005, 364.36201155000003, 364.38495525, 364.57962415, 364.63893005, 364.6849369, 364.6972671, 364.75858815000004, 364.87110135, 365.07398655, 365.15218215000004, 365.20430235000003, 365.28449209999997, 365.32824865, 365.33716215, 365.3861015, 365.43114805000005, 365.44486525, 365.47959425, 365.50959294999996, 365.57671995, 365.79255975, 365.96776915, 366.02200895, 366.08712849999995, 366.21986545, 366.35184835, 366.37997225000004, 366.43502865000005, 366.47891445000005, 366.52646780000003, 366.65341525, 366.7424882, 366.84174169999994, 367.0598872, 367.1163222, 367.22064175, 367.42464010000003, 367.74342429999996, 367.80177184999997, 367.8466529, 367.87149465, 367.87908755, 367.98766565, 368.11221125, 368.21667375, 368.3337693, 368.37516915000003, 368.3954802, 368.4908493, 368.6455194, 368.76350234999995, 368.89549975, 369.05128924999997, 369.2375814, 369.4767497, 369.53532595, 369.6435517, 369.7382027, 369.78037235, 369.8182401, 369.870229, 370.1503179, 370.20061755, 370.2561981, 370.29988435, 370.47792719999995, 370.65085945, 370.7120203, 370.77598725000007, 370.83023095, 370.93358229999996, 371.0270548, 371.07712449999997, 371.23027245000003, 371.31618265000003, 371.41497630000003, 371.54098885, 371.67828145, 371.758759, 371.90990155000003, 372.05615235, 372.09562889999995, 372.16671030000003, 372.25103895, 372.33279965, 372.43918555000005, 372.52285325, 372.57526195, 372.65487645, 372.75593405, 372.8104953, 372.85341795, 373.044761, 373.14713565, 373.24056214999996, 373.2629609, 373.3020651, 373.34762735000004, 373.37257875, 373.38076359999997, 373.50963840000003, 373.69744765, 373.83249925, 373.89975505, 374.0919929, 374.23914609999997, 374.3315476, 374.4305495, 374.549359, 374.5923104, 374.70281105000004, 374.7809179, 374.87172910000004, 375.1089554, 375.17785634999996, 375.30826019999995, 375.33153775000005, 375.3479179, 375.36192205000003, 375.39014065000003, 375.4943233, 375.5929116, 375.75330410000004, 375.89972615, 375.99215025, 376.06174565000003, 376.0875303, 376.15042455, 376.2464475, 376.30657694999996, 376.44727385, 376.5094679, 376.5922329, 376.67503839999995, 376.7916721, 376.82059805, 376.90746185, 376.99407345, 377.14986435000003, 377.37352375, 377.4265669, 377.52906405, 377.64584905, 377.73142505, 377.79225585, 377.9641518, 378.1309302, 378.1738881, 378.2046374, 378.29115205, 378.4850532, 378.66026324999996, 378.76377835, 378.79478295, 378.83758415, 378.8601846, 378.92748, 379.0963842, 379.2178748, 379.27938904999996, 379.4480849, 379.52093905000004, 379.60658715, 379.76229465000006, 379.8255437, 380.1212186, 380.21890515, 380.4461728, 380.46621799999997, 380.63211825, 380.6397547, 380.64669480000003, 380.68125865, 380.80534969999997, 380.8681972, 380.9392962, 381.04356985, 381.1650423, 381.20034835, 381.3995284, 381.5580268, 381.58823110000003, 381.63493325, 381.6740792, 381.83146525000006, 381.86653325000003, 382.01830605000004, 382.18448120000005, 382.22040765, 382.26483195000003, 382.32136135, 382.34614969999996, 382.37817715, 382.42064525, 382.47993995, 382.54178035000007, 382.6068398, 382.66298675, 382.7481186, 382.86339475, 383.09805270000004, 383.2626032, 383.29925369999995, 383.31613139999996, 383.36342290000005, 383.42293060000003, 383.5244589, 383.6242377, 383.6536648, 383.67886515, 383.77316665, 383.912008, 383.9885705, 384.0214173, 384.06523225, 384.14014699999996, 384.19859485, 384.2351366, 384.33933794999996, 384.43813215, 384.5509939, 384.6613894, 384.6912963, 384.8390449, 385.02009355, 385.08855095, 385.1636469, 385.2457372, 385.28377585, 385.34284990000003, 385.39454885, 385.46761155, 385.8566049, 385.93530845, 385.9823543, 386.00495355, 386.0747377, 386.1327111, 386.1784193, 386.22606800000005, 386.32248475000006, 386.44388130000004, 386.6159656, 386.6613987, 386.67324835, 386.7145008, 386.74122925, 386.8115441, 386.94484085, 387.0325808, 387.0579553, 387.11689045, 387.1760015, 387.26704325000003, 387.35727305, 387.41824694999997, 387.49167850000003, 387.63550799999996, 387.67099525, 387.69274425000003, 387.72606325000004, 387.82187319999997, 387.9494264, 388.07962030000004, 388.19626905000007, 388.43230975, 388.62591839999993, 388.66307465, 388.6974833, 388.70192265000003, 388.7616938, 388.83119464999993, 388.9080162, 389.12254945, 389.14200095, 389.17275895, 389.1977917, 389.24492595, 389.31670295, 389.42266945, 389.6412102, 389.79536205, 389.86408115, 390.14227905, 390.2704338, 390.44130905, 390.45214, 390.48991315, 390.56003935, 390.62829650000003, 390.74643445000004, 390.83227665000004, 390.95227435, 391.08390994999996, 391.13978865, 391.22953555000004, 391.34207505, 391.45320289999995, 391.51006745, 391.63246955000005, 391.94306489999997, 392.10563275000004, 392.21040045, 392.2742156, 392.34238545, 392.42720545, 392.49283465, 392.52482560000004, 392.6463674, 392.7433807, 392.76711205, 392.85253685, 392.9658551, 393.1716544, 393.1979546, 393.32063644999994, 393.56467795, 393.69410065, 393.74990894999996, 393.82938924999996, 393.9505802, 393.9715028, 394.0252633, 394.09699745, 394.2190214000001, 394.2844433, 394.33072995, 394.35099285, 394.3843743, 394.41242470000003, 394.454116, 394.5827337, 394.73809120000004, 394.80755650000003, 394.86832615, 395.09990425, 395.15282435, 395.2231355, 395.30666040000006, 395.39446970000006, 395.46005420000006, 395.52028829999995, 395.5650463, 395.59592255, 395.68550245, 395.83114839999996, 395.91998624999997, 395.95535835, 395.99067139999994, 396.00186714999995, 396.06124635, 396.122336, 396.15632925, 396.2406524, 396.32493039999997, 396.38277215000005, 396.43402675000004, 396.46264790000004, 396.7536171, 396.80081470000005, 396.83283860000006, 396.87566195, 397.0508046, 397.2055163, 397.3107681, 397.38387775, 397.4715471, 397.5344268, 397.59890040000005, 397.71979065000005, 397.76721970000006, 397.81589055, 398.11427745000003, 398.123496, 398.1376684, 398.2122185, 398.27948825, 398.28283665000004, 398.28741325, 398.3033528, 398.35239655, 398.53428105, 398.54256369999996, 398.6318851, 398.77901665, 398.90496600000006, 398.99057934999996, 399.08875615, 399.1882781, 399.3291911, 399.49696335, 399.619, 399.7420015, 399.81321805, 399.82770719999996, 399.8402962, 399.8849564000001, 399.97988335, 399.982318, 400.03393265, 400.25288589999997, 400.33652179999996, 400.36559109999996, 400.4580423, 400.54665625, 400.63260185, 400.67654719999996, 400.68530235000003, 400.7042801, 400.73777275, 400.91172015000006, 400.97199725, 401.01378884999997, 401.04758395, 401.18455485000004, 401.21431490000003, 401.21927200000005, 401.26008205000005, 401.35097275, 401.4471039, 401.49216305, 401.57471115000004, 401.61983385, 401.88849480000005, 401.9374151, 402.14975849999996, 402.20533704999997, 402.23768565, 402.2739182, 402.322631, 402.4758087, 402.62976815, 402.75502385, 402.84787855, 402.94595619999996, 403.04222025, 403.12951069999997, 403.19277815000004, 403.30867515, 403.46102264999996, 403.5373482, 403.5800719, 403.72849925, 403.86211515, 403.93663555, 404.03233984999997, 404.0670265, 404.0798657, 404.09460335000006, 404.1419899, 404.22730105, 404.2829176, 404.317368, 404.42781885, 404.45526659999996, 404.4791371, 404.50613225, 404.5822801, 404.6409852, 404.74800400000004, 404.89125900000005, 404.9752777, 405.05364785, 405.08681085, 405.0960013, 405.1130481, 405.12652955000004, 405.14110515000004, 405.2466756, 405.34442379999996, 405.36152459999994, 405.44230830000004, 405.51018680000004, 405.53906914999993, 405.57407125, 405.60705055000005, 405.68679280000003, 405.82924660000003, 405.9248501, 405.93691985, 405.9459668, 406.1019548, 406.46879625, 406.5234712, 406.58358280000004, 406.61272125, 406.71190505, 406.98145115, 407.19414825, 407.37081985, 407.49476675, 407.56206775000004, 407.5746987, 408.16279715, 408.21140195, 408.28856215, 408.36224735, 408.3715244, 408.72411575, 408.7392495, 408.78342815, 408.85415755, 408.89050775, 408.96821275, 409.0500425, 409.11029705, 409.16835430000003, 409.20254780000005, 409.24498925, 409.3172522, 409.3866644, 409.4050266, 409.42939015, 409.45865434999996, 409.47528495, 409.51070055, 409.5445013, 409.6528059, 409.67468944999996, 409.70132635, 409.7549079, 409.8460963, 409.9534959, 410.02524615, 410.0820323, 410.15574530000004, 410.2035647, 410.29791179999995, 410.41279605, 410.46072990000005, 410.54210745, 410.6694489, 410.7332403, 410.76126554999996, 410.80582335, 410.8658614, 410.95438865, 411.0187912, 411.0816486, 411.21490515, 411.3345292, 411.59430349999997, 411.76108630000004, 411.8949185, 411.96892885, 412.06660205000003, 412.1340239, 412.19061855, 412.21072654999995, 412.23323439999996, 412.2622815, 412.32029320000004, 412.38640375, 412.43819340000005, 412.47494574999996, 412.51476189999994, 412.59484575, 412.64997205, 412.67722855, 412.71699395, 412.83234725, 412.91525435, 412.92601990000003, 412.93135435, 412.96963235, 413.01194305, 413.04164620000006, 413.12706405000006, 413.25404795000003, 413.37140955000007, 413.39403285000003, 413.52332645, 413.62997025, 413.776974, 413.92108110000004, 413.99889970000004, 414.05412475, 414.42790745, 414.53044405, 414.6599416, 414.7930454, 414.86618869999995, 414.90827160000003, 414.9414587, 414.97212974999997, 415.0070674, 415.07426250000003, 415.11731365, 415.1600968, 415.24327965000003, 415.29294685, 415.3394257, 415.38763535, 415.5228247, 415.60776135000003, 415.6984887, 415.8049213, 415.85242115, 415.90591744999995, 415.95053665, 415.96598874999995, 416.00048069999997, 416.02905315, 416.10733335, 416.42256675, 416.60814289999996, 416.7061071, 416.77700980000003, 416.93230400000004, 417.0886569, 417.16623615, 417.27840695000003, 417.32213935, 417.41596169999997, 417.54676815, 417.6518628, 417.71032715, 417.71998045, 417.7933002, 417.8614401, 417.9698245, 418.04213435, 418.1125919, 418.1786088, 418.2080582, 418.24868604999995, 418.26848385, 418.28551749999997, 418.35067159999994, 418.37657245, 418.4018986, 418.42190129999994, 418.57074835, 418.62979234999995, 418.78359209999996, 418.9670663, 419.0415334, 419.1356765, 419.20156199999997, 419.3239773, 419.43807300000003, 419.47158175000004, 419.48038505, 419.5460943, 419.61940534999997, 419.66398565, 419.70599285, 419.87210575, 420.07919799999996, 420.29287465, 420.3704806, 420.50665875, 420.5975384, 420.63093530000003, 420.6707599, 420.70784675000004, 420.7779012, 420.84670900000003, 420.9271834, 421.06301105, 421.18928965, 421.28687875, 421.3597221, 421.4639162, 421.53604070000006, 421.58958370000005, 421.6740698, 421.8201994, 421.84447835, 421.9549356, 422.01125085, 422.08477224999996, 422.16090744999997, 422.20338319999996, 422.4521573, 422.70463374999997, 422.90697049999994, 422.9864708, 423.23407185, 423.31363965, 423.3681538, 423.39780625000003, 423.436234, 423.46350685, 423.49042224999994, 423.53614214999993, 423.57810789999996, 423.59603919999995, 423.67373530000003, 423.75470805000003, 423.82290939999996, 423.95443739999996, 424.01404775000003, 424.043699, 424.0689638, 424.12343769999995, 424.18539455, 424.20257055, 424.2526035, 424.33151175, 424.3815135, 424.4237617, 424.78991525, 424.8671123, 425.0158087, 425.16457779999996, 425.19846585, 425.25011035, 425.29853785, 425.36317080000003, 425.4915874, 425.54495725000004, 425.62665544999993, 425.6828941, 425.77177159999997, 425.8411127, 425.9619852, 426.0219821, 426.0755362, 426.1116941, 426.17729905, 426.2324837, 426.28584290000003, 426.3320069, 426.33604360000004, 426.3392064, 426.37621045000003, 426.5445205, 426.6926498, 426.80272445, 426.8928924, 427.12960710000004, 427.23574885, 427.25016404999997, 427.40328955, 427.47705825, 427.75333009999997, 428.02162484999997, 428.09824955, 428.10455720000004, 428.23243924999997, 428.33693730000005, 428.4397866, 428.46361824999997, 428.48577375, 428.58654165, 429.20276775, 429.23820724999996, 429.29765399999997, 429.3582754, 429.39412985, 429.43130429999997, 429.5459687, 429.68003875, 429.85434615, 429.91161265000005, 429.96233755000003, 430.03844735000007, 430.12865815, 430.21046210000003, 430.26794225000003, 430.4717435, 430.52484615, 430.6661179, 430.8819669, 430.996348, 431.03383395000003, 431.1252008, 431.21409314999994, 431.54237355, 431.58670455, 431.63867165, 431.7552068, 431.88255725, 431.99592029999997, 432.0653751, 432.21210895, 432.33740350000005, 432.38177099999996, 432.46070075, 432.51857129999996, 432.52397014999997, 432.52832015, 432.54102375, 432.57618095, 432.60775085, 432.63977235, 432.68052105, 432.73552135, 432.81483490000005, 432.88597445000005, 432.9368225, 432.99207855, 433.05154819999996, 433.1157633, 433.17063210000003, 433.25881634999996, 433.34855600000003, 433.39249255, 433.480637, 433.6964256, 433.81056795, 433.85405755, 433.87478680000004, 433.90127125000004, 433.96803345, 434.0325621, 434.1117797, 434.5887676, 434.66251224999996, 434.6876036, 434.70504695000005, 434.72333715, 434.76870699999995, 434.80713525, 435.05767319999995, 435.1900889, 435.27197279999996, 435.31953805, 435.34479655, 435.4650685, 435.51553364999995, 435.57448995000004, 435.6239148, 435.65859715, 435.74939475, 435.95953745, 436.09008195, 436.153946, 436.20175014999995, 436.54362849999995, 436.62821929999996, 436.73845115, 436.8476991, 437.13445895, 437.1879005, 437.2091051, 437.31406530000004, 437.39215914999994, 437.44251965, 437.5830347, 437.68319045, 437.7557568, 437.7975762, 437.93981525000004, 437.971093, 438.0024314, 438.29456555, 438.60537280000005, 438.64917790000004, 438.6690415, 438.67495365, 438.7835025, 438.95063785, 439.11003859999994, 439.1422729, 439.1852774, 439.21939875, 439.24313859999995, 439.29222389999995, 439.35564115, 439.48723800000005, 439.6307746, 439.7005059500001, 439.7336390500001, 439.8039282, 439.89757475, 439.970236, 440.61398970000005, 440.76584634999995, 440.82475969999996, 440.8956177, 440.96691384999997, 441.03035445, 441.1223509, 441.2458024, 441.30541885, 441.35764200000006, 441.3984362, 441.5402654, 441.66593035, 441.74004160000004, 441.80460425, 441.85788594999997, 441.8808472, 441.9557092, 442.09167525, 442.1862118, 442.2246977, 442.4050192, 442.60410115, 442.6543724, 442.69271095, 442.76941755, 443.2131783, 443.3672012, 443.38458575000004, 443.45709524999995, 443.5342199, 443.5626372, 443.57017744999996, 443.57467369999995, 443.5979595, 443.62608025, 443.6428566000001, 443.6574193, 443.8469734, 443.88017835, 443.89872015000003, 443.99566565, 444.07037355, 444.2480107, 444.31684340000004, 444.34827700000005, 444.40393585000004, 444.48277425, 444.5425077, 444.6398651, 444.77474275, 444.85599559999997, 444.8803767, 444.9013193, 444.9512032, 445.0030903, 445.13693845, 445.40848565, 445.56997609999996, 445.79315395000003, 445.9288659, 445.98485595, 446.13394685000003, 446.23989605, 446.27254165, 446.34433679999995, 446.41920589999995, 446.44626675, 446.45847875, 446.56293174999996, 446.69945039999993, 446.88559875, 447.0663995, 447.15093670000005, 447.23238375000005, 447.2894162, 447.35282374999997, 447.4480939, 447.52556165, 447.71585559999994, 447.7788216, 447.91167709999996, 447.95828554999997, 448.08516105, 448.20587595, 448.27273605, 448.3254389, 448.37785965, 448.41931024999997, 448.4476267, 448.50971374999995, 448.55590785000004, 448.5702747, 448.64589090000004, 448.73666325, 448.78695960000005, 448.80057170000003, 448.85584900000003, 448.91519475, 449.01688, 449.12856999999997, 449.191865, 449.2541758, 449.32641164999995, 449.40289314999995, 449.48737495, 449.59522945, 449.70313649999997, 449.7474876, 449.8054956, 449.8746782, 450.09114905, 450.27730399999996, 450.32938045000003, 450.4258199, 450.47223635, 450.53769025, 450.66015400000003, 450.73558815, 450.83882930000004, 450.93218970000004, 450.99841530000003, 451.0610729, 451.11885184999994, 451.16189610000004, 451.19301305, 451.2751955, 451.36525065, 451.5780981, 451.5965857, 451.70897825000003, 451.82219135, 451.8611502, 451.97544815000003, 452.14697590000003, 452.26273885, 452.35784435, 452.40019785, 452.74628885, 452.8161068, 452.8668091, 452.88896265, 452.89373579999994, 452.90412489999994, 453.0120682, 453.10072645, 453.1368841, 453.3060828, 453.42061605000004, 453.47327970000003, 453.51179705, 453.53745325, 453.56314510000004, 453.6328466, 453.7108666, 453.7409492, 453.7527946, 453.80935875, 453.8609121, 453.99259425, 454.18298000000004, 454.2728217, 454.3909003, 454.61210245000007, 454.68362565, 454.7366568, 454.8312759, 454.91261925, 454.9747137, 455.0367596, 455.1061472, 455.1654594, 455.224173, 455.41298980000005, 455.60324405000006, 455.74131919999996, 455.9330109499999, 456.02677645, 456.125509, 456.24721425, 456.31811045, 456.4374502, 456.65939905, 456.84801500000003, 456.92501120000003, 456.94756425, 457.25450255, 457.2929284, 457.51766690000005, 457.53349565, 457.5711751, 457.62287330000004, 457.6887557, 457.76595425, 457.81983055, 457.90274415, 457.9708653, 458.0014926, 458.03765544999993, 458.06591624999993, 458.10744615, 458.14215245, 458.30775515000005, 458.3234371, 458.36494600000003, 458.4322686, 458.51046414999996, 458.66811420000005, 459.00083825, 459.07950365, 459.08620874999997, 459.1641855, 459.25634160000004, 459.28381164999996, 459.36348669999995, 459.45370734999995, 459.504192, 459.59825125, 459.69448875, 459.76195399999995, 459.8986309999999, 460.0399327, 460.05767579999997, 460.07753855, 460.1034284, 460.21777169999996, 460.35521835, 460.82980169999996, 461.20830920000003, 461.2841799, 461.40192870000004, 461.65533485000003, 461.8242045, 461.876117, 462.05035430000004, 462.23163550000004, 462.51658975, 462.71089859999995, 462.84528539999997, 462.91488649999997, 462.93739589999996, 462.95218520000003, 462.98146844999997, 463.0173214, 463.10953400000005, 463.23045035, 463.2989085, 463.34557369999993, 463.42123454999995, 463.51074185, 463.5712289, 463.62857245, 463.66654305000003, 463.7290719, 463.8301476500001, 463.8912955, 464.11463760000004, 464.17606715, 464.25993525, 464.87802094999995, 465.08605224999997, 465.32468135, 465.43072625, 465.44426835, 465.4730455, 465.5093131, 465.55418599999996, 465.60185995, 465.67132095, 465.6979665, 465.77650415, 465.8946034, 465.95885599999997, 465.9931538, 466.11967355, 466.30505625, 466.4783119, 466.5631198, 466.58919515, 466.74812575000004, 466.82005455, 466.87551365, 466.9053473, 466.99833764999994, 467.08351129999994, 467.1006907, 467.1429021, 467.17677994999997, 467.3252863, 467.46976085, 467.50061900000003, 467.55657295000003, 467.72453665, 467.97135095, 468.08416905, 468.20177575, 468.50502105, 468.57331634999997, 468.7585285, 468.94991495, 469.11231964999996, 469.23018774999997, 469.3343727, 469.38096785000005, 469.40661045, 469.42092425, 469.4278131, 469.4522233, 469.47530224999997, 469.47837419999996, 469.47922535, 469.51658425000005, 469.72115885000005, 469.96885015000004, 470.19367509999995, 470.26778885, 470.2791822, 470.35308970000006, 470.61798515000004, 470.86877045, 470.9547346, 471.15190205, 471.32462684999996, 471.33098735, 471.7932624, 471.8049592, 471.8167691, 471.84497215, 471.87657170000006, 471.90454645, 471.95766525, 472.26239325000006, 472.47367844999997, 472.5580664, 472.6093625, 472.66750885, 472.67797454999993, 472.69001695, 472.74566804999995, 473.2764398, 473.28725875000003, 473.29156290000003, 473.3021220999999, 473.39482745, 473.47825524999996, 473.6243525, 473.74567909999996, 473.98882549999996, 474.03146255, 474.25557255, 474.53296025, 474.6107728999999, 474.6468602, 474.7272117, 474.79605355, 474.83523985, 474.98921585, 475.27806605, 475.44702555000003, 475.51678775, 475.5653825, 475.66980759999996, 475.7325606, 475.80042460000004, 475.852953, 475.87170875, 475.93036914999993, 476.0166454, 476.05436735, 476.16995225000005, 476.5491669, 476.88240155, 477.0133178, 477.09784325, 477.23733605, 477.32497315, 477.36484085000006, 477.44869155000004, 477.55595435, 477.9131788, 478.1762648, 478.27436695, 478.6149166, 478.71486325, 478.8320787, 478.92799645, 479.01829050000003, 479.08316640000004, 479.10030930000005, 479.11346275, 479.12890245, 479.14767205, 479.2108999, 479.28233380000006, 479.34986635000007, 479.41115945, 479.44599644999994, 479.45342934999996, 479.46068204999995, 479.47050715, 479.51343314999997, 479.61300435, 479.68559205, 479.89612645, 480.05223565000006, 480.24972075000005, 480.37792379999996, 480.4183874, 480.45807845, 480.5097109, 480.55169939999996, 480.6129097, 480.804318, 480.98974685, 481.0397311, 481.1830577, 481.26527345, 481.29094585000007, 481.3677185, 481.55791055000003, 481.6673733, 481.73961375, 481.7569535, 481.8482589, 482.12253830000003, 482.31567535, 482.4485592, 482.6024747, 482.73106029999997, 482.7697315, 482.8250203, 482.84531735, 482.85094664999997, 482.93552365, 483.0494234, 483.14157255, 483.33753750000005, 483.35801449999997, 483.5051057, 483.5485593, 483.58741150000003, 483.6396136, 483.68609960000003, 483.70519785, 483.72529735, 483.7717313, 483.83538669999996, 483.8852778, 483.97255959999995, 484.17077605, 484.26696055, 484.4238786, 484.5528961, 484.64171225, 484.70309335, 484.77217575000003, 484.83201405, 484.8909248, 485.10047729999997, 485.47204454999996, 485.74560675000004, 485.85322755000004, 485.92982335, 486.03716764999996, 486.13213785, 486.2773083999999, 486.3083577, 486.4179374, 486.54105979999997, 486.66108694999997, 486.78111794999995, 486.89340995, 487.14527884999995, 487.35338160000003, 487.39502595, 487.40808225, 487.4798591, 487.77441375, 487.8230385, 487.92938525, 487.99791625, 488.22842275000005, 488.33261330000005, 488.47751795, 488.5649958, 488.77942835, 488.7983709, 488.91998379999995, 489.20124350000003, 489.4170679, 489.48555735, 489.6511513, 489.71632385, 489.78928395, 489.93881665000004, 490.0634212, 490.25491550000004, 490.27728555, 490.2987216, 490.34180145000005, 490.430616, 490.4969731, 490.60488665, 490.7072324, 490.7384717, 490.74571834999995, 490.781893, 491.11030385000004, 491.34589260000007, 491.4463741, 491.48770779999995, 491.5160088, 491.60499975000005, 491.71388505, 491.7771424, 491.79084335000005, 491.79926265, 491.85102505, 492.1263252, 492.19812394999997, 492.2548124, 492.31454205, 492.3834783, 492.4646638, 492.49621555, 492.54348695, 492.59467729999994, 492.6536187, 492.797371, 492.91661185, 492.95861875, 493.1271129, 493.32335715, 493.3733833, 493.39831025, 493.41208415, 493.48133874999996, 493.55937029999996, 493.5876254, 493.6223804, 493.6699413, 493.76301515, 493.8833058, 493.9501008, 494.02761810000004, 494.1618909, 494.35757659999996, 494.42777824999996, 494.50249469999994, 494.66742545, 494.77470905, 494.79895220000003, 494.93738475, 495.2957144, 495.37627275, 495.43804840000007, 495.46978665000006, 495.574449, 495.6971259, 495.91731225, 496.00434900000005, 496.09821605, 496.24307305, 496.3356658, 496.4005003, 496.49983335, 496.58108860000004, 496.60969565000005, 496.6529938, 496.68039625, 496.69639474999997, 496.72834009999997, 496.78792339999995, 496.91171255, 496.9495789, 496.9838222000001, 497.10207075, 497.3819402, 497.3993211, 497.46793885, 497.64108869999995, 497.81016345, 497.86512880000004, 497.88781905, 497.9339038, 497.99589249999997, 498.03388925, 498.05372420000003, 498.1060408, 498.1675105, 498.42361015, 498.47563694999997, 498.62068145, 498.86949660000005, 499.02732549999996, 499.07736094999996, 499.1328117, 499.2365002, 499.3840397, 499.50922595, 499.55749369999995, 499.600823, 499.6472867, 499.76524165, 499.878168, 500.04715039999996, 500.21292395, 500.23474880000003, 500.27401770000006, 500.33415045000004, 500.49866475, 500.6890069, 500.75858474999995, 500.78323835, 500.85401445, 500.94303105, 501.0929151, 501.1744381, 501.24007900000004, 501.48927595000004, 501.5220516, 501.54855195, 501.67050895, 501.9359895, 502.12236444999996, 502.16912274999993, 502.2230205999999, 502.29508450000003, 502.35163339999997, 502.39414385, 502.41363029999997, 502.56393115, 502.6698268, 502.8293525, 502.97669824999997, 503.03085434999997, 503.2766595, 503.33189235, 503.58747730000005, 503.75076065, 503.81663464999997, 503.84990195, 504.1313581, 504.67520165, 504.7390521, 504.7954582, 504.86068495, 504.92002195, 504.98852074999996, 505.08663594999996, 505.1528075, 505.20793634999995, 505.2421865, 505.31424685, 505.3861193, 505.4180719000001, 505.45336205, 505.66916845, 505.718654, 505.75878075, 505.78168454999997, 505.81523644999993, 505.96168415, 506.0469173, 506.08077875, 506.13933654999994, 506.24874695, 506.30708435, 506.41983385000003, 506.5344948, 506.61687625, 506.71554385, 506.81099444999995, 506.8872682, 507.55768424999997, 507.60720395, 507.72628384999996, 507.87273115, 508.11552395, 508.14462355, 508.16861755, 508.19370295, 508.236177, 508.28521579999995, 508.35694215, 508.40712195000003, 508.42832575, 508.70299800000004, 508.80965115000004, 508.91567460000005, 509.2295464, 509.25079535, 509.27061090000007, 509.51213405, 509.83648135, 509.92025954999997, 510.00205890000007, 510.1253924, 510.25549954999997, 510.40126135, 510.5044864, 510.5147677, 510.5265425, 510.5408844, 510.64456745000007, 510.75778620000006, 510.79434875000004, 511.00205975, 511.2048801, 511.3062554, 511.40465294999996, 511.47555209999996, 511.65097675, 511.70285145, 511.78520845, 511.88027975000006, 511.95269905000004, 512.02324515, 512.0460127, 512.19891515, 512.22965875, 512.32049195, 512.39365695, 512.4868146000001, 512.5977625500001, 512.6659556500001, 512.7198798, 512.9503221499999, 513.1988285499999, 513.4762826, 513.6196255, 513.6500865, 513.70203005, 513.9510089, 514.1577423, 514.17425585, 514.2540277, 514.32699155, 514.5079767499999, 514.5382847000001, 514.5717375500001, 514.60004805, 514.73718345, 514.8699769, 514.9508794000001, 515.06738735, 515.1253584000001, 515.15689175, 515.19537105, 515.2668947, 515.3097827500001, 515.4259537, 515.6270184499999, 515.9597968, 516.2335857, 516.3719718, 516.4264627499999, 516.4303431999999, 516.6689857000001, 516.9640516000001, 517.11802595, 517.27422245, 517.4978242499999, 517.7451571, 517.7850201, 517.79895455, 517.8234736500001, 517.9223761000001, 518.08997705, 518.14774585, 518.2468492999999, 518.31684375, 518.71384645, 518.76793595, 518.7892102999999, 518.81956685, 518.85548245, 518.9214066, 519.01181345, 519.15713, 519.19874605, 519.2651859, 519.31934255, 519.39658465, 519.46613075, 519.6354592499999, 519.74651745, 519.78831155, 519.8018832, 520.1488032500001, 520.22679825, 520.41743485, 520.6633847999999, 520.7639062000001, 520.8852763500001, 520.96488845, 521.03826055, 521.07730405, 521.13136865, 521.2003215, 521.2295283000001, 521.27694065, 521.3675895, 521.50782475, 521.63274585, 521.68083355, 521.7507743000001, 521.8253944500001, 522.0066003000001, 522.2806079000001, 522.4399379, 522.56384365, 522.5811792, 522.6122512, 522.6441356, 522.8091448, 522.97436355, 523.0596852, 523.19489, 523.31625265, 523.45865355, 523.53285975, 523.55895575, 523.64689675, 523.7918553, 523.8760352500001, 523.8952009000001, 524.03347795, 524.1924489, 524.3850544, 524.6286296999999, 524.71034685, 524.9223439, 525.0614356000001, 525.1262461, 525.19809695, 525.36040875, 525.52366455, 525.7447423, 525.8682139999999, 525.93742385, 526.0635769, 526.3093312999999, 526.4530541, 526.5292985000001, 526.6462824, 526.7298773499999, 526.7916219, 526.81992835, 526.89491615, 527.1269453, 527.1812572, 527.2459497, 527.4462691, 527.6178626, 527.63862885, 527.77499675, 527.9785600499999, 528.08108165, 528.1391860000001, 528.24437005, 528.2973111, 528.3397155, 528.4367755, 528.50411695, 528.70380385, 528.98879435, 529.0024025499999, 529.01560135, 529.0676472, 529.1415137, 529.2059578999999, 529.3785356000001, 529.3878602, 529.39796615, 529.47140205, 529.59751565, 529.6109200999999, 529.6745354, 529.7938963500001, 529.8478787500001, 529.9158267, 529.9753667, 530.0530416, 530.13316835, 530.27596035, 530.37366535, 530.4072306, 530.45888445, 530.51737955, 530.5849753499999, 530.6510724, 530.7831238, 530.9445536999999, 531.0311282, 531.10750495, 531.1840725999999, 531.2477966499999, 531.2712170499999, 531.69191415, 531.91461255, 532.18408455, 532.3629853, 532.3728158499999, 532.4924698, 532.54876895, 532.58885355, 532.6864398, 532.7608370999999, 532.7968203, 532.89868765, 533.05652855, 533.1665447, 533.4242319, 533.4552954000001, 533.57776555, 533.701129, 533.7152594500001, 533.8273884500001, 533.9395439, 534.05386955, 534.09269275, 534.12353465, 534.1970359500001, 534.3326711, 534.5474196999999, 534.7105242, 534.8021106, 535.08516255, 535.3367300499999, 535.407312, 535.8482145, 536.0824774, 536.1310295, 536.209428, 536.39058095, 536.47641555, 536.63360325, 536.8266524999999, 537.09597455, 537.1715596500001, 537.2281333000001, 537.3288478, 537.4729701, 537.54348435, 537.5980883999999, 537.63634425, 537.7967982, 537.9315102, 537.9816035, 538.0704215, 538.19356475, 538.27815225, 538.4179706, 538.5817743, 538.8036576500001, 539.0168438000001, 539.07663755, 539.1805909, 539.2557995, 539.3672295, 539.3915736, 539.437112, 539.5379998999999, 539.81834385, 539.92642695, 540.0366898, 540.14086425, 540.2348044, 540.2781598500001, 540.3065993, 540.3461648, 540.49588305, 540.6543973999999, 540.77498995, 540.8593347, 540.95521175, 541.1924378, 541.4326671, 541.51831055, 541.7796154, 542.05436045, 542.13788975, 542.165707, 542.3312755, 542.39122465, 542.4132415, 542.4440021, 542.533201, 542.6302401, 542.8125383500001, 542.88930625, 543.0973349999999, 543.1938623, 543.25446055, 543.3073618999999, 543.43959995, 543.4791863999999, 543.5444473499999, 543.85417305, 543.9252282, 543.9706026000001, 544.0590286500001, 544.1382136, 544.19595855, 544.2847470500001, 544.3532912000001, 544.56107795, 544.78531635, 544.8888397, 544.9986296999999, 545.1096599, 545.13843145, 545.2246561, 545.3211408, 545.367754, 545.4917303, 545.6066721, 545.77348955, 545.8320347, 545.91475545, 545.9700699499999, 546.02877775, 546.2219702, 546.4485231000001, 546.56119275, 546.6069579, 546.7013456999999, 546.8152399999999, 546.8619415999999, 546.96575965, 547.15266845, 547.2619631499999, 547.3262370499999, 547.4257782, 547.5066977500001, 547.5385555, 547.6232359, 547.7854083499999, 547.87218405, 547.88484985, 547.96748725, 547.9808752, 548.1201199, 548.2843822, 548.3922718, 548.5527379, 548.6576694500001, 548.69004225, 548.7082519999999, 548.72170035, 548.73198055, 548.8950606000001, 548.9937992499999, 549.1973201999999, 549.3920701499999, 549.44159325, 549.5783265, 549.7260954, 549.84085235, 549.9305464, 549.9432526, 549.9619411000001, 550.08868655, 550.3631323000001, 550.43397725, 550.52892845, 550.64580065, 550.7235545999999, 550.7915635500001, 550.79308425, 550.95156465, 551.04920945, 551.0716403, 551.0876753, 551.10135125, 551.10962175, 551.1644686499999, 551.22460905, 551.24921625, 551.370226, 551.48809565, 551.49926225, 551.6594697, 551.88238955, 552.1416873500001, 552.4414214499999, 552.58446465, 552.653919, 552.8325993, 553.00658185, 553.22429085, 553.4316887499999, 553.4374967, 553.4682174, 553.53366375, 553.5785229, 554.1103752500001, 554.2307316, 554.33894595, 554.4084870500001, 554.5799566999999, 554.7453983999999, 554.76239085, 554.76618345, 554.7833581499999, 554.8518514499999, 554.9006039, 554.9322725999999, 555.0038339, 555.0826708, 555.0988461, 555.1989023, 555.3952919999999, 555.5454948500001, 555.6157141000001, 555.64393025, 555.66001265, 555.74252355, 555.86331165, 555.9013711499999, 556.0142456, 556.1250645499999, 556.23386965, 556.2642489499999, 556.4137739, 556.5151653, 556.55609895, 556.60601115, 556.70327225, 556.8173429000001, 556.9230210000001, 556.9772176500001, 557.0089912000001, 557.057409, 557.0900912, 557.12251635, 557.2937316, 557.5750152, 557.72275955, 557.86118895, 557.9943112, 558.2787888, 558.3469941000001, 558.4236811999999, 558.5226699, 558.5860827000001, 558.6202857000001, 558.74829565, 558.89060585, 558.99344555, 559.07870685, 559.1364588, 559.21285935, 559.38201195, 559.5664859, 559.66151515, 559.7184433, 559.74068505, 559.8108917500001, 559.8806311000001, 560.2565625499999, 560.6233498, 560.6396980000001, 560.6933302, 560.805454, 560.85528765, 560.91419305, 560.94332545, 561.1819626500001, 561.4429377, 561.5515722, 561.6065005999999, 561.68241945, 561.8359129, 561.9513527, 561.9623725, 562.13770755, 562.3650548999999, 562.4782400500001, 562.61211895, 562.6882925, 562.7301443499999, 562.86308815, 562.9548313, 563.15677275, 563.2144428, 563.2264899500001, 563.24071255, 563.2888755500001, 563.34607085, 563.3723043, 563.4663581499999, 563.54803255, 563.5600901, 563.57268875, 563.80354325, 563.91673325, 563.93759985, 563.9549413999999, 564.0073017499999, 564.0991944, 564.40354595, 564.5106234499999, 564.55442675, 564.6084678499999, 564.6502173, 564.741883, 564.8623315, 564.92458015, 565.1235643, 565.30914745, 565.5183686, 565.5799264, 565.6138999, 565.6809145, 565.8145657499999, 565.953951, 566.0047466, 566.0194076, 566.0430595, 566.08704505, 566.26698205, 566.4613357, 566.5496082999999, 566.6222472, 566.7173235, 566.84255095, 566.8980445, 566.9896482, 567.13865425, 567.23809005, 567.3349900999999, 567.386483, 567.51859065, 567.9067842, 568.35101805, 568.50366315, 568.561381, 568.60510865, 568.6790368, 568.7474031500001, 568.85518185, 568.95052855, 569.08290845, 569.206825, 569.260641, 569.31792605, 569.4447341, 569.54081305, 569.5940588000001, 569.9568506, 570.16464735, 570.27502395, 570.3514928, 570.51781185, 570.52374495, 570.53264155, 570.5551513, 570.6561424500001, 570.85074535, 570.9803506000001, 571.0387708000001, 571.11010415, 571.14092145, 571.15266905, 571.2808998, 571.56291755, 571.6708466, 571.7257521500001, 571.9613618999999, 571.9802193, 572.01479905, 572.0663429, 572.19868855, 572.21130975, 572.27702215, 572.42265425, 572.56791565, 572.7283254, 572.83431865, 572.9256992999999, 573.06059835, 573.2644896500001, 573.40578665, 573.57257865, 573.7183881999999, 573.8529675500001, 573.94142925, 573.9852296500001, 574.12314505, 574.2869789000001, 574.29102805, 574.3085896, 574.3369911499999, 574.4065793, 574.4650428, 574.8040920000001, 574.9150059000001, 574.95523575, 575.1265675, 575.34742145, 575.53267095, 575.65018445, 575.78083105, 575.98415795, 576.3102538, 576.5122833, 576.5602973, 576.6168482, 576.6419719, 577.02728255, 577.09940825, 577.1102892, 577.1909002499999, 577.34096525, 577.465142, 577.53409985, 577.6558365000001, 577.8056351500001, 577.9290265999999, 578.0756629499999, 578.5651272, 578.5842754, 578.6055196, 578.7969779499999, 578.8588272, 578.9468489999999, 579.0250136, 579.0725031, 579.15859485, 579.264809, 579.35059505, 579.4004864000001, 579.4578955, 579.4987022, 579.6716825000001, 579.85102235, 579.8849400500001, 579.98691715, 580.0195463499999, 580.1820028, 580.3404816, 580.46324725, 580.6934634500001, 580.8470383, 580.9327367999999, 581.36804905, 581.5491274, 581.6289528, 581.78303605, 582.1999567999999, 582.4497804499999, 582.5830978500001, 582.6729974, 582.8247940000001, 583.1262592999999, 583.18984305, 583.2540162, 583.31715055, 583.5577766000001, 583.6061970999999, 583.7512998999999, 584.0607179000001, 584.31310795, 584.6465971499999, 585.0732296, 585.588996, 585.72395785, 585.8299976999999, 585.9335109499999, 585.9761907499999, 586.0152201, 586.1205493499999, 586.2357466, 586.31467785, 586.4329774, 586.4911590500001, 586.5966738, 586.7295866500001, 586.75834625, 586.7995923999999, 586.8660164, 586.97679225, 587.2762889999999, 587.3393947499999, 587.3892337, 587.53441705, 587.6086133499999, 587.6539731, 588.09531455, 588.1396432000001, 588.20238195, 588.36117205, 588.5718744000001, 588.85753905, 589.1798336500001, 589.3811322500001, 589.6563759999999, 589.9251256499999, 590.12405345, 590.32337285, 590.3898491499999, 590.5343627, 590.69698935, 590.8632259000001, 591.0759520500001, 591.2326346, 591.46572415, 591.60684575, 591.689147, 591.74905425, 591.8538434, 591.926571, 592.09351715, 592.6304191, 593.03616035, 593.1963155999999, 593.4854725, 593.7668944, 593.9171617999999, 594.0274089, 594.15250515, 594.2066318, 594.26714975, 594.58521225, 594.6542998, 594.6849611, 594.700881, 594.79114455, 594.934663, 595.0398814499999, 595.0754211000001, 595.1122482000001, 595.3431403500001, 595.6157032000001, 595.8857664, 595.9990512500001, 596.1692948499999, 596.38211595, 596.44478085, 596.86090265, 597.14050085, 597.24258575, 597.48860155, 597.6803417, 597.8746462500001, 598.10918975, 598.3505625500001, 598.46027315, 598.53069815, 598.55855465, 598.63804695, 598.72253165, 598.86810575, 598.9457774, 599.2843717000001, 599.52795375, 599.7254613, 599.7649781499999, 599.9394757, 600.4336647499999, 600.7367102000001, 600.94231595, 601.13527705, 601.1485121999999, 601.26386785, 601.56876195, 601.97644585, 602.3490164, 602.56626385, 602.69368515, 602.7900327, 602.8247306, 602.9817760999999, 603.0972193499999, 603.25594645, 603.4302428999999, 603.5147568, 603.5826739, 603.61652785, 603.6570792, 603.7019875, 603.9363743499999, 604.1738147, 604.2372069, 604.3642973999999, 604.9887577, 605.2178096499999, 605.53423555, 605.64156415, 605.97323165, 606.12731105, 606.5562752999999, 606.6722428, 606.7781937, 607.1707287500001, 607.25093125, 607.31052125, 607.7641511500001, 607.8494711999999, 607.9645072000001, 608.1744121500001, 608.3220213500001, 608.60333685, 608.9288521999999, 609.0215617499999, 609.1129165, 609.2132703999999, 609.4235391, 609.5978098999999, 609.7595050499999, 609.7901107, 609.8387521499999, 609.89884765, 610.0726106000001, 610.2353593, 610.42401565, 610.55416515, 610.67200115, 610.9243196, 611.1028875500001, 611.3270090999999, 611.42360335, 611.4924382, 611.53025315, 611.614304, 611.7135541499999, 611.8151544, 611.9183472, 612.0310187, 612.1473751999999, 612.4101641499999, 612.77907735, 612.9218899, 613.13911365, 613.4016686499999, 613.45702115, 613.47836515, 613.6310094, 613.8155439, 614.0914296999999, 614.4903942000001, 614.771788, 614.85579945, 615.0248125999999, 615.1276151, 615.1415646, 615.43300325, 615.5138563, 615.5932744, 615.7141559500001, 615.8956676500001, 617.00479615, 617.16534755, 617.3280261, 617.8682710999999, 617.9037386, 617.98241345, 618.0733285, 618.3823272500001, 618.4270002000001, 618.85547465, 618.9948207, 619.0775304, 619.1864082, 619.3872848999999, 619.83493065, 619.9594164499999, 619.9961102499999, 620.0624723999999, 620.2750285, 620.4277614499999, 620.4727869999999, 620.5087607, 620.540039, 620.6389442, 620.7213356, 620.8102503499999, 620.9429203, 621.25446065, 621.43552885, 621.6983121999999, 622.3239052000001, 622.85074535, 622.9643391499999, 623.1703311, 623.3656816, 623.5897558, 623.6899767, 624.18670755, 624.2589878000001, 624.3851474999999, 624.5499284499999, 624.67294695, 624.68949795, 624.69525935, 624.74299145, 624.8459624, 624.9231507, 624.94286355, 625.0096765999999, 625.5157088, 625.6265186, 625.752062, 625.8891281, 625.9306304, 625.9596734500001, 626.01254925, 626.3267378999999, 626.64355325, 626.8625729, 626.97428685, 627.0402024, 627.6342928500001, 627.6486445, 627.6585708499999, 627.9327329, 628.64255815, 628.8284156, 629.0317980499999, 629.147136, 629.4427083, 629.7167226500001, 629.820314, 629.89779665, 629.9550878, 630.08119945, 630.1272454, 630.2199215, 630.2502254, 630.3458712500001, 630.52176725, 630.6411321, 630.7918402, 630.9725109000001, 631.0891917, 631.3096014, 631.4500849000001, 631.6057579999999, 631.8316324, 631.9430207999999, 631.95594735, 631.9695854, 632.02895335, 632.0817905, 632.3244021999999, 632.46632485, 632.7723754000001, 633.1242801000001, 633.3373752499999, 633.4013884, 633.4349890999999, 633.4847744499999, 633.5528479, 633.67944425, 633.7729998499999, 633.83612065, 634.5978854, 634.7470910500001, 634.8301515, 635.0922456000001, 635.4268288000001, 635.7569625499999, 636.1193349, 636.3237318, 636.3767661, 636.52198415, 636.84546245, 637.05388685, 637.12996735, 637.18371955, 637.66656465, 638.1955820000001, 638.2881685499999, 638.4152831499999, 638.5433845499999, 638.633236, 638.7105359, 638.8402626, 638.94672445, 639.1029459, 639.2027556, 639.30116125, 639.4050150999999, 639.43772375, 639.68416465, 639.96204315, 640.1206880499999, 640.4239268499999, 640.667942, 640.70354005, 640.78354115, 640.9900081999999, 641.43427625, 641.91630935, 642.0961158499999, 642.24540035, 642.3594616999999, 642.91486165, 643.4392619, 643.8242732, 643.90672345, 643.9872445999999, 644.1662675, 644.3892937, 644.65201865, 644.9425086, 645.0115613999999, 645.0975628, 645.401208, 645.8345635999999, 646.1742919000001, 646.25887035, 646.29432855, 646.5759642, 647.6673564, 647.76882275, 647.97803665, 648.0561705499999, 648.1798486, 648.27252475, 648.3018445499999, 648.4173587, 648.9507360499999, 649.0934797, 649.1590102, 649.2130468, 649.38130005, 649.55866085, 650.13028335, 650.232202, 650.3059456000001, 650.4770062, 650.6122978, 650.6656762499999, 650.7367069, 651.1944249, 651.2990036, 651.3774789, 651.50955145, 651.6153596, 651.65483595, 651.7047273, 651.74108895, 651.81466335, 652.0041094000001, 652.2012935, 652.2742464, 652.3168038000001, 652.38116835, 652.3900531, 652.4360312, 652.6856092, 653.1743844, 653.23840045, 653.3022915500001, 653.3513871499999, 653.3993052000001, 653.4295157, 653.5295269000001, 653.61930575, 653.8872002500001, 654.1456377500001, 654.92830905, 655.2266137, 655.26755235, 655.47146125, 655.527662, 655.7025182, 655.8395546500001, 655.9018298999999, 655.9920067, 656.3923816500001, 656.8083583499999, 656.9413915499999, 657.12004815, 657.3676266, 657.5902827, 657.7291074, 658.0466956, 658.3862984, 658.84329445, 658.9867608, 659.0613494, 659.2392294499999, 659.6539975000001, 659.7716288, 659.8844702, 660.2963382, 660.65694105, 660.71243925, 660.7264303, 660.75738675, 660.891602, 661.08029135, 661.4771404, 661.8454322, 662.1053786, 662.3008597, 663.1891907500001, 663.3817949, 663.91055355, 664.3275742000001, 664.5052307000001, 664.7759767, 665.11827435, 665.5262191499999, 665.8143711, 666.04027495, 666.2577087500001, 666.4539160500001, 666.51458155, 666.71231345, 666.91513105, 666.9441856000001, 666.9663482, 667.02020485, 667.13419385, 667.2484510500001, 667.40733085, 667.65544865, 667.7805920000001, 668.1047867500001, 668.2845144500001, 668.4994054, 668.7788398499999, 669.46341775, 669.58364135, 669.88279025, 670.4432954, 670.8505007, 670.94507575, 671.2421906, 671.4080273000001, 671.6508473000001, 672.0202092, 672.23993195, 672.7288846499999, 673.1883841499999, 673.53028855, 673.6684642499999, 673.85807415, 674.1155032500001, 674.57418245, 674.8137875499999, 674.8235342, 675.0366853, 675.29104305, 675.3663734500001, 675.5882214999999, 675.90603775, 676.0318901, 676.4278064499999, 676.75557785, 677.17588505, 677.30808185, 677.3383695, 677.4532532, 677.6853242999999, 677.83471505, 677.8632379500001, 677.87745445, 678.02992735, 678.18864535, 678.47668055, 679.1499993, 679.9456081000001, 680.3365724, 681.6363173, 682.0990627, 682.2816306999999, 682.35005555, 682.42654155, 682.6196691, 682.8447199999999, 683.1465708, 683.58206535, 684.16251235, 684.46645195, 684.7185361500001, 684.89624655, 684.96057795, 685.1982869999999, 685.45061485, 685.70322815, 686.14629835, 686.4422052499999, 686.61078705, 686.7538829499999, 686.8166300999999, 686.9156598, 687.0311709499999, 687.2554813, 687.4190250500001, 687.45146445, 687.51389345, 687.60212015, 688.0732855000001, 688.329421, 688.7406481, 689.3269221, 689.8952801, 689.9454031, 690.1089262, 690.3275318000001, 690.4350591, 691.02293905, 691.2593093999999, 691.52168575, 691.6505326, 691.7558715, 692.1373629499999, 692.52885465, 692.5826950000001, 692.75004245, 693.1622924999999, 693.5045077499999, 693.6652296, 693.7722167499999, 693.9250466999999, 694.8950886, 694.92000955, 694.9456113, 694.9910608, 695.0471142, 695.4701514000001, 695.96796865, 696.3236146, 696.7346492500001, 696.96663375, 697.1757679, 698.1205593499999, 698.5824144, 698.9069460999999, 698.9916314999999, 699.3551600999999, 699.7621345499999, 700.0028494999999, 700.1693918999999, 700.4466872, 700.70341585, 700.8067345500001, 702.0565056, 702.77502515, 703.40972385, 704.18907235, 704.5984437, 704.6552481, 704.7495363, 704.8631002, 704.92646995, 705.2904089, 705.347381, 705.4419182500001, 705.5452987000001, 705.96738115, 706.3560293, 706.9330399, 707.1337554500001, 707.3527987499999, 707.7161896, 708.0562799, 708.1769685, 708.5831600500001, 708.9566026, 709.3630605000001, 710.2438536999999, 710.4171646, 710.5802851999999, 710.71955085, 710.9093119000001, 711.1802715000001, 711.44746495, 711.97008695, 712.5207365, 712.7362358, 712.9452306000001, 713.3257213499999, 713.6337103999999, 714.13538445, 714.2762259, 714.4016123, 714.6636193500001, 715.09105915, 715.76709475, 716.7459379, 717.4758973999999, 717.7053317, 718.64334125, 719.0648408, 719.1417009, 720.0017471, 720.8983900999999, 721.6230995, 722.13146545, 722.4894285, 722.5236656999999, 722.69238645, 723.08820765, 723.44107045, 723.5699581000001, 723.90399415, 724.2335305500001, 724.3692534500001, 724.7920156, 725.5347568, 726.0620479500001, 727.6428042, 728.2643003000001, 728.5487846000001, 729.17118, 729.40651795, 729.56984495, 730.0820972500001, 730.6455183, 730.7322267500001, 734.2206157999999, 734.51336065, 734.5993669, 735.4125849, 735.71328115, 735.99678255, 736.1627705, 736.7056116000001, 737.25162305, 738.08725925, 738.5394311, 738.8803591999999, 740.31336045, 741.1180187499999, 742.6545040000001, 744.17865885, 744.50238815, 744.915535, 745.34234105, 745.4594508499999, 745.5466311499999, 745.6820872999999, 745.82303215, 746.55871505, 747.8049777, 748.1132414, 748.5645916000001, 748.7315761, 749.1914276499999, 749.4907588, 749.84059665, 750.0925939, 750.9599048, 751.87402315, 752.52419245, 753.2624731000001, 753.33789085, 753.5810918499999, 753.989885, 754.41545605, 755.17002015, 755.8177805, 756.1880226999999, 756.6982823999999, 759.4436507500001, 761.0093754000001, 764.1669659500001, 764.2592674, 764.3376304999999, 765.0952299, 769.22226185, 769.52356505, 769.7367312, 770.5357311500001, 772.0338233, 773.08062845, 773.2470071500001, 773.2868674000001, 773.3317773, 773.7196829500001, 773.9191658500001, 777.7678662000001, 778.4427891, 779.2555819499999, 779.82639935, 780.2635803999999, 784.88725115, 787.1360563, 787.47828215, 791.2653253000001, 791.60496255, 797.0950806999999, 797.10586625, 798.1425200000001, 799.05224305, 799.21757045, 799.7221835, 801.5230282499999, 803.1469649, 804.31191375, 805.2904616999999, 806.79514895, 806.84941245, 808.1344084, 809.8314531999999, 811.6107806499999, 813.6754081500001, 814.5759589500001, 815.53448015, 821.6440622, 822.94070515, 825.0129996500001, 825.87379835, 835.6924609, 836.87843095, 837.394355, 841.17180115, 844.96641375, 846.40385035, 847.0042204, 847.5145984, 854.0256023500001, 855.1710133500001, 862.6675686, 867.6712781000001, 889.05881445, 890.92104695, 895.6493177500001, 898.93450255, 902.7553371500001, 907.5534317500001, 911.3979507500001, 912.4562938, 941.97147835, 946.1309986499999, 950.7441473499999, 954.5544786])
labels = array([1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 0.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 4.0, 0.0, 1.0, 0.0, 4.0, 0.0, 1.0, 4.0, 1.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 0.0, 1.0, 3.0, 1.0, 4.0, 3.0, 1.0, 0.0, 1.0, 2.0, 0.0, 4.0, 1.0, 0.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 3.0, 1.0, 3.0, 1.0, 0.0, 1.0, 4.0, 1.0, 3.0, 1.0, 0.0, 2.0, 4.0, 1.0, 0.0, 4.0, 0.0, 1.0, 4.0, 1.0, 2.0, 1.0, 2.0, 0.0, 3.0, 1.0, 0.0, 4.0, 1.0, 2.0, 1.0, 4.0, 2.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 4.0, 2.0, 4.0, 1.0, 4.0, 0.0, 4.0, 0.0, 4.0, 3.0, 4.0, 0.0, 1.0, 3.0, 1.0, 0.0, 1.0, 3.0, 4.0, 1.0, 4.0, 1.0, 2.0, 1.0, 4.0, 2.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 4.0, 0.0, 4.0, 1.0, 4.0, 1.0, 2.0, 1.0, 3.0, 2.0, 4.0, 1.0, 0.0, 2.0, 1.0, 2.0, 4.0, 0.0, 4.0, 2.0, 0.0, 1.0, 2.0, 4.0, 1.0, 3.0, 1.0, 4.0, 1.0, 4.0, 3.0, 1.0, 4.0, 1.0, 4.0, 2.0, 4.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 2.0, 1.0, 0.0, 1.0, 4.0, 1.0, 3.0, 1.0, 0.0, 3.0, 4.0, 2.0, 4.0, 3.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 4.0, 2.0, 3.0, 1.0, 0.0, 3.0, 4.0, 1.0, 4.0, 1.0, 0.0, 1.0, 4.0, 3.0, 1.0, 2.0, 4.0, 2.0, 1.0, 0.0, 3.0, 1.0, 3.0, 2.0, 3.0, 1.0, 3.0, 0.0, 1.0, 3.0, 1.0, 4.0, 1.0, 2.0, 1.0, 4.0, 3.0, 1.0, 2.0, 1.0, 4.0, 1.0, 2.0, 3.0, 1.0, 2.0, 4.0, 2.0, 0.0, 4.0, 2.0, 1.0, 3.0, 1.0, 4.0, 2.0, 1.0, 4.0, 0.0, 2.0, 0.0, 4.0, 3.0, 1.0, 4.0, 0.0, 3.0, 2.0, 1.0, 0.0, 2.0, 3.0, 2.0, 0.0, 2.0, 4.0, 0.0, 2.0, 1.0, 4.0, 1.0, 3.0, 4.0, 0.0, 1.0, 2.0, 1.0, 4.0, 3.0, 2.0, 1.0, 4.0, 3.0, 1.0, 2.0, 4.0, 1.0, 4.0, 1.0, 2.0, 3.0, 4.0, 1.0, 0.0, 4.0, 1.0, 2.0, 3.0, 0.0, 1.0, 3.0, 1.0, 0.0, 4.0, 0.0, 4.0, 0.0, 1.0, 3.0, 0.0, 3.0, 2.0, 0.0, 4.0, 1.0, 0.0, 3.0, 4.0, 1.0, 2.0, 3.0, 0.0, 2.0, 1.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 2.0, 1.0, 3.0, 1.0, 3.0, 1.0, 2.0, 3.0, 1.0, 0.0, 2.0, 4.0, 0.0, 1.0, 4.0, 1.0, 2.0, 4.0, 2.0, 3.0, 4.0, 2.0, 1.0, 4.0, 1.0, 4.0, 1.0, 4.0, 3.0, 1.0, 0.0, 1.0, 4.0, 1.0, 3.0, 0.0, 3.0, 0.0, 4.0, 3.0, 2.0, 0.0, 2.0, 3.0, 2.0, 3.0, 2.0, 4.0, 2.0, 4.0, 1.0, 2.0, 1.0, 0.0, 3.0, 4.0, 1.0, 4.0, 0.0, 4.0, 2.0, 3.0, 1.0, 0.0, 1.0, 2.0, 4.0, 3.0, 2.0, 1.0, 3.0, 2.0, 1.0, 4.0, 0.0, 1.0, 3.0, 2.0, 0.0, 4.0, 0.0, 4.0, 0.0, 3.0, 2.0, 4.0, 0.0, 4.0, 1.0, 4.0, 2.0, 1.0, 2.0, 1.0, 0.0, 4.0, 2.0, 1.0, 2.0, 4.0, 1.0, 4.0, 1.0, 4.0, 1.0, 3.0, 2.0, 0.0, 4.0, 1.0, 4.0, 2.0, 1.0, 2.0, 3.0, 4.0, 1.0, 0.0, 3.0, 2.0, 4.0, 1.0, 4.0, 3.0, 4.0, 1.0, 2.0, 4.0, 2.0, 1.0, 4.0, 1.0, 3.0, 1.0, 0.0, 1.0, 2.0, 4.0, 0.0, 4.0, 1.0, 0.0, 1.0, 0.0, 1.0, 4.0, 3.0, 4.0, 0.0, 1.0, 0.0, 3.0, 2.0, 4.0, 1.0, 3.0, 4.0, 2.0, 3.0, 1.0, 0.0, 1.0, 4.0, 0.0, 3.0, 0.0, 3.0, 4.0, 1.0, 0.0, 1.0, 0.0, 2.0, 4.0, 0.0, 3.0, 4.0, 1.0, 4.0, 1.0, 0.0, 4.0, 2.0, 4.0, 2.0, 1.0, 3.0, 2.0, 4.0, 3.0, 2.0, 0.0, 1.0, 3.0, 2.0, 4.0, 3.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 2.0, 4.0, 0.0, 1.0, 2.0, 0.0, 2.0, 1.0, 2.0, 0.0, 1.0, 4.0, 3.0, 4.0, 0.0, 2.0, 3.0, 0.0, 1.0, 2.0, 4.0, 0.0, 1.0, 4.0, 0.0, 2.0, 4.0, 1.0, 2.0, 0.0, 3.0, 0.0, 4.0, 2.0, 4.0, 2.0, 4.0, 0.0, 1.0, 4.0, 0.0, 1.0, 3.0, 0.0, 1.0, 4.0, 1.0, 2.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 4.0, 1.0, 4.0, 1.0, 3.0, 0.0, 2.0, 4.0, 2.0, 1.0, 3.0, 2.0, 1.0, 2.0, 4.0, 3.0, 1.0, 3.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 4.0, 1.0, 0.0, 2.0, 3.0, 0.0, 4.0, 0.0, 1.0, 4.0, 1.0, 3.0, 4.0, 1.0, 4.0, 3.0, 2.0, 3.0, 2.0, 1.0, 3.0, 2.0, 4.0, 0.0, 1.0, 2.0, 3.0, 4.0, 2.0, 1.0, 0.0, 4.0, 0.0, 4.0, 2.0, 0.0, 2.0, 4.0, 0.0, 1.0, 0.0, 2.0, 1.0, 2.0, 4.0, 0.0, 3.0, 4.0, 2.0, 1.0, 0.0, 4.0, 1.0, 0.0, 2.0, 1.0, 2.0, 4.0, 2.0, 4.0, 0.0, 1.0, 0.0, 4.0, 2.0, 3.0, 1.0, 4.0, 0.0, 4.0, 2.0, 4.0, 2.0, 4.0, 3.0, 2.0, 3.0, 2.0, 0.0, 2.0, 0.0, 4.0, 0.0, 3.0, 1.0, 3.0, 4.0, 0.0, 1.0, 4.0, 3.0, 4.0, 3.0, 2.0, 4.0, 1.0, 0.0, 4.0, 2.0, 0.0, 1.0, 4.0, 1.0, 2.0, 4.0, 3.0, 0.0, 4.0, 0.0, 3.0, 4.0, 2.0, 3.0, 2.0, 4.0, 1.0, 2.0, 3.0, 2.0, 4.0, 3.0, 2.0, 1.0, 3.0, 4.0, 2.0, 3.0, 1.0, 3.0, 0.0, 3.0, 4.0, 0.0, 2.0, 4.0, 3.0, 1.0, 4.0, 1.0, 4.0, 3.0, 0.0, 3.0, 2.0, 3.0, 4.0, 2.0, 1.0, 4.0, 3.0, 1.0, 0.0, 4.0, 3.0, 0.0, 4.0, 1.0, 0.0, 1.0, 3.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 3.0, 2.0, 3.0, 4.0, 3.0, 0.0, 1.0, 2.0, 3.0, 1.0, 0.0, 4.0, 3.0, 4.0, 3.0, 2.0, 0.0, 4.0, 0.0, 4.0, 1.0, 3.0, 0.0, 3.0, 0.0, 2.0, 3.0, 4.0, 1.0, 3.0, 0.0, 4.0, 1.0, 2.0, 3.0, 0.0, 4.0, 1.0, 4.0, 3.0, 1.0, 2.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 3.0, 2.0, 1.0, 3.0, 0.0, 2.0, 1.0, 2.0, 0.0, 2.0, 4.0, 2.0, 4.0, 0.0, 1.0, 3.0, 1.0, 4.0, 3.0, 2.0, 1.0, 3.0, 4.0, 3.0, 2.0, 3.0, 2.0, 3.0, 4.0, 1.0, 4.0, 3.0, 4.0, 2.0, 1.0, 0.0, 4.0, 3.0, 0.0, 2.0, 4.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 4.0, 3.0, 4.0, 1.0, 0.0, 3.0, 4.0, 0.0, 1.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 4.0, 3.0, 0.0, 3.0, 2.0, 1.0, 4.0, 3.0, 4.0, 2.0, 4.0, 0.0, 2.0, 1.0, 3.0, 1.0, 0.0, 4.0, 3.0, 2.0, 1.0, 4.0, 2.0, 0.0, 4.0, 2.0, 4.0, 1.0, 4.0, 1.0, 0.0, 4.0, 1.0, 4.0, 0.0, 4.0, 3.0, 4.0, 3.0, 1.0, 4.0, 0.0, 3.0, 4.0, 0.0, 3.0, 0.0, 1.0, 0.0, 4.0, 2.0, 0.0, 2.0, 4.0, 1.0, 0.0, 4.0, 1.0, 2.0, 1.0, 4.0, 1.0, 4.0, 3.0, 2.0, 1.0, 4.0, 0.0, 2.0, 0.0, 1.0, 2.0, 4.0, 0.0, 3.0, 0.0, 4.0, 2.0, 1.0, 4.0, 2.0, 0.0, 4.0, 3.0, 4.0, 1.0, 3.0, 2.0, 3.0, 2.0, 1.0, 2.0, 4.0, 1.0, 2.0, 0.0, 4.0, 3.0, 1.0, 0.0, 1.0, 4.0, 1.0, 0.0, 1.0, 3.0, 4.0, 2.0, 3.0, 2.0, 1.0, 4.0, 3.0, 1.0, 0.0, 2.0, 4.0, 0.0, 2.0, 0.0, 4.0, 2.0, 3.0, 1.0, 3.0, 0.0, 2.0, 4.0, 3.0, 1.0, 3.0, 2.0, 0.0, 1.0, 4.0, 1.0, 3.0, 2.0, 1.0, 4.0, 3.0, 0.0, 2.0, 4.0, 3.0, 2.0, 3.0, 2.0, 0.0, 1.0, 2.0, 4.0, 2.0, 0.0, 2.0, 1.0, 0.0, 1.0, 3.0, 4.0, 2.0, 1.0, 0.0, 1.0, 0.0, 2.0, 4.0, 3.0, 2.0, 3.0, 0.0, 1.0, 3.0, 2.0, 0.0, 4.0, 2.0, 0.0, 4.0, 0.0, 2.0, 4.0, 0.0, 1.0, 2.0, 0.0, 4.0, 1.0, 0.0, 2.0, 1.0, 3.0, 1.0, 3.0, 4.0, 0.0, 4.0, 0.0, 2.0, 1.0, 0.0, 3.0, 4.0, 0.0, 3.0, 2.0, 4.0, 3.0, 4.0, 3.0, 0.0, 1.0, 3.0, 4.0, 0.0, 4.0, 0.0, 2.0, 1.0, 4.0, 0.0, 2.0, 1.0, 4.0, 1.0, 4.0, 1.0, 3.0, 4.0, 1.0, 2.0, 0.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 4.0, 2.0, 3.0, 2.0, 4.0, 1.0, 4.0, 1.0, 3.0, 4.0, 2.0, 1.0, 3.0, 0.0, 4.0, 2.0, 3.0, 4.0, 0.0, 1.0, 3.0, 4.0, 1.0, 0.0, 2.0, 4.0, 1.0, 0.0, 4.0, 3.0, 0.0, 3.0, 0.0, 3.0, 2.0, 4.0, 0.0, 1.0, 0.0, 1.0, 4.0, 0.0, 2.0, 4.0, 1.0, 4.0, 1.0, 4.0, 3.0, 0.0, 3.0, 4.0, 3.0, 1.0, 4.0, 0.0, 4.0, 3.0, 1.0, 4.0, 3.0, 1.0, 3.0, 4.0, 0.0, 4.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 2.0, 4.0, 0.0, 3.0, 4.0, 3.0, 0.0, 1.0, 4.0, 0.0, 4.0, 1.0, 3.0, 4.0, 1.0, 3.0, 0.0, 4.0, 3.0, 4.0, 0.0, 4.0, 1.0, 4.0, 1.0, 4.0, 2.0, 0.0, 1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 2.0, 1.0, 3.0, 0.0, 3.0, 2.0, 3.0, 4.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 0.0, 3.0, 4.0, 3.0, 0.0, 4.0, 1.0, 4.0, 3.0, 4.0, 0.0, 3.0, 4.0, 1.0, 3.0, 1.0, 4.0, 3.0, 4.0, 2.0, 4.0, 0.0, 4.0, 2.0, 1.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 4.0, 0.0, 4.0, 3.0, 0.0, 4.0, 1.0, 0.0, 3.0, 4.0, 1.0, 3.0, 1.0, 0.0, 2.0, 0.0, 2.0, 3.0, 4.0, 2.0, 1.0, 3.0, 1.0, 0.0, 4.0, 3.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 3.0, 1.0, 3.0, 4.0, 3.0, 4.0, 2.0, 3.0, 2.0, 1.0, 0.0, 4.0, 0.0, 3.0, 2.0, 1.0, 4.0, 2.0, 0.0, 4.0, 1.0, 3.0, 4.0, 3.0, 0.0, 3.0, 4.0, 1.0, 3.0, 2.0, 1.0, 3.0, 4.0, 0.0, 3.0, 1.0, 3.0, 1.0, 2.0, 0.0, 1.0, 3.0, 0.0, 4.0, 0.0, 1.0, 2.0, 1.0, 4.0, 1.0, 4.0, 0.0, 3.0, 1.0, 4.0, 1.0, 3.0, 0.0, 1.0, 2.0, 3.0, 0.0, 1.0, 4.0, 3.0, 0.0, 2.0, 4.0, 3.0, 0.0, 3.0, 4.0, 0.0, 3.0, 4.0, 2.0, 0.0, 2.0, 0.0, 3.0, 0.0, 4.0, 0.0, 1.0, 0.0, 1.0, 3.0, 1.0, 0.0, 2.0, 4.0, 3.0, 0.0, 2.0, 3.0, 0.0, 1.0, 3.0, 4.0, 0.0, 3.0, 4.0, 2.0, 3.0, 1.0, 2.0, 1.0, 3.0, 0.0, 2.0, 3.0, 0.0, 1.0, 4.0, 0.0, 3.0, 0.0, 3.0, 1.0, 3.0, 0.0, 1.0, 2.0, 4.0, 1.0, 3.0, 1.0, 3.0, 0.0, 3.0, 4.0, 0.0, 1.0, 4.0, 3.0, 0.0, 1.0, 4.0, 0.0, 3.0, 1.0, 4.0, 0.0, 4.0, 3.0, 1.0, 0.0, 4.0, 3.0, 4.0, 1.0, 3.0, 4.0, 2.0, 4.0, 1.0, 4.0, 3.0, 0.0, 4.0, 1.0, 4.0, 3.0, 1.0, 3.0, 4.0, 0.0, 3.0, 1.0, 4.0, 3.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, 3.0, 0.0, 4.0, 2.0, 1.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 2.0, 0.0, 3.0, 2.0, 4.0, 0.0, 3.0, 4.0, 0.0, 3.0, 4.0, 2.0, 4.0, 2.0, 4.0, 2.0, 0.0, 1.0, 2.0, 1.0, 3.0, 1.0, 4.0, 0.0, 4.0, 3.0, 1.0, 3.0, 1.0, 3.0, 0.0, 3.0, 4.0, 3.0, 0.0, 4.0, 3.0, 1.0, 0.0, 4.0, 3.0, 2.0, 4.0, 3.0, 1.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 1.0, 2.0, 3.0, 2.0, 1.0, 3.0, 0.0, 3.0, 0.0, 2.0, 0.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 1.0, 4.0, 0.0, 1.0, 4.0, 1.0, 0.0, 2.0, 4.0, 3.0, 4.0, 1.0, 0.0, 2.0, 4.0, 1.0, 3.0, 4.0, 0.0, 1.0, 4.0, 3.0, 4.0, 0.0, 4.0, 2.0, 3.0, 1.0, 3.0, 4.0, 0.0, 1.0, 3.0, 4.0, 1.0, 4.0, 3.0, 0.0, 4.0, 0.0, 1.0, 3.0, 4.0, 0.0, 2.0, 0.0, 1.0, 0.0, 4.0, 3.0, 1.0, 3.0, 4.0, 0.0, 4.0, 0.0, 4.0, 1.0, 0.0, 1.0, 3.0, 4.0, 3.0, 1.0, 4.0, 3.0, 4.0, 1.0, 4.0, 0.0, 1.0, 0.0, 1.0, 0.0, 4.0, 3.0, 4.0, 3.0, 0.0, 4.0, 3.0, 4.0, 2.0, 4.0, 1.0, 2.0, 4.0, 0.0, 1.0, 2.0, 3.0, 4.0, 0.0, 2.0, 1.0, 4.0, 0.0, 1.0, 2.0, 0.0, 2.0, 3.0, 4.0, 1.0, 2.0, 0.0, 1.0, 3.0, 0.0, 2.0, 1.0, 0.0, 3.0, 4.0, 0.0, 2.0, 3.0, 0.0, 1.0, 2.0, 1.0, 3.0, 2.0, 0.0, 2.0, 3.0, 1.0, 0.0, 4.0, 1.0, 2.0, 0.0, 1.0, 4.0, 3.0, 4.0, 0.0, 1.0, 3.0, 0.0, 1.0, 0.0, 4.0, 0.0, 4.0, 0.0, 1.0, 0.0, 4.0, 0.0, 1.0, 0.0, 2.0, 3.0, 0.0, 2.0, 0.0, 4.0, 3.0, 0.0, 1.0, 4.0, 3.0, 0.0, 2.0, 3.0, 1.0, 0.0, 1.0, 4.0, 3.0, 0.0, 4.0, 0.0, 4.0, 1.0, 2.0, 3.0, 0.0, 4.0, 3.0, 4.0, 1.0, 3.0, 0.0, 2.0, 3.0, 4.0, 0.0, 4.0, 3.0, 0.0, 3.0, 1.0, 2.0, 4.0, 3.0, 2.0, 3.0, 2.0, 4.0, 3.0, 2.0, 3.0, 4.0, 1.0, 2.0, 0.0, 2.0, 1.0, 4.0, 1.0, 3.0, 2.0, 0.0, 3.0, 1.0, 0.0, 4.0, 1.0, 3.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.0, 3.0, 2.0, 0.0, 1.0, 2.0, 0.0, 3.0, 2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 3.0, 0.0, 3.0, 4.0, 0.0, 4.0, 3.0, 0.0, 3.0, 4.0, 1.0, 3.0, 4.0, 1.0, 3.0, 4.0, 1.0, 0.0, 3.0, 1.0, 3.0, 0.0, 2.0, 1.0, 2.0, 1.0, 4.0, 3.0, 0.0, 3.0, 1.0, 3.0, 1.0, 2.0, 3.0, 2.0, 4.0, 3.0, 1.0, 3.0, 4.0, 3.0, 4.0, 2.0, 0.0, 2.0, 3.0, 0.0, 2.0, 4.0, 1.0, 2.0, 0.0, 4.0, 3.0, 0.0, 2.0, 3.0, 2.0, 3.0, 4.0, 3.0, 1.0, 4.0, 0.0, 2.0, 4.0, 2.0, 4.0, 3.0, 4.0, 0.0, 1.0, 2.0, 0.0, 3.0, 0.0, 3.0, 4.0, 1.0, 0.0, 4.0, 2.0, 0.0, 2.0, 1.0, 4.0, 1.0, 0.0, 2.0, 1.0, 0.0, 4.0, 1.0, 3.0, 4.0, 0.0, 3.0, 1.0, 3.0, 0.0, 4.0, 2.0, 3.0, 1.0, 4.0, 0.0, 1.0, 4.0, 0.0, 2.0, 3.0, 1.0, 2.0, 3.0, 0.0, 2.0, 0.0, 3.0, 4.0, 1.0, 4.0, 2.0, 1.0, 4.0, 3.0, 1.0, 0.0, 3.0, 2.0, 4.0, 0.0, 1.0, 4.0, 3.0, 1.0, 4.0, 2.0, 3.0, 0.0, 3.0, 2.0, 3.0, 0.0, 3.0, 2.0, 4.0, 2.0, 0.0, 2.0, 3.0, 0.0, 4.0, 1.0, 2.0, 4.0, 1.0, 2.0, 3.0, 1.0, 4.0, 1.0, 2.0, 4.0, 3.0, 4.0, 2.0, 1.0, 3.0, 0.0, 3.0, 4.0, 3.0, 0.0, 4.0, 3.0, 2.0, 4.0, 0.0, 3.0, 1.0, 4.0, 2.0, 1.0, 0.0, 2.0, 0.0, 4.0, 1.0, 2.0, 0.0, 1.0, 2.0, 4.0, 1.0, 2.0, 4.0, 1.0, 4.0, 2.0, 0.0, 2.0, 4.0, 2.0, 4.0, 2.0, 3.0, 2.0, 4.0, 1.0, 3.0, 1.0, 0.0, 1.0, 4.0, 0.0, 4.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, 3.0, 4.0, 3.0, 0.0, 3.0, 4.0, 3.0, 0.0, 2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, 0.0, 4.0, 2.0, 4.0, 0.0, 2.0, 0.0, 1.0, 3.0, 0.0, 2.0, 1.0, 4.0, 3.0, 1.0, 4.0, 0.0, 3.0, 2.0, 3.0, 4.0, 1.0, 0.0, 3.0, 4.0, 0.0, 3.0, 1.0, 4.0, 0.0, 3.0, 0.0, 1.0, 3.0, 2.0, 1.0, 2.0, 1.0, 2.0, 0.0, 1.0, 0.0, 4.0, 1.0, 0.0, 4.0, 2.0, 3.0, 0.0, 1.0, 2.0, 3.0, 4.0, 2.0, 1.0, 3.0, 1.0, 2.0, 0.0, 4.0, 2.0, 1.0, 0.0, 2.0, 3.0, 0.0, 2.0, 1.0, 4.0, 3.0, 1.0, 3.0, 2.0, 1.0, 0.0, 2.0, 0.0, 3.0, 1.0, 2.0, 4.0, 0.0, 3.0, 0.0, 3.0, 4.0, 1.0, 4.0, 2.0, 4.0, 0.0, 4.0, 2.0, 0.0, 3.0, 0.0, 2.0, 0.0, 4.0, 3.0, 2.0, 1.0, 3.0, 0.0, 2.0, 3.0, 0.0, 2.0, 0.0, 1.0, 4.0, 1.0, 0.0, 4.0, 2.0, 1.0, 2.0, 0.0, 3.0, 0.0, 4.0, 3.0, 2.0, 4.0, 0.0, 2.0, 4.0, 2.0, 3.0, 1.0, 4.0, 3.0, 0.0, 2.0, 0.0, 2.0, 3.0, 0.0, 1.0, 0.0, 1.0, 4.0, 3.0, 2.0, 0.0, 1.0, 4.0, 3.0, 1.0, 2.0, 1.0, 2.0, 0.0, 3.0, 2.0, 4.0, 2.0, 3.0, 2.0, 3.0, 2.0, 3.0, 4.0, 0.0, 2.0, 4.0, 0.0, 4.0, 3.0, 2.0, 3.0, 2.0, 1.0, 0.0, 4.0, 2.0, 0.0, 4.0, 3.0, 4.0, 0.0, 1.0, 0.0, 4.0, 3.0, 0.0, 2.0, 4.0, 1.0, 3.0, 2.0, 4.0, 2.0, 4.0, 2.0, 0.0, 2.0, 3.0, 2.0, 4.0, 0.0, 1.0, 3.0, 2.0, 3.0, 2.0, 0.0, 2.0, 1.0, 3.0, 0.0, 2.0, 0.0, 4.0, 0.0, 2.0, 3.0, 2.0, 3.0, 4.0, 3.0, 2.0, 4.0, 3.0, 4.0, 2.0, 3.0, 4.0, 2.0, 0.0, 1.0, 4.0, 1.0, 3.0, 4.0, 0.0, 2.0, 4.0, 2.0, 0.0, 4.0, 2.0, 3.0, 1.0, 2.0, 1.0, 2.0, 0.0, 3.0, 4.0, 0.0, 3.0, 2.0, 3.0, 4.0, 2.0, 1.0, 3.0, 2.0, 3.0, 2.0, 0.0, 4.0, 2.0, 0.0, 4.0, 3.0, 2.0, 4.0, 1.0, 2.0, 0.0, 1.0, 2.0, 3.0, 4.0, 0.0, 4.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 4.0, 1.0, 4.0, 0.0, 4.0, 0.0, 1.0, 0.0, 3.0, 4.0, 3.0, 4.0, 0.0, 4.0, 1.0, 0.0, 2.0, 3.0, 1.0, 4.0, 3.0, 2.0, 4.0, 0.0, 3.0, 1.0, 3.0, 1.0, 2.0, 0.0, 3.0, 4.0, 0.0, 1.0, 4.0, 2.0, 4.0, 3.0, 1.0, 2.0, 4.0, 0.0, 1.0, 0.0, 2.0, 3.0, 4.0, 1.0, 2.0, 0.0, 1.0, 3.0, 4.0, 3.0, 2.0, 1.0, 0.0, 3.0, 1.0, 3.0, 0.0, 3.0, 2.0, 0.0, 2.0, 0.0, 4.0, 2.0, 1.0, 2.0, 0.0, 3.0, 2.0, 0.0, 1.0, 3.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 1.0, 0.0, 3.0, 4.0, 3.0, 1.0, 4.0, 3.0, 2.0, 0.0, 2.0, 0.0, 4.0, 3.0, 4.0, 2.0, 4.0, 3.0, 1.0, 2.0, 3.0, 0.0, 4.0, 3.0, 1.0, 3.0, 2.0, 0.0, 2.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 4.0, 1.0, 2.0, 3.0, 4.0, 0.0, 2.0, 4.0, 2.0, 4.0, 0.0, 2.0, 1.0, 3.0, 2.0, 1.0, 2.0, 4.0, 0.0, 2.0, 0.0, 2.0, 4.0, 3.0, 4.0, 2.0, 0.0, 4.0, 1.0, 2.0, 0.0, 2.0, 4.0, 2.0, 0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 2.0, 4.0, 2.0, 4.0, 2.0, 4.0, 2.0, 4.0, 0.0, 3.0, 4.0, 1.0, 2.0, 1.0, 2.0, 4.0, 0.0, 4.0, 2.0, 0.0, 2.0, 1.0, 4.0, 1.0, 2.0, 1.0, 2.0, 1.0, 3.0, 1.0, 3.0, 0.0, 2.0, 1.0, 3.0, 1.0, 2.0, 0.0, 2.0, 0.0, 3.0, 4.0, 0.0, 2.0, 0.0, 2.0, 1.0, 3.0, 1.0, 2.0, 3.0, 0.0, 1.0, 0.0, 4.0, 1.0, 4.0, 3.0, 2.0, 3.0, 0.0, 1.0, 2.0, 0.0, 2.0, 3.0, 0.0, 2.0, 0.0, 2.0, 4.0, 0.0, 2.0, 0.0, 2.0, 0.0, 4.0, 0.0, 2.0, 0.0, 4.0, 2.0, 4.0, 1.0, 3.0, 4.0, 1.0, 2.0, 3.0, 1.0, 2.0, 4.0, 3.0, 2.0, 1.0, 4.0, 1.0, 0.0, 2.0, 1.0, 4.0, 1.0, 2.0, 0.0, 4.0, 0.0, 3.0, 2.0, 1.0, 2.0, 4.0, 0.0, 2.0, 3.0, 4.0, 2.0, 4.0, 0.0, 3.0, 4.0, 2.0, 0.0, 2.0, 3.0, 4.0, 2.0, 0.0, 4.0, 0.0, 1.0, 3.0, 4.0, 3.0, 2.0, 0.0, 4.0, 2.0, 1.0, 3.0, 2.0, 3.0, 2.0, 1.0, 3.0, 4.0, 1.0, 2.0, 0.0, 4.0, 2.0, 1.0, 0.0, 2.0, 3.0, 2.0, 0.0, 3.0, 2.0, 0.0, 2.0, 1.0, 3.0, 4.0, 1.0, 4.0, 0.0, 3.0, 2.0, 4.0, 2.0, 1.0, 0.0, 3.0, 4.0, 2.0, 4.0, 1.0, 0.0, 1.0, 3.0, 2.0, 3.0, 1.0, 0.0, 3.0, 4.0, 0.0, 3.0, 2.0, 1.0, 3.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 1.0, 3.0, 2.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0, 4.0, 2.0, 3.0, 0.0, 4.0, 2.0, 3.0, 0.0, 3.0, 0.0, 2.0, 1.0, 3.0, 0.0, 2.0, 3.0, 2.0, 4.0, 2.0, 0.0, 4.0, 3.0, 4.0, 0.0, 3.0, 2.0, 4.0, 0.0, 2.0, 4.0, 3.0, 0.0, 1.0, 0.0, 2.0, 1.0, 3.0, 2.0, 4.0, 2.0, 1.0, 2.0, 3.0, 0.0, 4.0, 0.0, 2.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, 4.0, 3.0, 2.0, 4.0, 1.0, 2.0, 4.0, 1.0, 4.0, 0.0, 4.0, 1.0, 0.0, 2.0, 0.0, 3.0, 2.0, 1.0, 2.0, 1.0, 3.0, 0.0, 2.0, 1.0, 2.0, 1.0, 4.0, 0.0, 2.0, 4.0, 3.0, 4.0, 0.0, 2.0, 4.0, 0.0, 2.0, 0.0, 2.0, 1.0, 2.0, 4.0, 3.0, 1.0, 0.0, 4.0, 2.0, 4.0, 0.0, 3.0, 2.0, 0.0, 4.0, 2.0, 0.0, 2.0, 3.0, 2.0, 4.0, 0.0, 2.0, 0.0, 1.0, 4.0, 2.0, 0.0, 2.0, 0.0, 4.0, 1.0, 3.0, 2.0, 3.0, 1.0, 4.0, 3.0, 0.0, 2.0, 4.0, 2.0, 4.0, 2.0, 1.0, 4.0, 1.0, 2.0, 0.0, 4.0, 3.0, 4.0, 2.0, 4.0, 2.0, 3.0, 1.0, 2.0, 4.0, 2.0, 4.0, 0.0, 3.0, 2.0, 3.0, 4.0, 2.0, 1.0, 3.0, 2.0, 4.0, 0.0, 2.0, 3.0, 4.0, 0.0, 4.0, 0.0, 3.0, 0.0, 4.0, 2.0, 1.0, 4.0, 0.0, 2.0, 4.0, 1.0, 3.0, 2.0, 4.0, 0.0, 2.0, 4.0, 3.0, 2.0, 4.0, 2.0, 0.0, 4.0, 1.0, 2.0, 1.0, 0.0, 2.0, 3.0, 2.0, 4.0, 1.0, 3.0, 4.0, 2.0, 0.0, 4.0, 3.0, 1.0, 3.0, 0.0, 3.0, 4.0, 2.0, 4.0, 2.0, 4.0, 2.0, 3.0, 2.0, 4.0, 3.0, 2.0, 1.0, 2.0, 3.0, 0.0, 4.0, 3.0, 2.0, 3.0, 2.0, 0.0, 2.0, 4.0, 3.0, 4.0, 1.0, 3.0, 2.0, 0.0, 3.0, 2.0, 4.0, 3.0, 4.0, 3.0, 0.0, 4.0, 2.0, 3.0, 4.0, 2.0, 0.0, 4.0, 0.0, 1.0, 2.0, 0.0, 1.0, 2.0, 4.0, 3.0, 4.0, 0.0, 4.0, 2.0, 4.0, 2.0, 1.0, 2.0, 4.0, 2.0, 4.0, 0.0, 4.0, 3.0, 0.0, 4.0, 2.0, 0.0, 4.0, 3.0, 4.0, 3.0, 0.0, 2.0, 0.0, 4.0, 2.0, 4.0, 0.0, 1.0, 2.0, 3.0, 0.0, 4.0, 3.0, 4.0, 1.0, 2.0, 3.0, 4.0, 0.0, 4.0, 2.0, 0.0, 4.0, 2.0, 3.0, 4.0, 0.0, 2.0, 3.0, 4.0, 3.0, 1.0, 4.0, 3.0, 0.0, 2.0, 4.0, 2.0, 0.0, 4.0, 0.0, 2.0, 0.0, 3.0, 2.0, 3.0, 2.0, 4.0, 2.0, 4.0, 3.0, 2.0, 4.0, 0.0, 2.0, 4.0, 3.0, 0.0, 2.0, 4.0, 2.0, 4.0, 3.0, 4.0, 1.0, 4.0, 0.0, 3.0, 2.0, 3.0, 0.0, 4.0, 1.0, 2.0, 3.0, 2.0, 3.0, 0.0, 4.0, 2.0, 3.0, 2.0, 4.0, 2.0, 4.0, 2.0, 3.0, 0.0, 3.0, 2.0, 0.0, 4.0, 3.0, 0.0, 2.0, 4.0, 1.0, 3.0, 2.0, 3.0, 4.0, 2.0, 1.0, 2.0, 3.0, 2.0, 3.0, 4.0, 1.0, 4.0, 3.0, 1.0, 0.0, 4.0, 2.0, 1.0, 4.0, 0.0, 3.0, 2.0, 4.0, 1.0, 4.0, 1.0, 3.0, 2.0, 3.0, 4.0, 3.0, 1.0, 3.0, 0.0, 4.0, 0.0, 2.0, 3.0, 0.0, 4.0, 1.0, 3.0, 4.0, 2.0, 3.0, 2.0, 1.0, 2.0, 3.0, 4.0, 2.0, 4.0, 1.0, 4.0, 2.0, 3.0, 2.0, 3.0, 0.0, 2.0, 0.0, 4.0, 0.0, 3.0, 4.0, 2.0, 3.0, 4.0, 2.0, 0.0, 2.0, 3.0, 4.0, 1.0, 3.0, 4.0, 3.0, 4.0, 2.0, 3.0, 2.0, 1.0, 2.0, 1.0, 0.0, 4.0, 0.0, 2.0, 3.0, 2.0, 3.0, 0.0, 4.0, 2.0, 1.0, 0.0, 2.0, 0.0, 4.0, 2.0, 4.0, 2.0, 4.0, 3.0, 2.0, 1.0, 3.0, 2.0, 3.0, 2.0, 3.0, 1.0, 3.0, 2.0, 4.0, 0.0, 3.0, 2.0, 0.0, 2.0, 4.0, 2.0, 4.0, 3.0, 2.0, 3.0, 2.0, 3.0, 0.0, 3.0, 4.0, 2.0, 4.0, 1.0, 2.0, 4.0, 1.0, 3.0, 1.0, 3.0, 1.0, 0.0, 2.0, 4.0, 2.0, 3.0, 1.0, 0.0, 1.0, 2.0, 3.0, 2.0, 3.0, 1.0, 4.0, 3.0, 2.0, 1.0, 4.0, 3.0, 2.0, 3.0, 0.0, 3.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0, 0.0, 3.0, 2.0, 0.0, 2.0, 3.0, 4.0, 2.0, 4.0, 3.0, 0.0, 2.0, 3.0, 2.0, 0.0, 2.0, 4.0, 1.0, 2.0, 0.0, 4.0, 2.0, 1.0, 4.0, 3.0, 0.0, 4.0, 3.0, 0.0, 2.0, 4.0, 0.0, 3.0, 0.0, 4.0, 1.0, 3.0, 4.0, 2.0, 0.0, 3.0, 2.0, 4.0, 3.0, 2.0, 0.0, 1.0, 3.0, 2.0, 1.0, 2.0, 4.0, 3.0, 0.0, 3.0, 4.0, 2.0, 3.0, 2.0, 3.0, 4.0, 1.0, 4.0, 2.0, 3.0, 2.0, 4.0, 2.0, 3.0, 0.0, 4.0, 0.0, 4.0, 2.0, 0.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 1.0, 3.0, 4.0, 2.0, 1.0, 3.0, 4.0, 2.0, 4.0, 3.0, 4.0, 0.0, 4.0, 0.0, 2.0, 0.0, 3.0, 2.0, 4.0, 2.0, 3.0, 2.0, 3.0, 0.0, 2.0, 0.0, 2.0, 1.0, 0.0, 2.0, 3.0, 4.0, 0.0, 3.0, 4.0, 3.0, 4.0, 0.0, 2.0, 4.0, 1.0, 0.0, 3.0, 0.0, 4.0, 3.0, 0.0, 3.0, 1.0, 4.0, 3.0, 1.0, 2.0, 3.0, 1.0, 2.0, 4.0, 3.0, 2.0, 1.0, 2.0, 3.0, 2.0, 3.0, 0.0, 4.0, 0.0, 4.0, 3.0, 4.0, 3.0, 2.0, 4.0, 3.0, 4.0, 3.0, 4.0, 3.0, 4.0, 0.0, 3.0, 4.0, 3.0, 0.0, 3.0, 1.0, 2.0, 3.0, 1.0, 0.0, 4.0, 2.0, 3.0, 0.0, 2.0, 4.0, 0.0, 1.0, 3.0, 2.0, 3.0, 1.0, 3.0, 4.0, 3.0, 2.0, 4.0, 3.0, 2.0, 0.0, 4.0, 1.0, 0.0, 3.0, 0.0, 4.0, 1.0, 3.0, 0.0, 4.0, 3.0, 1.0, 3.0, 4.0, 0.0, 3.0, 4.0, 3.0, 2.0, 3.0, 0.0, 4.0, 3.0, 0.0, 3.0, 0.0, 4.0, 2.0, 3.0, 0.0, 4.0, 3.0, 1.0, 3.0, 2.0, 1.0, 4.0, 0.0, 4.0, 0.0, 3.0, 0.0, 3.0, 0.0, 2.0, 0.0, 1.0, 3.0, 4.0, 0.0, 3.0, 4.0, 2.0, 3.0, 0.0, 4.0, 3.0, 1.0, 0.0, 1.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 2.0, 3.0, 0.0, 2.0, 4.0, 3.0, 4.0, 3.0, 2.0, 3.0, 2.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 4.0, 0.0, 1.0, 3.0, 1.0, 3.0, 2.0, 3.0, 0.0, 3.0, 4.0, 2.0, 3.0, 1.0, 3.0, 0.0, 3.0, 0.0, 3.0, 2.0, 0.0, 3.0, 0.0, 2.0, 3.0, 1.0, 0.0, 3.0, 2.0, 3.0, 0.0, 3.0, 0.0, 4.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 2.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0, 0.0, 3.0])
def eqenergy(rows):
    return np.sum(rows, axis=1)
def classify(rows):
    energys = eqenergy(rows)

    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = labels[numers[indys]]
        outputs[defaultindys] = 0.0
        return outputs
    return thresh_search(energys)

numthresholds = 3743



# Main method
model_cap = numthresholds


def Validate(file):
    #Load Array
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs, cleanarr[:, -1]


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:, -1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    cleanarr = cleanarr.reshape(cleanarr.shape[0], -1)
    with open(preprocessedfile, 'r') as csvinput:
        dirtyreader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(dirtyreader, None) + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            print(str(','.join(str(j) for j in (['"' + i + '"' if ',' in i else i for i in row]))) + ',' + str(get_key(int(outputs[k]), classmapping)))



#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    args = parser.parse_args()
    faulthandler.enable()

    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile, classmapping)


    else:
        classifier_type = 'DT'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds, true_labels = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count
        
            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            if args.json:
                #                json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'n_classes':2, 'Number of False Negative Instances': num_FN, 'Number of False Positive Instances': num_FP, 'Number of True Positive Instances': num_TP, 'Number of True Negative Instances': num_TN,   'False Negatives': FN, 'False Positives': FP, 'True Negatives': TN, 'True Positives': TP, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0}
                json_dict = {'instance_count':                        count ,
                            'classifier_type':                        classifier_type ,
                            'n_classes':                            2 ,
                            'number_of_false_negative_instances':    num_FN ,
                            'number_of_false_positive_instances':    num_FP ,
                            'number_of_true_positive_instances':    num_TP ,
                            'number_of_true_negative_instances':    num_TN,
                            'false_negatives':                        FN ,
                            'false_positives':                        FP ,
                            'true_negatives':                        TN ,
                            'true_positives':                        TP ,
                            'number_correct':                        num_correct ,
                            'best_guess':                            randguess ,
                            'model_accuracy':                        modelacc ,
                            'model_capacity':                        model_cap ,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                             }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            if args.json:
        #        json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0, 'n_classes': n_classes}
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'n_classes':                            n_classes,
                            'number_correct':                        num_correct,
                            'best_guess':                            randguess,
                            'model_accuracy':                        modelacc,
                            'model_capacity':                        model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                            }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                sys.exit()
            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")


            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            n_labels = labels.size
            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]
            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm
        mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        os.remove(preprocessedfile)


