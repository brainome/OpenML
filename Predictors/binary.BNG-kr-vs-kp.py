#!/usr/bin/env python3
#
# This code has been produced by an evaluation version of Daimensions(tm).
# Portions of this code copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.
# Distribution of this code in binary form or commercial use of any kind is forbidden.
# For a detailed license agreement see: http://brainome.ai/license
# Use of predictions results at your own risk.
#
# Output of Brainome Daimensions(tm) 0.98 Table Compiler v0.98.
# Invocation: btc -f NN -target class BNG-kr-vs-kp.csv -o BNG-kr-vs-kp_NN.py -nsamples 0 --yes -nsamples 0 -e 20
# Total compiler execution time: 2:57:46.66. Finished on: Sep-03-2020 21:30:09.
# This source code requires Python 3.
#
"""
Classifier Type:                     Neural Network
System Type:                         Binary classifier
Training/Validation Split:           60:40%
Best-guess accuracy:                 52.18%
Overall Model accuracy:              94.96% (949673/1000000 correct)
Overall Improvement over best guess: 42.78% (of possible 47.82%)
Model capacity (MEC):                351 bits
Generalization ratio:                2705.62 bits/bit
Model efficiency:                    0.12%/parameter
System behavior
True Negatives:                      46.35% (463475/1000000)
True Positives:                      48.62% (486198/1000000)
False Negatives:                     3.57% (35677/1000000)
False Positives:                     1.47% (14650/1000000)
True Pos. Rate/Sensitivity/Recall:   0.93
True Neg. Rate/Specificity:          0.97
Precision:                           0.97
F-1 Measure:                         0.95
False Negative Rate/Miss Rate:       0.07
Critical Success Index:              0.91
Confusion Matrix:
 [46.35% 1.47%]
 [3.57% 48.62%]
Overfitting:                         No
Note: Labels have been remapped to 'nowin'=0, 'won'=1.
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "BNG-kr-vs-kp.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 36
n_classes = 2

mappings = [{1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {30677878.0: 0, 2517025534.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {476252946.0: 0, 1908338681.0: 1, 2013832146.0: 2}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2238339752.0: 1}, {1993550816.0: 0, 2013832146.0: 1, 2238339752.0: 2}]
list_of_cols_to_normalize = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values())) + 1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize, mappings):
            if i >= data_arr.shape[1]:
                break
            col = data_arr[:, i]
            normcol = column_norm(col,mapping)
            data_arr[:, i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([0.11149333333333333, 0.07106333333333334, 0.03789, 0.10191833333333333, 0.334385, 0.4569433333333333, 0.36862666666666666, 0.22356, 0.382755, 0.306655, 0.42659833333333336, 0.118605, 0.68825, 0.00553, 1.630935, 0.05262666666666667, 0.034805, 0.6833133333333333, 0.00971, 0.16325166666666666, 0.19054333333333334, 0.20230833333333334, 0.06557166666666667, 0.3793316666666667, 0.0039766666666666665, 0.69535, 0.069065, 0.0006566666666666666, 0.024858333333333333, 0.049661666666666666, 0.18822333333333333, 0.06836833333333334, 0.39624166666666666, 0.6447733333333333, 0.7181866666666666, 1.2472533333333333])
        components = np.array([array([ 9.89877317e-02,  2.52434044e-02,  6.08701874e-03, -2.41571062e-04,
       -7.52663476e-02, -2.25886946e-02,  9.58971581e-03, -1.49345239e-02,
       -3.89374343e-03, -1.68490543e-02,  4.15632292e-01, -1.38891624e-02,
        1.80083102e-01,  4.08449638e-03, -6.59462308e-01,  3.47014627e-03,
        1.54734058e-02, -9.85000017e-02,  1.14392934e-02,  2.05603688e-01,
        3.30394438e-02,  5.98560629e-03,  1.44713566e-02,  1.67260922e-02,
        2.64609437e-03,  3.07084959e-01, -1.48352784e-02, -5.92878325e-05,
        2.26241227e-02,  9.14033896e-03,  2.36686121e-01,  3.14984371e-02,
       -6.85406208e-02, -6.57966312e-02, -2.49608077e-01, -2.54982560e-01]), array([ 5.62686919e-02, -7.80865978e-02,  2.25070967e-02,  3.21391607e-02,
        3.29431452e-01,  4.45469918e-02,  5.09987423e-01,  4.63799561e-01,
        5.40261478e-01,  8.43693531e-02,  4.84393277e-02, -5.99316758e-02,
       -8.16864089e-02,  5.92737495e-04, -4.21298906e-02, -1.41180580e-02,
       -1.00160345e-02,  1.34109233e-01, -3.73357207e-03, -6.73815090e-03,
       -1.48898146e-01,  1.72257944e-01, -8.69059082e-03, -6.48410639e-02,
        1.07893760e-03,  5.22825006e-02, -2.84137571e-03,  1.27532989e-04,
        4.91006790e-03, -1.92390406e-02, -9.55122177e-02,  1.14368695e-02,
       -1.28152843e-02,  9.13413098e-03, -3.48979672e-02, -4.21880418e-02]), array([-2.72499759e-02,  8.94753450e-02,  3.50147061e-02,  1.10972284e-01,
        1.51038540e-01,  1.68991616e-01,  8.01669124e-02,  8.28072532e-02,
        7.38548152e-02, -1.04823630e-01, -1.31757907e-01, -2.91894244e-02,
        3.37158578e-01,  9.40946161e-04,  4.67762899e-03,  5.05228907e-02,
       -1.29060598e-02, -5.76766023e-01,  1.39896791e-02,  2.79217841e-02,
        1.26144208e-01, -1.32060747e-02, -3.18763685e-02,  1.08457516e-01,
       -1.27912424e-03, -1.36245047e-01,  4.21318291e-02, -4.41207217e-06,
       -5.17768597e-03,  4.81887953e-02,  2.14709550e-02, -2.74966999e-02,
        7.97019445e-02, -5.88820091e-01,  7.99232945e-02,  1.26998635e-01]), array([-5.55513783e-02, -5.20788261e-02, -1.48069905e-02,  1.42271559e-02,
        5.29882837e-02,  2.64894475e-01, -1.19831474e-02, -3.98937958e-02,
       -3.36077822e-02,  3.39829429e-02, -2.38594465e-01, -8.66632976e-03,
       -4.64161139e-02, -6.11273480e-03, -6.55616788e-01, -4.21393686e-02,
       -9.47373858e-03,  1.03849599e-01, -5.87323253e-03, -1.13053755e-01,
        4.94534228e-02,  4.50719197e-03, -2.75665288e-02, -3.14646012e-02,
       -4.34141623e-03, -2.86293185e-01, -6.52732563e-03, -2.08066026e-04,
        2.36675872e-02, -1.62441176e-02, -1.38968783e-01, -5.96621343e-02,
       -2.45718215e-01,  1.21750542e-01,  3.95223815e-01,  2.46882220e-01]), array([-6.54888012e-02,  6.67987076e-02, -5.44898568e-02, -4.10948208e-02,
       -1.37172602e-01, -2.04003487e-01, -3.09621296e-01, -9.63768336e-02,
        4.49622823e-01, -3.26523801e-01, -9.12144475e-02,  7.35385983e-02,
        1.13783545e-01, -1.98500205e-03, -3.02037463e-02,  1.24739278e-02,
        9.82819586e-03, -3.24267006e-02, -7.53776637e-04,  5.35718152e-02,
        1.41211074e-01,  5.99631457e-01,  1.32532356e-02, -2.16584401e-01,
       -1.71526964e-03, -1.06093484e-01,  2.78694494e-02, -2.48695184e-04,
        7.11512182e-03,  1.81682702e-02,  1.31374769e-01,  7.66765441e-03,
        2.75822080e-02,  1.15925075e-01, -1.33323218e-02,  9.88212473e-02]), array([-7.62226273e-02,  3.75932960e-02, -8.61590941e-03, -1.39078922e-02,
       -1.00028919e-01, -1.19419795e-01, -5.66646700e-02,  1.45537355e-01,
        2.69144475e-02,  5.87133998e-01, -4.07216593e-02,  2.55899047e-02,
        8.99528791e-02, -1.31728561e-03, -1.24350179e-01,  2.40389755e-02,
        9.14541894e-03, -5.17903442e-03, -4.78929036e-03,  3.96291190e-02,
        2.72093858e-01, -9.64516456e-02,  3.03462623e-02, -4.00782902e-01,
        2.52041978e-03, -6.02414262e-02,  6.57491064e-02,  1.55906152e-04,
        9.16596909e-03,  4.53095969e-03,  3.05925927e-02, -1.48528791e-02,
        5.53428442e-01,  2.89901008e-02,  3.00579064e-02,  9.00908610e-02]), array([-3.81968106e-02, -3.23708311e-02, -6.34411312e-03,  1.32457937e-01,
       -3.83554217e-02,  8.04068779e-01, -7.40321048e-02, -1.12364448e-01,
        4.30792782e-02, -1.12858077e-01,  1.77949413e-01, -1.13109383e-01,
       -1.20785382e-01, -6.34574405e-03,  9.69500069e-02,  2.69422031e-02,
       -9.62484958e-03,  1.28378592e-01, -5.35216586e-04,  2.00154144e-02,
        9.37437460e-02,  1.64404541e-01, -5.23788928e-02,  9.32563141e-03,
        1.50579289e-03,  1.15253798e-01,  3.12158951e-02, -1.20911448e-04,
       -3.60989706e-02, -3.20934866e-02,  7.23387493e-02, -1.17907769e-01,
        3.64708302e-01,  5.74553648e-02, -7.70517439e-03, -4.76817279e-02]), array([ 5.75902420e-02, -2.62987738e-02,  4.97654439e-02,  2.88814477e-02,
        1.48090810e-01, -2.59147215e-01,  2.33565963e-02, -5.30167243e-02,
        1.43928795e-02, -1.19435886e-01, -7.44525808e-02, -7.53148940e-03,
       -1.48785214e-01,  2.80351294e-03, -2.13206647e-01,  1.39829714e-02,
       -2.13653575e-02,  8.05121990e-02,  7.81911443e-03, -3.57213950e-02,
        9.76704765e-02,  4.83919961e-02, -4.83495284e-02,  6.79964562e-01,
        1.49524777e-03, -7.89195036e-02,  1.22417144e-01,  2.15955806e-04,
        1.82049267e-02,  5.32571728e-02, -4.34068177e-02,  2.78502270e-02,
        5.11932616e-01,  9.61358837e-02, -1.03593709e-01,  1.35748184e-01]), array([-2.03772020e-02,  5.73895425e-02,  7.65892665e-02, -3.79338515e-02,
       -5.99264800e-01,  1.12970689e-01, -5.03463140e-02,  1.98910828e-01,
        2.57731950e-01,  3.77233388e-01,  2.76619673e-02,  1.30423099e-01,
        7.09225613e-02,  1.32656866e-03,  8.92869016e-02,  3.67752471e-03,
        3.13978337e-02, -6.96546898e-03,  7.48760256e-03, -3.49026406e-04,
        4.04899504e-02,  9.43307691e-02,  5.82354094e-02,  4.94854827e-01,
       -3.03126426e-05,  7.56801631e-03, -6.35185645e-02,  9.25132527e-06,
       -8.51779323e-03, -1.80512134e-03, -2.26129346e-04, -3.13459980e-02,
       -1.94166505e-01, -2.13839122e-02,  1.66295720e-01, -6.79585309e-02]), array([ 1.76931617e-01,  2.35300004e-02,  3.28688297e-02,  3.92755653e-02,
       -3.72034494e-01, -1.90315236e-02,  3.21109137e-04,  4.82092561e-02,
        2.90342516e-02, -1.75969978e-01, -5.00204804e-02,  1.74422625e-01,
       -5.48776746e-01,  1.41498545e-02, -1.68271747e-01,  5.49028961e-02,
        4.66571908e-02, -6.79151947e-02, -6.75437141e-03, -2.29275432e-01,
       -1.01912733e-01, -4.27642240e-03,  1.10237309e-01, -2.05176536e-01,
        4.07526260e-04,  5.09780392e-02,  4.66453222e-02, -1.12397359e-05,
        5.59014249e-02,  5.68946154e-04, -2.97829910e-01,  9.71444489e-02,
        1.24195873e-01, -4.05492496e-01, -1.26279288e-01, -3.33410180e-02]), array([ 1.52673000e-02, -1.22547454e-02,  3.49409760e-03, -9.17172987e-02,
       -3.26331684e-01,  1.46916154e-01,  2.62456650e-01,  5.48834519e-02,
       -6.73544352e-02, -2.34161179e-01, -1.77891711e-01,  1.90437639e-01,
        4.46727690e-01,  4.19386397e-03, -4.52798401e-02,  3.64552958e-02,
        4.55322780e-02,  1.22630795e-01, -1.03779113e-02,  1.45152199e-01,
       -4.29187236e-01, -1.30178138e-01,  1.24470847e-01, -6.08436278e-02,
        5.98671933e-04, -1.23299985e-01,  3.84943259e-02,  2.61113576e-04,
        7.08847063e-02, -2.69623679e-02, -2.15032498e-02,  1.35491893e-01,
        2.11931542e-01,  1.52224742e-01, -2.09939154e-01,  2.21195016e-01]), array([ 9.15242093e-02,  4.97791238e-02,  2.25266541e-02,  1.60960045e-01,
        1.78210746e-01,  2.22490032e-01, -1.44991009e-01, -9.73226476e-03,
        1.60052297e-02,  2.38822097e-01, -3.71259072e-01,  1.52505745e-01,
       -1.10970012e-01,  1.35759771e-02,  5.36704617e-03,  2.62576256e-02,
        7.29670675e-03,  5.10960386e-02, -3.74231905e-03,  2.12159688e-01,
        2.17515029e-01,  3.54641285e-02,  6.96055656e-02,  5.81852926e-02,
       -3.25266878e-03, -3.19253315e-02,  1.35775963e-02, -2.12430220e-04,
        1.02907900e-01, -1.30295669e-02,  4.30166238e-02,  3.82918647e-01,
       -2.49509055e-01,  1.03383944e-02, -5.42708886e-01,  8.31561279e-02]), array([-2.03338965e-02, -3.14187836e-02,  3.32154126e-03, -1.37801557e-01,
        3.98712660e-02,  6.53895945e-02,  1.15180314e-01, -1.27829784e-01,
       -2.77281157e-02, -2.26633848e-02, -1.57258969e-01,  7.80521772e-02,
        2.25188238e-01, -2.16597413e-03, -3.74097522e-02, -2.29330937e-02,
        6.80250479e-03, -7.59963828e-02, -8.98815559e-03, -2.42561413e-01,
        1.78322901e-01,  9.22211536e-02,  2.16481982e-02,  7.07231124e-03,
       -3.56348872e-03, -2.42370004e-01, -1.56369435e-01, -2.76890668e-04,
        3.74845249e-02, -4.29896457e-02, -3.19553518e-01,  1.19643932e-01,
        1.04528782e-01,  1.28343530e-01, -6.37262507e-02, -7.20743834e-01]), array([-9.11326162e-02,  5.16555563e-02, -3.51698702e-03,  5.15266067e-02,
       -2.01449373e-03, -2.69860303e-02, -6.17432338e-02,  9.67596197e-02,
        4.54153981e-03, -3.67346774e-02, -3.62046165e-02, -2.80349060e-02,
       -2.70818110e-01,  6.64440123e-04, -1.88171216e-02,  5.07435475e-02,
       -2.76100961e-04,  1.23149277e-01,  3.73615690e-03,  4.15199949e-01,
       -2.28798332e-01, -8.28744886e-02,  9.01061740e-03,  2.77296795e-03,
       -3.29827498e-04, -5.49374960e-01, -1.83685305e-02,  8.99461844e-05,
        1.14985077e-02, -1.80736473e-02,  4.04526634e-01,  9.78908110e-03,
        7.39442434e-02, -1.75178776e-01,  1.25664371e-01, -3.57132039e-01]), array([ 1.64082676e-01,  7.66508432e-02, -1.88487489e-03,  3.01604634e-01,
        1.53911478e-01, -2.18227128e-02, -4.66363260e-01,  1.84894972e-02,
        6.89811082e-02,  1.76142148e-01, -1.74062419e-01, -1.40336104e-01,
        2.26824708e-01,  1.49775522e-02, -6.16137424e-02,  9.41893348e-02,
       -4.52279386e-02,  1.36242299e-01, -1.30083143e-02, -1.50027239e-01,
       -5.28698983e-01,  7.00127636e-02, -6.60310556e-02,  2.88620406e-02,
        1.26375112e-03,  2.16564394e-01,  3.89383747e-02,  1.73573387e-04,
        3.13408510e-02, -1.55345355e-02, -1.03024625e-01,  1.28862062e-01,
        1.06378527e-01, -1.12435893e-01,  1.48469828e-01, -1.68368786e-01]), array([-1.49958724e-01,  4.52457807e-02,  7.61767171e-03, -1.11355470e-01,
        1.43181836e-01,  3.27731801e-03,  1.66365444e-01, -1.62463057e-01,
       -5.63522357e-02, -7.76519687e-03, -2.52071459e-01,  4.11102578e-01,
       -1.18703857e-01, -1.90079969e-03, -1.30393502e-03,  3.08256378e-02,
        7.49763253e-02, -5.63209651e-02, -3.36811293e-04,  3.20362219e-01,
       -1.44612932e-02,  1.09558996e-01,  1.67392810e-01,  2.31610313e-02,
       -4.05259983e-03,  4.78971712e-01, -1.06719629e-01, -2.92870713e-05,
        7.01657040e-02, -2.62701039e-02,  5.56686670e-02,  1.53849799e-01,
        1.17019244e-01, -1.68271100e-02,  4.35883083e-01, -9.07160566e-02]), array([ 4.48738465e-01, -1.00595528e-01, -6.80880492e-03,  3.69318359e-01,
        9.15451639e-02, -4.52564766e-03, -4.50450134e-02,  1.27389934e-01,
        3.83381757e-03, -1.34958634e-01,  1.64404203e-01,  4.75768657e-01,
        1.41765712e-01,  1.32393669e-02,  8.53304899e-02, -5.27903687e-02,
        1.11385469e-01,  1.18921307e-01,  4.73238503e-03, -1.18870513e-01,
        2.24965450e-01, -1.43425194e-01,  2.10963420e-01, -3.05900789e-02,
        4.08034404e-03, -1.04368847e-01,  2.65075142e-01,  2.31253575e-04,
        2.47580626e-02,  7.92226951e-02,  1.35846136e-01, -5.56735209e-02,
       -3.11456524e-02,  8.33338509e-02,  2.07541535e-01, -7.43716843e-02]), array([-2.14353047e-01,  2.59556618e-03, -9.89561487e-02,  2.39933624e-01,
       -8.90054465e-02,  2.76752101e-02, -1.66732535e-01,  3.52113876e-01,
        1.13363096e-01, -2.08249474e-01, -1.40012104e-01, -6.53200403e-02,
       -1.64255174e-01, -9.14144181e-03, -3.92402090e-02, -6.34463991e-02,
       -2.08111123e-02, -4.83460944e-01,  2.00844047e-02,  3.14688158e-02,
       -2.37748523e-02, -3.19378649e-01, -4.14055425e-02, -1.03130136e-02,
       -4.84446640e-04,  1.29005560e-01,  2.09452800e-02,  3.24627361e-04,
        1.16083294e-02,  7.74486974e-02, -1.12647391e-02,  5.94947059e-02,
        3.30475204e-02,  4.83299601e-01,  1.17618421e-02, -1.08885422e-01]), array([ 0.17860468, -0.01529356, -0.01431209, -0.05934033, -0.19607386,
       -0.05260211,  0.01554733,  0.22780751,  0.01537448, -0.29547256,
       -0.29448602, -0.35121082,  0.12651814,  0.0105836 ,  0.01451892,
        0.03032015, -0.10613202,  0.37651305, -0.02052679,  0.12741632,
        0.40566066, -0.21035435, -0.19107775, -0.0313221 , -0.00619975,
        0.21871258, -0.02546316, -0.00056187,  0.00156037, -0.05458187,
        0.03408627,  0.1630237 , -0.00100169, -0.16584346,  0.16040614,
       -0.08642129]), array([ 0.56406979,  0.27296248,  0.12961671, -0.084459  , -0.09153021,
        0.02835124,  0.22052366, -0.1897666 , -0.05201564,  0.1102865 ,
       -0.01886697, -0.28866217, -0.100996  ,  0.04182891,  0.05111181,
        0.16828642, -0.05530898, -0.2915627 ,  0.01521598,  0.17560087,
       -0.05075207,  0.15014433, -0.06063594, -0.05978237,  0.00438601,
       -0.06769881,  0.23805772,  0.00056645,  0.02069434,  0.11644179,
       -0.02084035,  0.13595287, -0.01775451,  0.27139686,  0.16844453,
       -0.00988466]), array([ 2.89078860e-01,  5.29032652e-02,  1.05564557e-01, -6.15197920e-01,
        1.72416919e-01,  1.49843078e-01, -2.42284672e-01,  1.98233036e-01,
        8.32701697e-02, -3.03988811e-02, -6.98418253e-02,  1.38980557e-01,
       -5.90192925e-02,  1.15501812e-02, -1.67102027e-02,  3.14579563e-02,
       -4.30646914e-03, -1.03779871e-01,  5.16100278e-03, -3.11673403e-01,
       -6.80708880e-02, -1.47613934e-01, -1.24231241e-02,  1.93755354e-02,
       -6.23846304e-03,  1.62714860e-02, -2.21486584e-01, -3.57643460e-04,
        1.44777007e-02, -8.00286108e-02,  3.68267005e-01,  6.26574575e-03,
        7.23157250e-02,  4.62608270e-02, -1.63258814e-02,  5.22228908e-02]), array([-1.66154929e-01,  6.96474682e-01, -7.01997977e-02,  3.15272132e-02,
        9.94156623e-02,  3.39033674e-03, -8.51902995e-04,  1.18224556e-01,
       -1.01925882e-02, -9.79220219e-02,  1.13412763e-01,  1.08890573e-01,
        2.38809083e-02,  3.74627255e-02, -2.17941173e-02,  5.65482467e-01,
        4.37161875e-02,  1.61464714e-01, -8.66645299e-03, -5.51079974e-02,
        9.88179866e-02, -9.97921389e-02,  5.93375932e-02,  2.30993681e-02,
        2.79926051e-04, -1.83614269e-02, -4.55715108e-02, -2.43550344e-04,
       -2.62846083e-02, -3.96498918e-02, -1.34723382e-01, -1.26400001e-01,
       -5.12100194e-02,  5.82060718e-02, -3.28671741e-02,  1.10659556e-02]), array([-1.62996946e-01,  4.54968878e-02, -5.84231175e-03,  1.95245988e-01,
       -8.09600889e-02, -4.01078471e-02,  2.46768546e-01, -9.68535483e-02,
       -5.22376962e-02,  2.12052187e-02, -1.58137973e-01, -3.24643703e-01,
       -1.84794788e-02,  9.49640774e-03, -1.47774804e-02,  6.90112098e-02,
        2.04250324e-01,  1.39167200e-02,  1.78046098e-02, -4.59018859e-01,
        1.92778226e-02,  6.97862643e-02,  4.27592821e-01,  1.11704262e-02,
       -2.22399648e-03,  6.29465169e-02,  1.11154714e-02,  2.54378593e-04,
        1.43191176e-01,  1.48751227e-02,  4.81306548e-01,  1.31760416e-01,
       -1.36537247e-02,  9.51885469e-03,  9.62671372e-03, -2.10844604e-02]), array([ 2.42292503e-02, -7.96753285e-02,  3.04715399e-02, -2.45762792e-01,
        1.22314088e-01,  4.13609438e-02, -2.61660541e-01,  9.87267601e-02,
        5.98469729e-02, -5.39605907e-02,  1.34956401e-01, -3.07732986e-01,
        3.05474533e-02,  6.82914307e-03,  2.30913809e-02, -4.84335736e-02,
        3.54625121e-01,  1.05481959e-02, -4.79536425e-03,  2.58213657e-01,
        4.10403431e-02, -7.21241293e-02,  6.14309497e-01,  4.92969245e-02,
        4.10313188e-03, -3.91384402e-02,  5.93203897e-02,  4.12278786e-04,
        1.24090110e-01,  3.55822602e-02, -3.29657783e-01,  7.19087194e-03,
        6.02587935e-03, -2.40618737e-02,  5.02703241e-02,  2.69555272e-02]), array([-2.71775502e-01, -7.46065120e-03, -7.90439843e-03, -3.14746789e-01,
        3.00192511e-02,  8.41300386e-02, -3.58100067e-02,  4.15563573e-02,
        5.65366195e-03,  1.65318248e-02, -4.87774786e-02,  6.52204364e-02,
        1.41499568e-02, -1.44965225e-02, -8.64169247e-03,  2.83952965e-02,
       -5.33492785e-02,  1.06235441e-01,  5.34991112e-02, -1.02853451e-01,
       -3.58150994e-02, -2.04391412e-02, -9.36137361e-02, -5.92181346e-03,
        2.01411193e-02,  6.89822851e-02,  6.55162419e-01,  2.99147595e-03,
       -5.00149569e-04,  5.43605587e-01,  4.10072738e-02,  6.70161866e-02,
       -8.89653040e-02, -8.01504490e-02, -2.14986597e-02, -1.37011769e-01]), array([ 0.12996173,  0.00416966, -0.07736644,  0.09490784, -0.01151244,
        0.00979272,  0.01848319, -0.00851829, -0.01110816,  0.00315366,
       -0.04252667, -0.01253619,  0.0049386 ,  0.00607931, -0.00395342,
        0.00596369,  0.02143608,  0.07244665,  0.09291895,  0.03992835,
       -0.00384245,  0.01242033,  0.03022011, -0.02430623, -0.01846729,
       -0.01178984, -0.54534163,  0.00603249, -0.01920197,  0.79718921,
       -0.01280686, -0.09262649,  0.04915024, -0.00854005, -0.0267559 ,
        0.04352579]), array([-6.30562990e-02, -5.01647453e-01,  5.27433240e-01,  5.36855289e-02,
       -1.28924489e-02, -4.06577656e-02, -4.08481559e-02, -2.61736177e-02,
        2.64423437e-02, -1.45866986e-02, -6.09713491e-02,  8.33073365e-03,
        9.87502503e-03,  3.54393060e-02, -1.07674146e-02,  6.45534105e-01,
       -2.84093497e-02, -4.95617093e-02,  1.66379595e-03,  3.92757125e-02,
        2.65347514e-02, -2.53465407e-02,  1.02994006e-02, -4.73447868e-02,
       -7.30555535e-04,  2.16153661e-02, -2.62765255e-02,  2.58770236e-04,
       -4.27455744e-02,  2.24570302e-02,  5.03303081e-03, -1.31471903e-01,
       -4.70122039e-02,  5.89113070e-02, -2.79928731e-02, -2.19796911e-02]), array([-1.48231176e-02,  3.20718074e-01,  6.35789897e-01,  7.73705664e-02,
        7.37951031e-03, -1.67920391e-02,  7.96753195e-03,  6.70218042e-03,
       -1.63704304e-02, -4.40708921e-02, -2.49556898e-01, -7.25849256e-03,
        6.93091467e-03, -1.45488669e-02, -3.93941895e-02, -4.07014102e-01,
        9.00134808e-02,  6.83450730e-02, -1.07253214e-02,  4.04530700e-02,
       -1.51913087e-02, -9.59173112e-03,  5.50266490e-02, -5.56378529e-02,
        1.25074527e-03,  8.84385903e-02,  5.18726093e-02,  4.28289381e-05,
       -1.69705291e-01,  1.58519963e-02,  1.06341294e-02, -4.12692506e-01,
       -4.79703393e-03,  1.74215877e-02, -1.23359609e-01, -8.79674733e-02]), array([-0.19209027,  0.13784805,  0.49993548,  0.06388583,  0.02790971,
        0.00811699, -0.04463958,  0.02414663,  0.01242611, -0.07557508,
        0.37800194,  0.0232149 ,  0.0246985 , -0.00534077,  0.04464021,
       -0.15478041, -0.16425091,  0.0499252 ,  0.00658335, -0.0806732 ,
        0.03906109, -0.03591912, -0.07246806, -0.04277827, -0.00333397,
       -0.10836919, -0.13081859,  0.00060159,  0.24818283,  0.07812627,
       -0.047896  ,  0.56913188,  0.03362728,  0.00879915,  0.16563453,
        0.11596436]), array([-1.48570568e-02, -1.48929182e-02,  4.28452248e-02,  1.38799146e-02,
       -4.51924565e-03, -3.41313006e-03,  1.15778462e-02, -1.37632583e-03,
       -7.01220959e-04,  2.92804658e-03,  3.42900680e-02,  9.38116580e-03,
        2.24764122e-04, -2.05252795e-02, -9.69177161e-06,  2.96305845e-02,
        8.50977966e-01, -7.55465965e-04,  8.11444947e-05, -2.15765073e-02,
       -6.77635829e-03, -1.12475160e-03, -4.32580447e-01, -1.19535552e-02,
       -1.62680840e-03, -2.24905736e-02, -1.39782118e-02,  1.89268585e-04,
       -2.01574698e-01,  8.93899764e-03,  9.46233645e-03,  2.00388653e-01,
        5.55218000e-03,  4.14624627e-03,  2.62231147e-02,  2.11277877e-02]), array([ 1.91280023e-02,  3.58983392e-03,  1.51245441e-03,  1.45784159e-03,
       -9.22925787e-03, -1.23198202e-02,  5.57858279e-03,  3.24176787e-03,
       -1.01952618e-02,  1.29591494e-02, -4.12740268e-02,  7.99711894e-03,
       -1.08854875e-03, -1.27969730e-02,  3.77165415e-02,  9.44759614e-04,
        1.59667349e-01, -1.11793952e-03, -7.55251883e-03,  2.96291751e-02,
        3.92266671e-03,  9.30759134e-03, -2.58454769e-01, -3.57801450e-03,
        1.97963836e-03,  2.83890878e-02,  1.37611812e-02, -2.48812278e-04,
        8.94176709e-01, -4.39975036e-03, -3.32631128e-03, -3.15396925e-01,
       -1.94666874e-02,  1.01640103e-03, -3.63398069e-02, -2.46871740e-02]), array([-4.73495847e-03, -4.04603916e-02,  2.41121834e-02, -4.34726129e-03,
        3.16470484e-03, -5.42407732e-05, -3.39041135e-02,  5.73716368e-01,
       -6.20619623e-01, -2.74449920e-02,  6.08435935e-03,  3.02636815e-03,
        8.02477788e-04,  1.27317821e-02, -1.01648477e-03,  1.17614299e-02,
       -1.67194725e-03, -3.69529472e-02,  7.24883297e-03,  3.31284041e-03,
        7.44914349e-03,  5.27781558e-01,  5.61698650e-03,  1.21760799e-02,
        1.45113011e-05, -7.16742674e-04, -4.90633794e-03, -1.87397536e-04,
       -8.30843953e-03, -5.72773985e-03, -3.70351100e-03,  8.31872719e-03,
        3.09575468e-03,  2.67021967e-02,  2.55274053e-03,  1.02243742e-03]), array([ 3.61688715e-03,  2.87998331e-03,  7.77372034e-03,  1.73584360e-03,
        2.08904912e-04, -4.44846438e-03, -5.29520159e-03, -4.00203422e-03,
        3.82477661e-03, -2.80108635e-03, -8.94553908e-03,  5.74525431e-04,
        6.64992374e-03, -1.68085427e-03,  1.86472880e-03, -2.99268840e-03,
        1.31108170e-03,  2.29057060e-02,  9.92842054e-01,  5.62146110e-03,
        5.76618976e-04, -4.24621315e-03, -4.13868031e-03, -1.17000035e-02,
       -3.68756052e-03,  8.87388894e-05,  1.17383091e-02, -3.94635436e-03,
        4.54609095e-03, -1.11344538e-01, -2.47206679e-02, -2.63362036e-03,
        1.71253877e-03, -4.12821789e-03, -6.16503671e-05,  3.09521890e-05])])
        whiten = False
        explained_variance = np.array([0.8325037032985011, 0.5204539915663943, 0.40261859549504014, 0.3148649053403112, 0.2866608357823466, 0.2741806691764079, 0.2544966248654404, 0.23707219954084044, 0.20748996260174085, 0.18570153844334727, 0.1651775587501004, 0.14282378173637486, 0.13749244209832054, 0.12746013251538701, 0.11503869912040533, 0.11275754284247082, 0.10161875445352449, 0.09244111926023384, 0.08947248533249266, 0.08420043240647115, 0.0735343460601492, 0.06914089510283933, 0.06481146185409983, 0.05920201153928787, 0.05546398021502918, 0.038336701859737025, 0.03280148311609395, 0.030903255640428482, 0.028666761123739504, 0.02310429841174215, 0.016592345950362526, 0.012926758366644964, 0.008820255929615115])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target="class"


def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target="class"
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return
    if (testfile):
        target = ''
        hc = -1
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'nowin': 0, 'won': 1}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)
# Classifier
def single_classify(row):
    #inits
    x = row
    o = [0] * num_output_logits


    #Nueron Equations
    h_0 = max((((6.0831566 * float(x[0]))+ (-27.498768 * float(x[1]))+ (23.707323 * float(x[2]))+ (40.062344 * float(x[3]))+ (36.22592 * float(x[4]))+ (-26.4463 * float(x[5]))+ (19.40832 * float(x[6]))+ (-13.380321 * float(x[7]))+ (0.77899504 * float(x[8]))+ (-16.737587 * float(x[9]))+ (-63.806683 * float(x[10]))+ (7.2591376 * float(x[11]))+ (21.855165 * float(x[12]))+ (-32.547768 * float(x[13]))+ (-93.29338 * float(x[14]))+ (-3.7893944 * float(x[15]))+ (49.322475 * float(x[16]))+ (4.9859695 * float(x[17]))+ (76.105545 * float(x[18]))+ (-11.960604 * float(x[19]))+ (-0.53176624 * float(x[20]))+ (13.687848 * float(x[21]))+ (-15.11313 * float(x[22]))+ (1.4289446 * float(x[23]))+ (-12.846612 * float(x[24]))+ (10.580663 * float(x[25]))+ (-13.073293 * float(x[26]))+ (4.731997 * float(x[27]))+ (-7.0927367 * float(x[28]))+ (12.09009 * float(x[29]))+ (-14.586942 * float(x[30]))+ (2.908241 * float(x[31]))+ (-3.0628283 * float(x[32]))) + 7.489117), 0)
    h_1 = max((((-4.8017807 * float(x[0]))+ (18.244282 * float(x[1]))+ (-17.212399 * float(x[2]))+ (-25.659813 * float(x[3]))+ (-23.820993 * float(x[4]))+ (13.835023 * float(x[5]))+ (-11.505152 * float(x[6]))+ (7.8744607 * float(x[7]))+ (-1.5107628 * float(x[8]))+ (12.661104 * float(x[9]))+ (45.824207 * float(x[10]))+ (-6.009873 * float(x[11]))+ (-13.807654 * float(x[12]))+ (23.113998 * float(x[13]))+ (64.61893 * float(x[14]))+ (2.5102367 * float(x[15]))+ (-34.533157 * float(x[16]))+ (-3.9480913 * float(x[17]))+ (-50.677563 * float(x[18]))+ (6.8908596 * float(x[19]))+ (4.004904 * float(x[20]))+ (-11.6556835 * float(x[21]))+ (7.4392633 * float(x[22]))+ (-1.933133 * float(x[23]))+ (4.2556334 * float(x[24]))+ (-3.2315564 * float(x[25]))+ (7.19308 * float(x[26]))+ (-4.574617 * float(x[27]))+ (0.37650737 * float(x[28]))+ (-7.9128833 * float(x[29]))+ (9.384369 * float(x[30]))+ (-3.2200873 * float(x[31]))+ (-1.0584357 * float(x[32]))) + -1.07698), 0)
    h_2 = max((((1.5639251 * float(x[0]))+ (-0.956257 * float(x[1]))+ (-3.263042 * float(x[2]))+ (13.280703 * float(x[3]))+ (-1.3760957 * float(x[4]))+ (-5.768152 * float(x[5]))+ (-14.072992 * float(x[6]))+ (-11.48371 * float(x[7]))+ (8.180516 * float(x[8]))+ (-3.4714446 * float(x[9]))+ (-4.2616034 * float(x[10]))+ (2.15898 * float(x[11]))+ (-3.1202896 * float(x[12]))+ (1.8992112 * float(x[13]))+ (-3.1576962 * float(x[14]))+ (-3.8279917 * float(x[15]))+ (7.277118 * float(x[16]))+ (-1.6590347 * float(x[17]))+ (3.6795602 * float(x[18]))+ (7.2245774 * float(x[19]))+ (-3.5839446 * float(x[20]))+ (-2.8231695 * float(x[21]))+ (2.1693323 * float(x[22]))+ (0.7563514 * float(x[23]))+ (-11.327718 * float(x[24]))+ (4.306537 * float(x[25]))+ (-6.448344 * float(x[26]))+ (7.263309 * float(x[27]))+ (-7.63993 * float(x[28]))+ (-9.348088 * float(x[29]))+ (-2.4871376 * float(x[30]))+ (-3.3523614 * float(x[31]))+ (-3.2974098 * float(x[32]))) + -1.2719221), 0)
    h_3 = max((((-3.1128058 * float(x[0]))+ (0.104157604 * float(x[1]))+ (1.6304538 * float(x[2]))+ (-8.055469 * float(x[3]))+ (-0.8276671 * float(x[4]))+ (1.3946327 * float(x[5]))+ (7.6865664 * float(x[6]))+ (2.050852 * float(x[7]))+ (-1.2705861 * float(x[8]))+ (2.611935 * float(x[9]))+ (4.2008777 * float(x[10]))+ (-0.85084593 * float(x[11]))+ (4.505907 * float(x[12]))+ (-2.700648 * float(x[13]))+ (1.8493888 * float(x[14]))+ (7.3897624 * float(x[15]))+ (-6.0726867 * float(x[16]))+ (2.6413064 * float(x[17]))+ (-0.8242306 * float(x[18]))+ (-10.350549 * float(x[19]))+ (4.506904 * float(x[20]))+ (-2.1975656 * float(x[21]))+ (-0.5005158 * float(x[22]))+ (-0.45041662 * float(x[23]))+ (-0.26430807 * float(x[24]))+ (1.0908648 * float(x[25]))+ (0.7846428 * float(x[26]))+ (-4.3522415 * float(x[27]))+ (10.517375 * float(x[28]))+ (6.9888043 * float(x[29]))+ (-2.545558 * float(x[30]))+ (1.7222731 * float(x[31]))+ (1.4485841 * float(x[32]))) + 1.1466217), 0)
    h_4 = max((((2.6829422 * float(x[0]))+ (0.81436807 * float(x[1]))+ (-0.099906996 * float(x[2]))+ (0.3218483 * float(x[3]))+ (-0.2585461 * float(x[4]))+ (1.965066 * float(x[5]))+ (-2.4762878 * float(x[6]))+ (2.2937064 * float(x[7]))+ (-0.6731487 * float(x[8]))+ (1.1525995 * float(x[9]))+ (1.9931455 * float(x[10]))+ (-3.3217957 * float(x[11]))+ (0.5933932 * float(x[12]))+ (1.6376213 * float(x[13]))+ (6.111166 * float(x[14]))+ (1.8917409 * float(x[15]))+ (0.40618968 * float(x[16]))+ (0.65414035 * float(x[17]))+ (-0.017141784 * float(x[18]))+ (3.8159003 * float(x[19]))+ (2.9451454 * float(x[20]))+ (4.08337 * float(x[21]))+ (1.7478875 * float(x[22]))+ (0.78728217 * float(x[23]))+ (-1.9521323 * float(x[24]))+ (2.2666008 * float(x[25]))+ (-0.059653617 * float(x[26]))+ (-2.761919 * float(x[27]))+ (-2.366806 * float(x[28]))+ (-0.8791653 * float(x[29]))+ (2.160147 * float(x[30]))+ (-1.6164607 * float(x[31]))+ (3.039552 * float(x[32]))) + 2.4568686), 0)
    h_5 = max((((-0.6272444 * float(x[0]))+ (-1.4020319 * float(x[1]))+ (0.933214 * float(x[2]))+ (3.371342 * float(x[3]))+ (2.7319856 * float(x[4]))+ (-1.5760702 * float(x[5]))+ (0.18143418 * float(x[6]))+ (-0.022620399 * float(x[7]))+ (-0.52075815 * float(x[8]))+ (0.21731347 * float(x[9]))+ (-0.44551346 * float(x[10]))+ (1.329511 * float(x[11]))+ (2.0622926 * float(x[12]))+ (-0.55567884 * float(x[13]))+ (-5.138689 * float(x[14]))+ (1.1084539 * float(x[15]))+ (0.8045283 * float(x[16]))+ (1.6298549 * float(x[17]))+ (3.801118 * float(x[18]))+ (-1.1113564 * float(x[19]))+ (-0.2360231 * float(x[20]))+ (1.2195978 * float(x[21]))+ (1.9575417 * float(x[22]))+ (0.35602236 * float(x[23]))+ (0.32913274 * float(x[24]))+ (0.95442307 * float(x[25]))+ (-0.13607968 * float(x[26]))+ (0.40775514 * float(x[27]))+ (-1.3202468 * float(x[28]))+ (-1.3458787 * float(x[29]))+ (0.46419397 * float(x[30]))+ (-0.1447536 * float(x[31]))+ (2.780792 * float(x[32]))) + -1.3654151), 0)
    h_6 = max((((2.094943 * float(x[0]))+ (0.051541064 * float(x[1]))+ (-0.24217981 * float(x[2]))+ (-0.23127945 * float(x[3]))+ (0.09160857 * float(x[4]))+ (4.165071 * float(x[5]))+ (0.021045746 * float(x[6]))+ (2.8876152 * float(x[7]))+ (-0.5976106 * float(x[8]))+ (0.52650976 * float(x[9]))+ (0.591054 * float(x[10]))+ (1.501099 * float(x[11]))+ (0.6369063 * float(x[12]))+ (0.31574163 * float(x[13]))+ (0.74293995 * float(x[14]))+ (0.61418885 * float(x[15]))+ (-1.1413842 * float(x[16]))+ (-0.19859251 * float(x[17]))+ (-1.3927852 * float(x[18]))+ (-0.033152055 * float(x[19]))+ (-0.3517766 * float(x[20]))+ (-0.115460046 * float(x[21]))+ (0.6027966 * float(x[22]))+ (-0.26602238 * float(x[23]))+ (0.9985104 * float(x[24]))+ (-0.8184981 * float(x[25]))+ (0.8877678 * float(x[26]))+ (-0.5766971 * float(x[27]))+ (0.7762201 * float(x[28]))+ (0.6741612 * float(x[29]))+ (-1.6351324 * float(x[30]))+ (0.2866998 * float(x[31]))+ (0.064732604 * float(x[32]))) + -0.22245009), 0)
    h_7 = max((((-1.8143853 * float(x[0]))+ (0.38340595 * float(x[1]))+ (-0.663917 * float(x[2]))+ (-2.8774798 * float(x[3]))+ (-0.941404 * float(x[4]))+ (1.0285988 * float(x[5]))+ (-0.13969633 * float(x[6]))+ (0.49884245 * float(x[7]))+ (0.07075905 * float(x[8]))+ (0.0386424 * float(x[9]))+ (0.39629838 * float(x[10]))+ (0.21872284 * float(x[11]))+ (-0.78995275 * float(x[12]))+ (0.29732814 * float(x[13]))+ (2.4478009 * float(x[14]))+ (-0.29486033 * float(x[15]))+ (-0.62397647 * float(x[16]))+ (-0.6001709 * float(x[17]))+ (-1.1839477 * float(x[18]))+ (1.5271252 * float(x[19]))+ (0.07715423 * float(x[20]))+ (-0.4545256 * float(x[21]))+ (-0.3600496 * float(x[22]))+ (-0.29327735 * float(x[23]))+ (0.66145414 * float(x[24]))+ (-0.72438914 * float(x[25]))+ (0.41082656 * float(x[26]))+ (0.00903918 * float(x[27]))+ (0.26527053 * float(x[28]))+ (-0.052664187 * float(x[29]))+ (1.5106412 * float(x[30]))+ (-0.019941194 * float(x[31]))+ (0.104372844 * float(x[32]))) + 2.4283962), 0)
    h_8 = max((((1.7921739 * float(x[0]))+ (0.5836042 * float(x[1]))+ (-0.53801763 * float(x[2]))+ (-0.35624972 * float(x[3]))+ (-0.5181377 * float(x[4]))+ (0.8284822 * float(x[5]))+ (-0.08919219 * float(x[6]))+ (1.0785027 * float(x[7]))+ (-0.53164595 * float(x[8]))+ (1.0888298 * float(x[9]))+ (1.4858283 * float(x[10]))+ (1.2380421 * float(x[11]))+ (-0.1355474 * float(x[12]))+ (0.28299665 * float(x[13]))+ (1.3661044 * float(x[14]))+ (-0.985987 * float(x[15]))+ (-0.92388517 * float(x[16]))+ (-0.37880716 * float(x[17]))+ (-1.2498242 * float(x[18]))+ (0.18294077 * float(x[19]))+ (0.47006705 * float(x[20]))+ (-0.47146818 * float(x[21]))+ (0.15899149 * float(x[22]))+ (0.089617655 * float(x[23]))+ (0.28022233 * float(x[24]))+ (-0.121206455 * float(x[25]))+ (-0.31088674 * float(x[26]))+ (0.10579232 * float(x[27]))+ (0.0024399809 * float(x[28]))+ (-0.25332212 * float(x[29]))+ (0.2611484 * float(x[30]))+ (-0.06912862 * float(x[31]))+ (0.61972445 * float(x[32]))) + 0.9213361), 0)
    h_9 = max((((1.5009797 * float(x[0]))+ (-0.9403664 * float(x[1]))+ (1.0048178 * float(x[2]))+ (0.8403537 * float(x[3]))+ (1.3150876 * float(x[4]))+ (0.45472893 * float(x[5]))+ (1.071398 * float(x[6]))+ (0.74474454 * float(x[7]))+ (-0.30345252 * float(x[8]))+ (-0.14404984 * float(x[9]))+ (-2.7738795 * float(x[10]))+ (1.3534212 * float(x[11]))+ (1.0471437 * float(x[12]))+ (-1.9362774 * float(x[13]))+ (-3.1321757 * float(x[14]))+ (-0.44129688 * float(x[15]))+ (1.8257508 * float(x[16]))+ (-0.06633873 * float(x[17]))+ (3.1440294 * float(x[18]))+ (0.07986986 * float(x[19]))+ (0.27759233 * float(x[20]))+ (1.0378274 * float(x[21]))+ (-0.7223969 * float(x[22]))+ (-0.004895142 * float(x[23]))+ (0.30294192 * float(x[24]))+ (-0.24301359 * float(x[25]))+ (-0.1901727 * float(x[26]))+ (-0.12181673 * float(x[27]))+ (0.252875 * float(x[28]))+ (1.1104369 * float(x[29]))+ (-0.436789 * float(x[30]))+ (0.23444632 * float(x[31]))+ (0.77357507 * float(x[32]))) + 3.3094187), 0)
    o[0] = (0.2747256 * h_0)+ (-0.45789775 * h_1)+ (-0.13722274 * h_2)+ (0.23299937 * h_3)+ (0.38410363 * h_4)+ (-0.9279001 * h_5)+ (1.0884652 * h_6)+ (1.9276938 * h_7)+ (2.8794606 * h_8)+ (-3.759388 * h_9) + 1.4005651

    

    #Output Decision Rule
    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)


def classify(arr, transform=False):
    #apply transformation if necessary
    if transform:
        arr[:,:-1] = transform(arr[:,:-1])
    #init
    w_h = np.array([[6.083156585693359, -27.498767852783203, 23.70732307434082, 40.06234359741211, 36.225921630859375, -26.446300506591797, 19.4083194732666, -13.38032054901123, 0.7789950370788574, -16.737586975097656, -63.80668258666992, 7.2591376304626465, 21.855165481567383, -32.547767639160156, -93.29338073730469, -3.7893943786621094, 49.32247543334961, 4.985969543457031, 76.10554504394531, -11.960603713989258, -0.5317662358283997, 13.687848091125488, -15.113129615783691, 1.4289445877075195, -12.846611976623535, 10.580662727355957, -13.07329273223877, 4.731997013092041, -7.092736721038818, 12.090089797973633, -14.586941719055176, 2.908241033554077, -3.062828302383423], [-4.801780700683594, 18.244281768798828, -17.212398529052734, -25.659812927246094, -23.820993423461914, 13.835022926330566, -11.505151748657227, 7.874460697174072, -1.5107628107070923, 12.661104202270508, 45.8242073059082, -6.009872913360596, -13.80765438079834, 23.113998413085938, 64.61892700195312, 2.5102367401123047, -34.53315734863281, -3.9480912685394287, -50.67756271362305, 6.890859603881836, 4.004903793334961, -11.655683517456055, 7.439263343811035, -1.9331330060958862, 4.255633354187012, -3.2315564155578613, 7.193079948425293, -4.5746169090271, 0.37650737166404724, -7.912883281707764, 9.384368896484375, -3.2200872898101807, -1.0584356784820557], [1.5639251470565796, -0.9562569856643677, -3.2630419731140137, 13.280702590942383, -1.3760956525802612, -5.768152236938477, -14.072992324829102, -11.483710289001465, 8.180516242980957, -3.471444606781006, -4.261603355407715, 2.158979892730713, -3.1202895641326904, 1.8992111682891846, -3.15769624710083, -3.8279917240142822, 7.27711820602417, -1.6590347290039062, 3.6795601844787598, 7.2245774269104, -3.58394455909729, -2.823169469833374, 2.169332265853882, 0.7563514113426208, -11.327717781066895, 4.30653715133667, -6.4483442306518555, 7.263309001922607, -7.63992977142334, -9.348088264465332, -2.48713755607605, -3.3523614406585693, -3.297409772872925], [-3.1128058433532715, 0.10415760427713394, 1.6304538249969482, -8.055468559265137, -0.8276671171188354, 1.3946326971054077, 7.686566352844238, 2.050852060317993, -1.2705861330032349, 2.6119349002838135, 4.200877666473389, -0.8508459329605103, 4.50590705871582, -2.700648069381714, 1.849388837814331, 7.3897624015808105, -6.072686672210693, 2.6413064002990723, -0.8242306113243103, -10.35054874420166, 4.506904125213623, -2.1975655555725098, -0.5005158185958862, -0.450416624546051, -0.26430806517601013, 1.0908647775650024, 0.7846428155899048, -4.352241516113281, 10.517374992370605, 6.988804340362549, -2.545557975769043, 1.7222731113433838, 1.4485840797424316], [2.6829421520233154, 0.8143680691719055, -0.09990699589252472, 0.3218483030796051, -0.2585461139678955, 1.9650659561157227, -2.476287841796875, 2.2937064170837402, -0.6731486916542053, 1.1525994539260864, 1.99314546585083, -3.321795701980591, 0.5933932065963745, 1.637621283531189, 6.111166000366211, 1.8917409181594849, 0.4061896800994873, 0.6541403532028198, -0.017141783609986305, 3.8159003257751465, 2.94514536857605, 4.083370208740234, 1.7478874921798706, 0.7872821688652039, -1.9521323442459106, 2.2666008472442627, -0.059653617441654205, -2.7619190216064453, -2.3668060302734375, -0.8791652917861938, 2.160146951675415, -1.6164606809616089, 3.0395519733428955], [-0.6272444128990173, -1.4020318984985352, 0.933214008808136, 3.3713419437408447, 2.731985569000244, -1.5760701894760132, 0.18143418431282043, -0.022620398551225662, -0.5207581520080566, 0.2173134684562683, -0.44551345705986023, 1.329511046409607, 2.0622925758361816, -0.5556788444519043, -5.138689041137695, 1.1084538698196411, 0.8045282959938049, 1.6298549175262451, 3.8011178970336914, -1.1113563776016235, -0.23602309823036194, 1.2195978164672852, 1.9575417041778564, 0.35602235794067383, 0.32913273572921753, 0.9544230699539185, -0.13607968389987946, 0.40775513648986816, -1.3202468156814575, -1.3458787202835083, 0.4641939699649811, -0.1447536051273346, 2.780791997909546], [2.094943046569824, 0.05154106393456459, -0.24217981100082397, -0.23127944767475128, 0.09160856902599335, 4.1650710105896, 0.02104574628174305, 2.887615203857422, -0.597610592842102, 0.5265097618103027, 0.5910540223121643, 1.5010989904403687, 0.6369063258171082, 0.315741628408432, 0.7429399490356445, 0.6141888499259949, -1.141384243965149, -0.19859251379966736, -1.3927851915359497, -0.033152054995298386, -0.3517765998840332, -0.11546004563570023, 0.6027966141700745, -0.26602238416671753, 0.9985104203224182, -0.8184980750083923, 0.8877677917480469, -0.5766971111297607, 0.7762200832366943, 0.6741611957550049, -1.6351324319839478, 0.2866998016834259, 0.06473260372877121], [-1.8143852949142456, 0.3834059536457062, -0.6639170050621033, -2.8774797916412354, -0.9414039850234985, 1.0285987854003906, -0.13969632983207703, 0.4988424479961395, 0.07075905054807663, 0.03864239901304245, 0.3962983787059784, 0.21872283518314362, -0.7899527549743652, 0.2973281443119049, 2.447800874710083, -0.2948603332042694, -0.623976469039917, -0.600170910358429, -1.1839476823806763, 1.5271252393722534, 0.07715422660112381, -0.45452558994293213, -0.36004960536956787, -0.2932773530483246, 0.6614541411399841, -0.7243891358375549, 0.41082656383514404, 0.009039180353283882, 0.26527053117752075, -0.05266418680548668, 1.5106412172317505, -0.019941193982958794, 0.10437284409999847], [1.7921738624572754, 0.5836042165756226, -0.5380176305770874, -0.35624971985816956, -0.5181376934051514, 0.8284822106361389, -0.08919218927621841, 1.0785026550292969, -0.5316459536552429, 1.088829755783081, 1.4858282804489136, 1.2380421161651611, -0.13554739952087402, 0.28299665451049805, 1.3661043643951416, -0.9859870076179504, -0.92388516664505, -0.3788071572780609, -1.2498241662979126, 0.1829407662153244, 0.4700670540332794, -0.4714681804180145, 0.1589914858341217, 0.08961765468120575, 0.2802223265171051, -0.12120645493268967, -0.3108867406845093, 0.1057923212647438, 0.002439980860799551, -0.25332212448120117, 0.2611483931541443, -0.06912861764431, 0.619724452495575], [1.5009796619415283, -0.9403663873672485, 1.0048178434371948, 0.8403537273406982, 1.3150875568389893, 0.4547289311885834, 1.0713980197906494, 0.7447445392608643, -0.3034525215625763, -0.14404983818531036, -2.7738795280456543, 1.3534212112426758, 1.0471436977386475, -1.9362773895263672, -3.1321756839752197, -0.44129687547683716, 1.8257508277893066, -0.06633873283863068, 3.144029378890991, 0.07986985892057419, 0.27759233117103577, 1.0378273725509644, -0.7223969101905823, -0.004895141813904047, 0.3029419183731079, -0.24301359057426453, -0.1901727020740509, -0.1218167319893837, 0.2528750002384186, 1.1104369163513184, -0.43678900599479675, 0.23444631695747375, 0.7735750675201416]])
    b_h = np.array([7.48911714553833, -1.0769799947738647, -1.2719221115112305, 1.1466217041015625, 2.4568686485290527, -1.365415096282959, -0.22245009243488312, 2.428396224975586, 0.9213361144065857, 3.3094186782836914])
    w_o = np.array([[0.2747255861759186, -0.45789775252342224, -0.13722273707389832, 0.23299936950206757, 0.3841036260128021, -0.9279000759124756, 1.0884652137756348, 1.9276938438415527, 2.879460573196411, -3.759387969970703]])
    b_o = np.array(1.4005651473999023)

    #Hidden Layer
    h = np.dot(arr, w_h.T) + b_h
    
    relu = np.maximum(h, np.zeros_like(h))


    #Output
    out = np.dot(relu, w_o.T) + b_o
    if num_output_logits == 1:
        return (out >= 0).astype('int').reshape(-1)
    else:
        return (np.argmax(out, axis=1)).reshape(-1)



def Predict(arr,headerless,csvfile, get_key, classmapping):
    with open(csvfile, 'r') as csvinput:
        #readers and writers
        reader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(reader, None) + ["Prediction"]))
        
        
        for i, row in enumerate(reader):
            #use the transformed array as input to predictor
            pred = str(get_key(int(single_classify(arr[i])), classmapping))
            #use original untransformed line to write out
            row.append(pred)
            print(','.join(row))


def Validate(cleanarr):
    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs
    


# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    args = parser.parse_args()
    faulthandler.enable()


    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}


    #load file
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')


    #Normalize
    cleanarr = Normalize(cleanarr)


    #Transform
    if transform_true:
        if args.validate:
            trans = transform(cleanarr[:, :-1])
            cleanarr = np.concatenate((trans, cleanarr[:, -1].reshape(-1, 1)), axis = 1)
        else:
            cleanarr = transform(cleanarr)


    #Predict
    if not args.validate:
        Predict(cleanarr, args.headerless, preprocessedfile, get_key, classmapping)


    #Validate
    else:
        classifier_type = 'NN'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = Validate(cleanarr)
        else:
            count, correct_count, numeachclass, preds = Validate(cleanarr)
        #Correct Labels
        true_labels = cleanarr[:, -1]


        #Report Metrics
        model_cap = 351
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count
        
            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            if args.json:
                #                json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'n_classes':2, 'Number of False Negative Instances': num_FN, 'Number of False Positive Instances': num_FP, 'Number of True Positive Instances': num_TP, 'Number of True Negative Instances': num_TN,   'False Negatives': FN, 'False Positives': FP, 'True Negatives': TN, 'True Positives': TP, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0}
                json_dict = {'instance_count':                        count ,
                            'classifier_type':                        classifier_type ,
                            'n_classes':                            2 ,
                            'number_of_false_negative_instances':    num_FN ,
                            'number_of_false_positive_instances':    num_FP ,
                            'number_of_true_positive_instances':    num_TP ,
                            'number_of_true_negative_instances':    num_TN,
                            'false_negatives':                        FN ,
                            'false_positives':                        FP ,
                            'true_negatives':                        TN ,
                            'true_positives':                        TP ,
                            'number_correct':                        num_correct ,
                            'best_guess':                            randguess ,
                            'model_accuracy':                        modelacc ,
                            'model_capacity':                        model_cap ,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                             }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            if args.json:
        #        json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0, 'n_classes': n_classes}
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'n_classes':                            n_classes,
                            'number_correct':                        num_correct,
                            'best_guess':                            randguess,
                            'model_accuracy':                        modelacc,
                            'model_capacity':                        model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                            }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                sys.exit()
            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")


            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            n_labels = labels.size
            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]
            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm
        mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])

    #Clean Up
    if not args.cleanfile:
        os.remove(cleanfile)
        os.remove(preprocessedfile)


