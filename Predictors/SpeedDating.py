#!/usr/bin/env python3
#
# This code is licensed under GNU GPL v2.0 or higher. Please see LICENSE for details.
#
#
# Output of Brainome Daimensions(tm) Table Compiler v0.91.
# Compile time: Mar-20-2020 02:11:09
# Invocation: btc -server brain.brainome.ai Data/SpeedDating.csv -o Models/SpeedDating.py -v -v -v -stopat 87.1 -port 8100 -f NN -e 10
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                83.52%
Model accuracy:                     99.90% (8370/8378 correct)
Improvement over best guess:        16.38% (of possible 16.48%)
Model capacity (MEC):               249 bits
Generalization ratio:               33.61 bits/bit
Model efficiency:                   0.06%/parameter
System behavior
True Negatives:                     83.53% (6998/8378)
True Positives:                     16.38% (1372/8378)
False Negatives:                    0.10% (8/8378)
False Positives:                    0.00% (0/8378)
True Pos. Rate/Sensitivity/Recall:  0.99
True Neg. Rate/Specificity:         1.00
Precision:                          1.00
F-1 Measure:                        1.00
False Negative Rate/Miss Rate:      0.01
Critical Success Index:             0.99

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF=100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE="SpeedDating.csv"


#Number of output logits
num_output_logits = 1

#Number of attributes
num_attr = 122
n_classes = 2

mappings = [{1249151596.0: 0, 1435361449.0: 1}, {18.0: 0, 19.0: 1, 20.0: 2, 21.0: 3, 22.0: 4, 23.0: 5, 24.0: 6, 25.0: 7, 26.0: 8, 27.0: 9, 28.0: 10, 29.0: 11, 30.0: 12, 31.0: 13, 32.0: 14, 33.0: 15, 34.0: 16, 35.0: 17, 36.0: 18, 37.0: 19, 38.0: 20, 39.0: 21, 42.0: 22, 1684325040.0: 23, 55.0: 24}, {18.0: 0, 19.0: 1, 20.0: 2, 21.0: 3, 22.0: 4, 23.0: 5, 24.0: 6, 25.0: 7, 26.0: 8, 27.0: 9, 28.0: 10, 29.0: 11, 30.0: 12, 31.0: 13, 32.0: 14, 33.0: 15, 34.0: 16, 35.0: 17, 36.0: 18, 37.0: 19, 38.0: 20, 39.0: 21, 42.0: 22, 55.0: 23, 1684325040.0: 24}, {48735271.0: 0, 891460694.0: 1, 1526403006.0: 2, 2597850670.0: 3}, {32599200.0: 0, 293327590.0: 1, 412686884.0: 2, 1684325040.0: 3, 2399808559.0: 4, 4087319907.0: 5}, {32599200.0: 0, 293327590.0: 1, 412686884.0: 2, 1684325040.0: 3, 2399808559.0: 4, 4087319907.0: 5}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {1149761359.0: 0, 1421673377.0: 1, 2597850670.0: 2}, {1149761359.0: 0, 1421673377.0: 1, 2597850670.0: 2}, {15017931.0: 0, 26596220.0: 1, 44306896.0: 2, 45905266.0: 3, 47730397.0: 4, 54464509.0: 5, 70499459.0: 6, 125480160.0: 7, 148074040.0: 8, 152415091.0: 9, 152645570.0: 10, 174075239.0: 11, 185490720.0: 12, 201029600.0: 13, 202069295.0: 14, 202918097.0: 15, 225501280.0: 16, 242652078.0: 17, 307361439.0: 18, 321187713.0: 19, 376497099.0: 20, 395107967.0: 21, 416360697.0: 22, 435878292.0: 23, 450920598.0: 24, 460805064.0: 25, 462970398.0: 26, 465879842.0: 27, 476883735.0: 28, 485909208.0: 29, 497974144.0: 30, 539054596.0: 31, 547815489.0: 32, 556358680.0: 33, 567325923.0: 34, 578162972.0: 35, 579089110.0: 36, 628612686.0: 37, 647408932.0: 38, 668560958.0: 39, 677667344.0: 40, 710571969.0: 41, 723882923.0: 42, 744116156.0: 43, 763128263.0: 44, 783136072.0: 45, 792979606.0: 46, 803176885.0: 47, 814319542.0: 48, 835505082.0: 49, 877007823.0: 50, 892527186.0: 51, 912111495.0: 52, 915709135.0: 53, 917775891.0: 54, 919988817.0: 55, 953004754.0: 56, 974512775.0: 57, 988992924.0: 58, 997845351.0: 59, 1049599207.0: 60, 1131395653.0: 61, 1135887503.0: 62, 1150786304.0: 63, 1229170577.0: 64, 1230742932.0: 65, 1252826409.0: 66, 1264104144.0: 67, 1266677118.0: 68, 1277843441.0: 69, 1301941636.0: 70, 1303067116.0: 71, 1379478425.0: 72, 1383944571.0: 73, 1412059160.0: 74, 1444290179.0: 75, 1472924849.0: 76, 1479944845.0: 77, 1501617769.0: 78, 1509678193.0: 79, 1514188848.0: 80, 1519435996.0: 81, 1565826303.0: 82, 1599948065.0: 83, 1615653514.0: 84, 1684325040.0: 85, 1705184133.0: 86, 1705543801.0: 87, 1723067856.0: 88, 1732187885.0: 89, 1733942901.0: 90, 1737319721.0: 91, 1744917525.0: 92, 1822983426.0: 93, 1855372604.0: 94, 1877464402.0: 95, 1903292387.0: 96, 1906081943.0: 97, 1965550008.0: 98, 1970028622.0: 99, 1971344674.0: 100, 1990682308.0: 101, 2021470578.0: 102, 2032881756.0: 103, 2046499943.0: 104, 2052821654.0: 105, 2059410989.0: 106, 2081176863.0: 107, 2129020949.0: 108, 2137141061.0: 109, 2140965185.0: 110, 2141759096.0: 111, 2149714801.0: 112, 2179109143.0: 113, 2185543202.0: 114, 2219141848.0: 115, 2225957936.0: 116, 2227358946.0: 117, 2236863473.0: 118, 2237218368.0: 119, 2274662888.0: 120, 2289208681.0: 121, 2291829392.0: 122, 2297044089.0: 123, 2304817352.0: 124, 2320419972.0: 125, 2355851193.0: 126, 2374151408.0: 127, 2391364880.0: 128, 2427146600.0: 129, 2428110460.0: 130, 2484243389.0: 131, 2486560480.0: 132, 2500341399.0: 133, 2525382830.0: 134, 2525804511.0: 135, 2543502991.0: 136, 2570404225.0: 137, 2594969509.0: 138, 2605828515.0: 139, 2614965780.0: 140, 2629663898.0: 141, 2631457595.0: 142, 2660226368.0: 143, 2680700989.0: 144, 2698023017.0: 145, 2704150950.0: 146, 2704391337.0: 147, 2705344731.0: 148, 2718511297.0: 149, 2777927614.0: 150, 2784562539.0: 151, 2790415716.0: 152, 2802803537.0: 153, 2837906509.0: 154, 2882982840.0: 155, 2891941146.0: 156, 2897068474.0: 157, 2911799538.0: 158, 2921381126.0: 159, 2922246272.0: 160, 2928445002.0: 161, 2929283935.0: 162, 2943733342.0: 163, 2946616011.0: 164, 2946753075.0: 165, 2968130490.0: 166, 2972234353.0: 167, 2986510902.0: 168, 3007168437.0: 169, 3013307454.0: 170, 3035916902.0: 171, 3044351403.0: 172, 3050114098.0: 173, 3054593790.0: 174, 3055157402.0: 175, 3081187861.0: 176, 3083034865.0: 177, 3084850148.0: 178, 3106646587.0: 179, 3115054711.0: 180, 3135487633.0: 181, 3136767196.0: 182, 3142142030.0: 183, 3151460722.0: 184, 3159900262.0: 185, 3161081293.0: 186, 3170319941.0: 187, 3175198032.0: 188, 3196150197.0: 189, 3228538363.0: 190, 3238683597.0: 191, 3238721665.0: 192, 3271835324.0: 193, 3355731387.0: 194, 3373371030.0: 195, 3377852338.0: 196, 3389968417.0: 197, 3399552473.0: 198, 3399686005.0: 199, 3413706075.0: 200, 3419816900.0: 201, 3442663479.0: 202, 3449926704.0: 203, 3456585449.0: 204, 3479035876.0: 205, 3486515508.0: 206, 3487760025.0: 207, 3489681553.0: 208, 3500039813.0: 209, 3504220298.0: 210, 3504567967.0: 211, 3560205868.0: 212, 3566038745.0: 213, 3571740086.0: 214, 3578160045.0: 215, 3583243144.0: 216, 3587950132.0: 217, 3617796739.0: 218, 3627149283.0: 219, 3630120526.0: 220, 3644545499.0: 221, 3668887089.0: 222, 3674889938.0: 223, 3779797910.0: 224, 3781602543.0: 225, 3787739279.0: 226, 3811281256.0: 227, 3812506524.0: 228, 3820396918.0: 229, 3831898384.0: 230, 3850168202.0: 231, 3892791767.0: 232, 3909865880.0: 233, 3911173544.0: 234, 3911423663.0: 235, 3950675667.0: 236, 3968181327.0: 237, 3976693656.0: 238, 4026731070.0: 239, 4029075826.0: 240, 4038959242.0: 241, 4041613553.0: 242, 4043792187.0: 243, 4053401639.0: 244, 4054125678.0: 245, 4086336312.0: 246, 4093368023.0: 247, 4108133746.0: 248, 4117487106.0: 249, 4172474564.0: 250, 4210444082.0: 251, 4214835180.0: 252, 4231158915.0: 253, 4231211930.0: 254, 4247169298.0: 255, 4250999620.0: 256, 4255704155.0: 257, 4289499319.0: 258, 4141746538.0: 259}, {0.0: 0, 2.0: 1, 5.0: 2, 6.67: 3, 7.0: 4, 7.5: 5, 8.0: 6, 8.33: 7, 8.51: 8, 9.0: 9, 9.09: 10, 9.52: 11, 9.76: 12, 10.0: 13, 11.11: 14, 11.36: 15, 11.54: 16, 12.0: 17, 12.24: 18, 12.77: 19, 13.04: 20, 13.21: 21, 13.51: 22, 14.0: 23, 14.29: 24, 14.55: 25, 14.58: 26, 14.71: 27, 14.89: 28, 15.0: 29, 15.09: 30, 15.22: 31, 15.38: 32, 15.52: 33, 15.56: 34, 15.91: 35, 16.0: 36, 16.07: 37, 16.28: 38, 16.36: 39, 16.67: 40, 16.98: 41, 17.0: 42, 17.02: 43, 17.24: 44, 17.31: 45, 17.39: 46, 17.5: 47, 17.65: 48, 17.78: 49, 18.0: 50, 18.18: 51, 18.37: 52, 18.6: 53, 18.75: 54, 19.0: 55, 19.05: 56, 19.15: 57, 19.44: 58, 19.57: 59, 19.61: 60, 20.0: 61, 20.45: 62, 20.51: 63, 20.83: 64, 20.93: 65, 21.0: 66, 21.28: 67, 21.43: 68, 22.0: 69, 23.0: 70, 23.81: 71, 24.0: 72, 25.0: 73, 25.64: 74, 27.0: 75, 27.78: 76, 28.0: 77, 30.0: 78, 31.58: 79, 33.33: 80, 35.0: 81, 40.0: 82, 45.0: 83, 50.0: 84, 55.0: 85, 58.0: 86, 60.0: 87, 70.0: 88, 75.0: 89, 80.0: 90, 90.0: 91, 95.0: 92, 100.0: 93, 1684325040.0: 94}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 5.0: 4, 5.13: 5, 7.0: 6, 8.0: 7, 10.0: 8, 10.53: 9, 10.87: 10, 11.11: 11, 12.0: 12, 12.5: 13, 13.0: 14, 13.46: 15, 13.95: 16, 14.0: 17, 14.29: 18, 14.53: 19, 14.71: 20, 15.0: 21, 15.09: 22, 15.22: 23, 15.56: 24, 15.69: 25, 16.0: 26, 16.28: 27, 16.33: 28, 16.36: 29, 16.67: 30, 16.98: 31, 17.0: 32, 17.02: 33, 17.24: 34, 17.31: 35, 17.39: 36, 17.5: 37, 17.65: 38, 17.78: 39, 17.86: 40, 17.95: 41, 18.0: 42, 18.18: 43, 18.37: 44, 18.75: 45, 18.87: 46, 18.92: 47, 19.0: 48, 19.05: 49, 19.15: 50, 19.23: 51, 19.44: 52, 19.51: 53, 19.57: 54, 20.0: 55, 20.41: 56, 20.45: 57, 20.83: 58, 20.93: 59, 21.0: 60, 21.28: 61, 21.74: 62, 22.0: 63, 22.5: 64, 22.73: 65, 23.0: 66, 23.08: 67, 23.81: 68, 24.0: 69, 25.0: 70, 26.0: 71, 30.0: 72, 32.0: 73, 35.0: 74, 40.0: 75, 47.0: 76, 60.0: 77, 1684325040.0: 78}, {0.0: 0, 1.0: 1, 2.0: 2, 5.0: 3, 8.0: 4, 10.0: 5, 11.11: 6, 14.71: 7, 15.0: 8, 15.22: 9, 15.38: 10, 15.79: 11, 16.0: 12, 16.33: 13, 16.67: 14, 16.98: 15, 17.0: 16, 17.02: 17, 17.24: 18, 17.31: 19, 17.39: 20, 17.5: 21, 17.65: 22, 17.78: 23, 17.86: 24, 18.0: 25, 18.18: 26, 18.37: 27, 18.6: 28, 18.75: 29, 18.87: 30, 19.0: 31, 19.05: 32, 19.15: 33, 19.23: 34, 19.44: 35, 19.51: 36, 19.57: 37, 20.0: 38, 20.41: 39, 20.45: 40, 20.51: 41, 20.83: 42, 21.0: 43, 21.28: 44, 21.43: 45, 21.62: 46, 22.0: 47, 22.22: 48, 22.73: 49, 23.0: 50, 23.08: 51, 23.26: 52, 23.81: 53, 24.79: 54, 25.0: 55, 27.0: 56, 27.27: 57, 28.0: 58, 30.0: 59, 35.0: 60, 40.0: 61, 42.86: 62, 45.0: 63, 50.0: 64, 1684325040.0: 65}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 5.0: 4, 8.0: 5, 9.52: 6, 10.0: 7, 11.11: 8, 12.0: 9, 12.5: 10, 12.77: 11, 12.82: 12, 13.0: 13, 13.51: 14, 13.64: 15, 14.0: 16, 14.29: 17, 14.58: 18, 14.63: 19, 14.71: 20, 15.0: 21, 15.56: 22, 15.69: 23, 16.0: 24, 16.28: 25, 16.33: 26, 16.67: 27, 16.98: 28, 17.0: 29, 17.02: 30, 17.09: 31, 17.24: 32, 17.31: 33, 17.39: 34, 17.78: 35, 17.86: 36, 17.95: 37, 18.0: 38, 18.18: 39, 18.37: 40, 18.6: 41, 18.75: 42, 18.87: 43, 19.0: 44, 19.05: 45, 19.15: 46, 19.23: 47, 19.57: 48, 20.0: 49, 20.41: 50, 20.45: 51, 20.51: 52, 20.83: 53, 21.05: 54, 21.28: 55, 21.43: 56, 22.0: 57, 22.5: 58, 23.0: 59, 23.26: 60, 23.81: 61, 24.0: 62, 25.0: 63, 27.0: 64, 27.78: 65, 30.0: 66, 35.0: 67, 40.0: 68, 45.0: 69, 50.0: 70, 1684325040.0: 71}, {0.0: 0, 1.0: 1, 2.0: 2, 2.33: 3, 2.38: 4, 2.56: 5, 2.78: 6, 3.0: 7, 4.0: 8, 4.76: 9, 5.0: 10, 5.98: 11, 6.0: 12, 6.25: 13, 6.38: 14, 6.67: 15, 7.0: 16, 8.0: 17, 9.0: 18, 9.52: 19, 9.62: 20, 10.0: 21, 10.26: 22, 10.53: 23, 10.87: 24, 11.0: 25, 11.11: 26, 11.36: 27, 11.54: 28, 11.63: 29, 11.9: 30, 12.0: 31, 12.5: 32, 12.77: 33, 13.0: 34, 13.04: 35, 13.21: 36, 13.33: 37, 13.46: 38, 13.51: 39, 13.64: 40, 13.79: 41, 13.95: 42, 14.0: 43, 14.29: 44, 14.81: 45, 14.89: 46, 15.0: 47, 15.22: 48, 15.38: 49, 15.56: 50, 15.69: 51, 16.0: 52, 16.28: 53, 16.33: 54, 16.36: 55, 16.67: 56, 16.98: 57, 17.0: 58, 17.24: 59, 17.31: 60, 17.65: 61, 17.78: 62, 17.86: 63, 17.95: 64, 18.0: 65, 18.18: 66, 18.37: 67, 18.75: 68, 18.87: 69, 19.0: 70, 19.05: 71, 19.15: 72, 19.23: 73, 19.51: 74, 19.57: 75, 20.0: 76, 20.41: 77, 20.59: 78, 25.0: 79, 30.0: 80, 53.0: 81, 1684325040.0: 82}, {0.0: 0, 1.0: 1, 2.0: 2, 2.27: 3, 2.38: 4, 2.78: 5, 3.0: 6, 4.0: 7, 5.0: 8, 6.0: 9, 6.12: 10, 6.67: 11, 7.0: 12, 7.5: 13, 7.62: 14, 8.0: 15, 8.33: 16, 8.51: 17, 9.0: 18, 9.09: 19, 9.52: 20, 10.0: 21, 10.26: 22, 10.53: 23, 10.64: 24, 10.87: 25, 11.0: 26, 11.11: 27, 11.36: 28, 11.54: 29, 11.63: 30, 11.9: 31, 12.0: 32, 12.5: 33, 12.77: 34, 13.0: 35, 13.04: 36, 13.21: 37, 13.33: 38, 13.46: 39, 13.64: 40, 13.73: 41, 14.0: 42, 14.29: 43, 14.55: 44, 14.89: 45, 15.0: 46, 15.09: 47, 15.22: 48, 15.38: 49, 15.52: 50, 15.56: 51, 15.69: 52, 16.0: 53, 16.28: 54, 16.33: 55, 16.36: 56, 16.67: 57, 16.98: 58, 17.0: 59, 17.07: 60, 17.09: 61, 17.24: 62, 17.31: 63, 17.39: 64, 17.78: 65, 18.0: 66, 18.18: 67, 18.52: 68, 18.75: 69, 18.92: 70, 19.0: 71, 19.15: 72, 19.23: 73, 19.57: 74, 20.0: 75, 20.51: 76, 20.59: 77, 21.0: 78, 21.28: 79, 22.0: 80, 22.22: 81, 23.81: 82, 25.0: 83, 30.0: 84, 1684325040.0: 85}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 3.5: 4, 4.0: 5, 5.0: 6, 6.0: 7, 6.5: 8, 7.0: 9, 7.5: 10, 8.0: 11, 9.0: 12, 9.5: 13, 10.0: 14, 1684325040.0: 15, 9.9: 16, 8.5: 17, 10.5: 18}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 4.5: 5, 5.0: 6, 6.0: 7, 7.0: 8, 8.0: 9, 8.5: 10, 9.0: 11, 10.0: 12, 1684325040.0: 13, 7.5: 14}, {0.0: 0, 1.0: 1, 2.0: 2, 2.5: 3, 3.0: 4, 4.0: 5, 5.0: 6, 5.5: 7, 6.0: 8, 7.0: 9, 7.5: 10, 8.0: 11, 8.5: 12, 9.0: 13, 10.0: 14, 1684325040.0: 15, 9.5: 16, 6.5: 17}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 5.5: 6, 6.0: 7, 6.5: 8, 7.0: 9, 7.5: 10, 8.0: 11, 9.0: 12, 9.5: 13, 10.0: 14, 1684325040.0: 15, 11.0: 16, 8.5: 17}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 5.5: 6, 6.0: 7, 7.0: 8, 7.5: 9, 8.0: 10, 9.0: 11, 9.5: 12, 10.0: 13, 1684325040.0: 14, 8.5: 15}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 5.5: 6, 6.0: 7, 6.5: 8, 7.0: 9, 7.5: 10, 8.0: 11, 9.0: 12, 10.0: 13, 1684325040.0: 14, 8.5: 15}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {0.0: 0, 2.0: 1, 5.0: 2, 6.67: 3, 7.0: 4, 7.5: 5, 8.0: 6, 8.33: 7, 8.51: 8, 9.0: 9, 9.09: 10, 9.52: 11, 9.76: 12, 10.0: 13, 11.11: 14, 11.36: 15, 11.54: 16, 12.0: 17, 12.24: 18, 12.77: 19, 13.04: 20, 13.21: 21, 13.51: 22, 14.0: 23, 14.29: 24, 14.55: 25, 14.58: 26, 14.71: 27, 14.89: 28, 15.0: 29, 15.09: 30, 15.22: 31, 15.38: 32, 15.52: 33, 15.56: 34, 15.91: 35, 16.0: 36, 16.07: 37, 16.28: 38, 16.36: 39, 16.67: 40, 16.98: 41, 17.0: 42, 17.02: 43, 17.24: 44, 17.31: 45, 17.39: 46, 17.5: 47, 17.65: 48, 17.78: 49, 18.0: 50, 18.18: 51, 18.37: 52, 18.6: 53, 18.75: 54, 19.0: 55, 19.05: 56, 19.15: 57, 19.44: 58, 19.57: 59, 19.61: 60, 20.0: 61, 20.45: 62, 20.51: 63, 20.83: 64, 20.93: 65, 21.0: 66, 21.28: 67, 21.43: 68, 22.0: 69, 23.0: 70, 23.81: 71, 24.0: 72, 25.0: 73, 25.64: 74, 27.0: 75, 27.78: 76, 28.0: 77, 30.0: 78, 31.58: 79, 33.33: 80, 35.0: 81, 40.0: 82, 45.0: 83, 50.0: 84, 55.0: 85, 58.0: 86, 60.0: 87, 70.0: 88, 75.0: 89, 80.0: 90, 90.0: 91, 95.0: 92, 100.0: 93, 1684325040.0: 94}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 5.0: 4, 5.13: 5, 7.0: 6, 8.0: 7, 10.0: 8, 10.53: 9, 10.87: 10, 11.11: 11, 12.0: 12, 12.5: 13, 13.0: 14, 13.46: 15, 13.95: 16, 14.0: 17, 14.29: 18, 14.53: 19, 14.71: 20, 15.0: 21, 15.09: 22, 15.22: 23, 15.56: 24, 15.69: 25, 16.0: 26, 16.28: 27, 16.33: 28, 16.36: 29, 16.67: 30, 16.98: 31, 17.0: 32, 17.02: 33, 17.24: 34, 17.31: 35, 17.39: 36, 17.5: 37, 17.65: 38, 17.78: 39, 17.86: 40, 17.95: 41, 18.0: 42, 18.18: 43, 18.37: 44, 18.75: 45, 18.87: 46, 18.92: 47, 19.0: 48, 19.05: 49, 19.15: 50, 19.23: 51, 19.44: 52, 19.51: 53, 19.57: 54, 20.0: 55, 20.41: 56, 20.45: 57, 20.83: 58, 20.93: 59, 21.0: 60, 21.28: 61, 21.74: 62, 22.0: 63, 22.5: 64, 22.73: 65, 23.0: 66, 23.08: 67, 23.81: 68, 24.0: 69, 25.0: 70, 26.0: 71, 30.0: 72, 32.0: 73, 35.0: 74, 40.0: 75, 47.0: 76, 60.0: 77, 1684325040.0: 78}, {0.0: 0, 1.0: 1, 2.0: 2, 5.0: 3, 8.0: 4, 10.0: 5, 11.11: 6, 14.71: 7, 15.0: 8, 15.22: 9, 15.38: 10, 15.79: 11, 16.0: 12, 16.33: 13, 16.67: 14, 16.98: 15, 17.0: 16, 17.02: 17, 17.24: 18, 17.31: 19, 17.39: 20, 17.5: 21, 17.65: 22, 17.78: 23, 17.86: 24, 18.0: 25, 18.18: 26, 18.37: 27, 18.6: 28, 18.75: 29, 18.87: 30, 19.0: 31, 19.05: 32, 19.15: 33, 19.23: 34, 19.44: 35, 19.51: 36, 19.57: 37, 20.0: 38, 20.41: 39, 20.45: 40, 20.51: 41, 20.83: 42, 21.0: 43, 21.28: 44, 21.43: 45, 21.62: 46, 22.0: 47, 22.22: 48, 22.73: 49, 23.0: 50, 23.08: 51, 23.26: 52, 23.81: 53, 24.79: 54, 25.0: 55, 27.0: 56, 27.27: 57, 28.0: 58, 30.0: 59, 35.0: 60, 40.0: 61, 42.86: 62, 45.0: 63, 50.0: 64, 1684325040.0: 65}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 5.0: 4, 8.0: 5, 9.52: 6, 10.0: 7, 11.11: 8, 12.0: 9, 12.5: 10, 12.77: 11, 12.82: 12, 13.0: 13, 13.51: 14, 13.64: 15, 14.0: 16, 14.29: 17, 14.58: 18, 14.63: 19, 14.71: 20, 15.0: 21, 15.56: 22, 15.69: 23, 16.0: 24, 16.28: 25, 16.33: 26, 16.67: 27, 16.98: 28, 17.0: 29, 17.02: 30, 17.09: 31, 17.24: 32, 17.31: 33, 17.39: 34, 17.78: 35, 17.86: 36, 17.95: 37, 18.0: 38, 18.18: 39, 18.37: 40, 18.6: 41, 18.75: 42, 18.87: 43, 19.0: 44, 19.05: 45, 19.15: 46, 19.23: 47, 19.57: 48, 20.0: 49, 20.41: 50, 20.45: 51, 20.51: 52, 20.83: 53, 21.05: 54, 21.28: 55, 21.43: 56, 22.0: 57, 22.5: 58, 23.0: 59, 23.26: 60, 23.81: 61, 24.0: 62, 25.0: 63, 27.0: 64, 27.78: 65, 30.0: 66, 35.0: 67, 40.0: 68, 45.0: 69, 50.0: 70, 1684325040.0: 71}, {0.0: 0, 1.0: 1, 2.0: 2, 2.33: 3, 2.38: 4, 2.56: 5, 2.78: 6, 3.0: 7, 4.0: 8, 4.76: 9, 5.0: 10, 5.98: 11, 6.0: 12, 6.25: 13, 6.38: 14, 6.67: 15, 7.0: 16, 8.0: 17, 9.0: 18, 9.52: 19, 9.62: 20, 10.0: 21, 10.26: 22, 10.53: 23, 10.87: 24, 11.0: 25, 11.11: 26, 11.36: 27, 11.54: 28, 11.63: 29, 11.9: 30, 12.0: 31, 12.5: 32, 12.77: 33, 13.0: 34, 13.04: 35, 13.21: 36, 13.33: 37, 13.46: 38, 13.51: 39, 13.64: 40, 13.79: 41, 13.95: 42, 14.0: 43, 14.29: 44, 14.81: 45, 14.89: 46, 15.0: 47, 15.22: 48, 15.38: 49, 15.56: 50, 15.69: 51, 16.0: 52, 16.28: 53, 16.33: 54, 16.36: 55, 16.67: 56, 16.98: 57, 17.0: 58, 17.24: 59, 17.31: 60, 17.65: 61, 17.78: 62, 17.86: 63, 17.95: 64, 18.0: 65, 18.18: 66, 18.37: 67, 18.75: 68, 18.87: 69, 19.0: 70, 19.05: 71, 19.15: 72, 19.23: 73, 19.51: 74, 19.57: 75, 20.0: 76, 20.41: 77, 20.59: 78, 25.0: 79, 30.0: 80, 53.0: 81, 1684325040.0: 82}, {0.0: 0, 1.0: 1, 2.0: 2, 2.27: 3, 2.38: 4, 2.78: 5, 3.0: 6, 4.0: 7, 5.0: 8, 6.0: 9, 6.12: 10, 6.67: 11, 7.0: 12, 7.5: 13, 7.62: 14, 8.0: 15, 8.33: 16, 8.51: 17, 9.0: 18, 9.09: 19, 9.52: 20, 10.0: 21, 10.26: 22, 10.53: 23, 10.64: 24, 10.87: 25, 11.0: 26, 11.11: 27, 11.36: 28, 11.54: 29, 11.63: 30, 11.9: 31, 12.0: 32, 12.5: 33, 12.77: 34, 13.0: 35, 13.04: 36, 13.21: 37, 13.33: 38, 13.46: 39, 13.64: 40, 13.73: 41, 14.0: 42, 14.29: 43, 14.55: 44, 14.89: 45, 15.0: 46, 15.09: 47, 15.22: 48, 15.38: 49, 15.52: 50, 15.56: 51, 15.69: 52, 16.0: 53, 16.28: 54, 16.33: 55, 16.36: 56, 16.67: 57, 16.98: 58, 17.0: 59, 17.07: 60, 17.09: 61, 17.24: 62, 17.31: 63, 17.39: 64, 17.78: 65, 18.0: 66, 18.18: 67, 18.52: 68, 18.75: 69, 18.92: 70, 19.0: 71, 19.15: 72, 19.23: 73, 19.57: 74, 20.0: 75, 20.51: 76, 20.59: 77, 21.0: 78, 21.28: 79, 22.0: 80, 22.22: 81, 23.81: 82, 25.0: 83, 30.0: 84, 1684325040.0: 85}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {42326737.0: 0, 3065050794.0: 1, 3805456058.0: 2}, {2.0: 0, 3.0: 1, 4.0: 2, 5.0: 3, 6.0: 4, 7.0: 5, 8.0: 6, 9.0: 7, 10.0: 8, 1684325040.0: 9}, {2.0: 0, 3.0: 1, 4.0: 2, 5.0: 3, 6.0: 4, 7.0: 5, 8.0: 6, 9.0: 7, 10.0: 8, 1684325040.0: 9}, {2.0: 0, 3.0: 1, 4.0: 2, 5.0: 3, 6.0: 4, 7.0: 5, 8.0: 6, 9.0: 7, 10.0: 8, 1684325040.0: 9}, {3.0: 0, 4.0: 1, 5.0: 2, 6.0: 3, 7.0: 4, 8.0: 5, 9.0: 6, 10.0: 7, 1684325040.0: 8}, {2.0: 0, 3.0: 1, 4.0: 2, 5.0: 3, 6.0: 4, 7.0: 5, 8.0: 6, 9.0: 7, 10.0: 8, 1684325040.0: 9}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 6.5: 7, 7.0: 8, 7.5: 9, 8.0: 10, 8.5: 11, 9.0: 12, 10.0: 13, 1684325040.0: 14, 3.5: 15, 9.5: 16, 9.9: 17}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 8.5: 9, 9.0: 10, 10.0: 11, 1684325040.0: 12, 4.5: 13, 7.5: 14}, {0.0: 0, 1.0: 1, 2.0: 2, 2.5: 3, 3.0: 4, 4.0: 5, 5.0: 6, 5.5: 7, 6.0: 8, 6.5: 9, 7.0: 10, 7.5: 11, 8.0: 12, 8.5: 13, 9.0: 14, 10.0: 15, 1684325040.0: 16, 9.5: 17}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 5.5: 6, 6.0: 7, 6.5: 8, 7.0: 9, 8.0: 10, 8.5: 11, 9.0: 12, 10.0: 13, 1684325040.0: 14, 9.5: 15, 7.5: 16}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 8.5: 9, 9.0: 10, 10.0: 11, 1684325040.0: 12, 5.5: 13, 9.5: 14, 7.5: 15}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 5.5: 6, 6.0: 7, 6.5: 8, 7.0: 9, 7.5: 10, 8.0: 11, 8.5: 12, 9.0: 13, 10.0: 14, 1684325040.0: 15}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 14.0: 11, 1684325040.0: 12}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 13.0: 10, 1684325040.0: 11}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {0.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 1684325040.0: 11}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {-0.83: 0, -0.7: 1, -0.64: 2, -0.63: 3, -0.62: 4, -0.61: 5, -0.59: 6, -0.58: 7, -0.57: 8, -0.56: 9, -0.55: 10, -0.54: 11, -0.52: 12, -0.51: 13, -0.5: 14, -0.49: 15, -0.48: 16, -0.47: 17, -0.46: 18, -0.45: 19, -0.44: 20, -0.43: 21, -0.42: 22, -0.41: 23, -0.4: 24, -0.39: 25, -0.38: 26, -0.37: 27, -0.36: 28, -0.35: 29, -0.34: 30, -0.33: 31, -0.32: 32, -0.31: 33, -0.3: 34, -0.29: 35, -0.28: 36, -0.27: 37, -0.26: 38, -0.25: 39, -0.24: 40, -0.23: 41, -0.22: 42, -0.21: 43, -0.2: 44, -0.19: 45, -0.18: 46, -0.17: 47, -0.16: 48, -0.15: 49, -0.14: 50, -0.13: 51, -0.12: 52, -0.11: 53, -0.1: 54, -0.09: 55, -0.08: 56, -0.07: 57, -0.06: 58, -0.05: 59, -0.04: 60, -0.03: 61, -0.02: 62, -0.01: 63, 0.0: 64, 0.01: 65, 0.02: 66, 0.03: 67, 0.04: 68, 0.05: 69, 0.06: 70, 0.07: 71, 0.08: 72, 0.09: 73, 0.1: 74, 0.11: 75, 0.12: 76, 0.13: 77, 0.14: 78, 0.15: 79, 0.16: 80, 0.17: 81, 0.18: 82, 0.19: 83, 0.2: 84, 0.21: 85, 0.22: 86, 0.23: 87, 0.24: 88, 0.25: 89, 0.26: 90, 0.27: 91, 0.28: 92, 0.29: 93, 0.3: 94, 0.31: 95, 0.32: 96, 0.33: 97, 0.34: 98, 0.35: 99, 0.36: 100, 0.37: 101, 0.38: 102, 0.39: 103, 0.4: 104, 0.41: 105, 0.42: 106, 0.43: 107, 0.44: 108, 0.45: 109, 0.46: 110, 0.47: 111, 0.48: 112, 0.49: 113, 0.5: 114, 0.51: 115, 0.52: 116, 0.53: 117, 0.54: 118, 0.55: 119, 0.56: 120, 0.57: 121, 0.58: 122, 0.59: 123, 0.6: 124, 0.61: 125, 0.62: 126, 0.63: 127, 0.64: 128, 0.65: 129, 0.66: 130, 0.67: 131, 0.68: 132, 0.69: 133, 0.7: 134, 0.71: 135, 0.72: 136, 0.73: 137, 0.74: 138, 0.75: 139, 0.76: 140, 0.77: 141, 0.78: 142, 0.79: 143, 0.8: 144, 0.81: 145, 0.82: 146, 0.83: 147, 0.85: 148, 0.87: 149, 0.88: 150, 0.9: 151, 0.91: 152, 1684325040.0: 153, 0.84: 154, -0.73: 155}, {531012540.0: 0, 1483731064.0: 1, 1548018671.0: 2}, {1.0: 0, 2.0: 1, 3.0: 2, 4.0: 3, 5.0: 4, 6.0: 5, 7.0: 6, 8.0: 7, 9.0: 8, 10.0: 9, 1684325040.0: 10}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 6.0: 6, 7.0: 7, 8.0: 8, 9.0: 9, 10.0: 10, 12.0: 11, 13.0: 12, 14.0: 13, 15.0: 14, 18.0: 15, 19.0: 16, 20.0: 17, 1684325040.0: 18}, {0.0: 0, 0.5: 1, 1.0: 2, 1.5: 3, 2.0: 4, 2.5: 5, 3.0: 6, 3.4: 7, 4.0: 8, 5.0: 9, 6.0: 10, 7.0: 11, 8.0: 12, 9.0: 13, 10.0: 14, 12.0: 15, 18.0: 16, 1684325040.0: 17}, {2045239039.0: 0, 3796330715.0: 1, 3887069803.0: 2}, {1838306074.0: 0, 2834197676.0: 1, 3714257777.0: 2}, {2985647597.0: 0, 3422422935.0: 1, 3959514308.0: 2}, {0.0: 0, 1.0: 1, 2.0: 2, 3.0: 3, 4.0: 4, 5.0: 5, 5.5: 6, 6.0: 7, 6.5: 8, 7.0: 9, 7.5: 10, 8.0: 11, 8.5: 12, 9.0: 13, 9.5: 14, 10.0: 15, 1684325040.0: 16, 4.5: 17, 9.7: 18}, {0.0: 0, 1.0: 1, 1.5: 2, 2.0: 3, 3.0: 4, 4.0: 5, 5.0: 6, 5.5: 7, 6.0: 8, 6.5: 9, 7.0: 10, 7.5: 11, 8.0: 12, 8.5: 13, 9.0: 14, 10.0: 15, 1684325040.0: 16, 4.5: 17, 3.5: 18, 9.5: 19}, {1852959419.0: 0, 3336003742.0: 1, 4273261354.0: 2}, {2045239039.0: 0, 3796330715.0: 1, 3887069803.0: 2}, {0.0: 0, 1.0: 1, 3.0: 2, 6.0: 3, 7.0: 4, 8.0: 5, 1684325040.0: 6, 5.0: 7}]
list_of_cols_to_normalize = [2, 3, 4, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119]

transform_true = False

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values()))+1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize,mappings):
            if i>=data_arr.shape[1]:
                break
            col = data_arr[:,i]
            normcol = column_norm(col,mapping)
            data_arr[:,i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([])
        components = np.array([])
        whiten = None
        explained_variance = np.array([])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist=[]
    clean.testfile=testfile
    clean.mapping={}
    

    def convert(cell):
        value=str(cell)
        try:
            result=int(value)
            return result
        except:
            try:
                result=float(value)
                if (rounding!=-1):
                    result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
                return result
            except:
                result=(binascii.crc32(value.encode('utf8')) % (1<<32))
                return result

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value=str(cell)
        if (value==''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping=={}):
            result=-1
            try:
                result=clean.mapping[cell]
            except:
                raise ValueError("Class label '"+value+"' encountered in input not defined in user-provided mapping.")
            if (not result==int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist=clean.classlist+[str(result)]
            return result
        try:
            result=float(cell)
            if (rounding!=-1):
                result=int(result*math.pow(10,rounding))/math.pow(10,rounding)
            else:
                result=int(int(result*100)/100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist=clean.classlist+[str(result)]
        except:
            result=(binascii.crc32(value.encode('utf8')) % (1<<32))
            if (result in clean.classlist):
                result=clean.classlist.index(result)
            else:
                clean.classlist=clean.classlist+[result]
                result=clean.classlist.index(result)
            if (not result==int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result<0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount=0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f=open(outfile,"w+")
        if (headerless==False):
            next(reader,None)
        outbuf=[]
        for row in reader:
            if (row==[]):  # Skip empty rows
                continue
            rowcount=rowcount+1
            rowlen=num_attr
            if (not testfile):
                rowlen=rowlen+1    
            if (not len(row)==rowlen):
                raise ValueError("Column count must match trained predictor. Row "+str(rowcount)+" differs.")
            i=0
            for elem in row:
                if(i+1<len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid=str(convertclassid(elem))
                    outbuf.append(classid)
                i=i+1
            if (len(outbuf)<IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf=[]
        print(''.join(outbuf),end="", file=f)
        f.close()

        if (testfile==False and not len(clean.classlist)>=2):
            raise ValueError("Number of classes must be at least 2.")



# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)

# Classifier
def classify(row):
    x=row
    o=[0]*num_output_logits
    h_0 = max((((0.41531226 * float(x[0]))+ (-0.06345125 * float(x[1]))+ (0.089564666 * float(x[2]))+ (-0.2884905 * float(x[3]))+ (0.067807995 * float(x[4]))+ (-0.21502282 * float(x[5]))+ (0.753413 * float(x[6]))+ (0.91246074 * float(x[7]))+ (-0.23331866 * float(x[8]))+ (0.5834501 * float(x[9]))+ (0.012792094 * float(x[10]))+ (0.10635952 * float(x[11]))+ (0.8361268 * float(x[12]))+ (-0.87319607 * float(x[13]))+ (-1.6670439 * float(x[14]))+ (-1.3979031 * float(x[15]))+ (0.3478347 * float(x[16]))+ (0.4349746 * float(x[17]))+ (0.0039852923 * float(x[18]))+ (0.24548179 * float(x[19]))+ (-0.52723134 * float(x[20]))+ (-0.09230945 * float(x[21]))+ (0.5457902 * float(x[22]))+ (-0.7787193 * float(x[23]))+ (0.24970908 * float(x[24]))+ (-0.7285616 * float(x[25]))+ (0.8592049 * float(x[26]))+ (-0.12102618 * float(x[27]))+ (-0.30607274 * float(x[28]))+ (-0.63662004 * float(x[29]))+ (0.44300202 * float(x[30]))+ (-0.16363682 * float(x[31]))+ (0.061535455 * float(x[32]))+ (-0.9628238 * float(x[33]))+ (0.23527099 * float(x[34]))+ (0.22419144 * float(x[35]))+ (0.23386799 * float(x[36]))+ (0.85776657 * float(x[37]))+ (0.33350766 * float(x[38]))+ (-0.48189133 * float(x[39]))+ (-0.955399 * float(x[40]))+ (-0.17524718 * float(x[41]))+ (-1.6161928 * float(x[42]))+ (0.012901866 * float(x[43]))+ (-0.779634 * float(x[44]))+ (-0.59450305 * float(x[45]))+ (-0.771877 * float(x[46]))+ (-0.39927626 * float(x[47]))+ (-0.30271143 * float(x[48]))+ (0.12532707 * float(x[49])))+ ((-0.15272823 * float(x[50]))+ (0.8865505 * float(x[51]))+ (-0.901174 * float(x[52]))+ (-0.6871067 * float(x[53]))+ (-0.7677799 * float(x[54]))+ (0.23068254 * float(x[55]))+ (-0.49341682 * float(x[56]))+ (-0.082243256 * float(x[57]))+ (-0.5260136 * float(x[58]))+ (-0.69712734 * float(x[59]))+ (-0.7792497 * float(x[60]))+ (0.1611876 * float(x[61]))+ (-0.8449727 * float(x[62]))+ (-0.8181694 * float(x[63]))+ (-0.41402125 * float(x[64]))+ (0.49091828 * float(x[65]))+ (-0.8831468 * float(x[66]))+ (0.67548645 * float(x[67]))+ (-0.80820656 * float(x[68]))+ (0.93765074 * float(x[69]))+ (-0.06310098 * float(x[70]))+ (0.938254 * float(x[71]))+ (0.1795581 * float(x[72]))+ (0.41886628 * float(x[73]))+ (-1.0256779 * float(x[74]))+ (-0.4952571 * float(x[75]))+ (-0.87993705 * float(x[76]))+ (-0.5431162 * float(x[77]))+ (-0.8979412 * float(x[78]))+ (-0.4690956 * float(x[79]))+ (-0.18694389 * float(x[80]))+ (-0.93136585 * float(x[81]))+ (0.26441237 * float(x[82]))+ (0.08719672 * float(x[83]))+ (-0.6044159 * float(x[84]))+ (-0.073834024 * float(x[85]))+ (-0.93224746 * float(x[86]))+ (0.0464276 * float(x[87]))+ (0.81339294 * float(x[88]))+ (-0.49765363 * float(x[89]))+ (0.3046878 * float(x[90]))+ (-0.7368077 * float(x[91]))+ (0.40292484 * float(x[92]))+ (-0.4360526 * float(x[93]))+ (-0.6484821 * float(x[94]))+ (0.15816107 * float(x[95]))+ (-0.9601883 * float(x[96]))+ (0.6277471 * float(x[97]))+ (-1.020742 * float(x[98]))+ (0.3405666 * float(x[99])))+ ((-0.48971364 * float(x[100]))+ (0.45552325 * float(x[101]))+ (0.9095123 * float(x[102]))+ (-0.50249374 * float(x[103]))+ (0.15231466 * float(x[104]))+ (0.1539509 * float(x[105]))+ (0.12923563 * float(x[106]))+ (-1.4722873 * float(x[107]))+ (0.8753651 * float(x[108]))+ (-0.16621687 * float(x[109]))+ (0.42162067 * float(x[110]))+ (0.36580017 * float(x[111]))+ (-0.42019257 * float(x[112]))+ (0.61252916 * float(x[113]))+ (-0.20698851 * float(x[114]))+ (0.5954667 * float(x[115]))+ (0.010670806 * float(x[116]))+ (0.76306736 * float(x[117]))+ (0.3846598 * float(x[118]))+ (0.4492984 * float(x[119]))+ (-0.012216026 * float(x[120]))+ (0.91216725 * float(x[121]))) + 0.082560524), 0)
    h_1 = max((((-1.8951111 * float(x[0]))+ (0.055343434 * float(x[1]))+ (-1.6557527 * float(x[2]))+ (-0.06034993 * float(x[3]))+ (-0.09505524 * float(x[4]))+ (0.09894514 * float(x[5]))+ (-0.11189146 * float(x[6]))+ (-0.055976212 * float(x[7]))+ (-0.007890745 * float(x[8]))+ (0.021943998 * float(x[9]))+ (-0.22349979 * float(x[10]))+ (0.055776507 * float(x[11]))+ (-0.7269093 * float(x[12]))+ (-0.20520772 * float(x[13]))+ (-0.00017498947 * float(x[14]))+ (-0.036236055 * float(x[15]))+ (-0.006479078 * float(x[16]))+ (-0.010514802 * float(x[17]))+ (-0.020289883 * float(x[18]))+ (-0.0068591926 * float(x[19]))+ (-0.02920258 * float(x[20]))+ (0.18413287 * float(x[21]))+ (0.7385104 * float(x[22]))+ (-0.21332845 * float(x[23]))+ (0.17427209 * float(x[24]))+ (-1.4716815 * float(x[25]))+ (-0.10126124 * float(x[26]))+ (0.24090126 * float(x[27]))+ (-0.24144305 * float(x[28]))+ (0.116276704 * float(x[29]))+ (0.15860465 * float(x[30]))+ (-0.273726 * float(x[31]))+ (0.093566105 * float(x[32]))+ (-0.30765954 * float(x[33]))+ (0.25567842 * float(x[34]))+ (-0.7845534 * float(x[35]))+ (0.1557436 * float(x[36]))+ (0.15550786 * float(x[37]))+ (-0.29825982 * float(x[38]))+ (0.014136512 * float(x[39]))+ (-9.371718e-05 * float(x[40]))+ (0.0016779578 * float(x[41]))+ (0.014100381 * float(x[42]))+ (-0.0052346997 * float(x[43]))+ (0.007268387 * float(x[44]))+ (0.2728791 * float(x[45]))+ (-0.42585787 * float(x[46]))+ (-0.5562056 * float(x[47]))+ (0.26541612 * float(x[48]))+ (0.1398242 * float(x[49])))+ ((-0.08620385 * float(x[50]))+ (0.03740371 * float(x[51]))+ (0.3536182 * float(x[52]))+ (-0.17121331 * float(x[53]))+ (-0.084548175 * float(x[54]))+ (0.32565904 * float(x[55]))+ (1.4185846 * float(x[56]))+ (0.34855616 * float(x[57]))+ (-0.9182153 * float(x[58]))+ (-0.7828885 * float(x[59]))+ (-0.015376858 * float(x[60]))+ (0.15454915 * float(x[61]))+ (-0.5196783 * float(x[62]))+ (0.16561681 * float(x[63]))+ (0.12577471 * float(x[64]))+ (-0.36472058 * float(x[65]))+ (0.10811675 * float(x[66]))+ (0.090959474 * float(x[67]))+ (0.48114365 * float(x[68]))+ (-0.24459666 * float(x[69]))+ (0.18014415 * float(x[70]))+ (0.20727585 * float(x[71]))+ (-0.12029401 * float(x[72]))+ (-0.045068286 * float(x[73]))+ (-0.19870609 * float(x[74]))+ (-0.10884603 * float(x[75]))+ (0.09773733 * float(x[76]))+ (-0.29865825 * float(x[77]))+ (0.23011096 * float(x[78]))+ (-0.06720265 * float(x[79]))+ (-0.1678642 * float(x[80]))+ (0.10267417 * float(x[81]))+ (0.0001888267 * float(x[82]))+ (-0.08000757 * float(x[83]))+ (-0.34212252 * float(x[84]))+ (-0.074705586 * float(x[85]))+ (-0.037370715 * float(x[86]))+ (0.027700465 * float(x[87]))+ (-0.2744113 * float(x[88]))+ (-0.13148633 * float(x[89]))+ (-0.024931215 * float(x[90]))+ (-0.70510423 * float(x[91]))+ (0.07103287 * float(x[92]))+ (0.48369446 * float(x[93]))+ (-0.34663418 * float(x[94]))+ (0.17286292 * float(x[95]))+ (0.07610625 * float(x[96]))+ (-0.7033639 * float(x[97]))+ (0.1744522 * float(x[98]))+ (0.06692304 * float(x[99])))+ ((-0.14391358 * float(x[100]))+ (-0.62741446 * float(x[101]))+ (0.34568182 * float(x[102]))+ (0.028407648 * float(x[103]))+ (-0.25491178 * float(x[104]))+ (-0.7695282 * float(x[105]))+ (-0.593149 * float(x[106]))+ (0.033371985 * float(x[107]))+ (1.538957 * float(x[108]))+ (-0.025070328 * float(x[109]))+ (-0.024378529 * float(x[110]))+ (0.07438374 * float(x[111]))+ (-0.045021515 * float(x[112]))+ (-1.4409268 * float(x[113]))+ (0.32979864 * float(x[114]))+ (0.21753807 * float(x[115]))+ (-0.07769908 * float(x[116]))+ (-1.4680188 * float(x[117]))+ (-0.81470203 * float(x[118]))+ (0.10023658 * float(x[119]))+ (16.130268 * float(x[120]))+ (16.56034 * float(x[121]))) + -1.4819185), 0)
    o[0] = (-0.7119313 * h_0)+ (0.6564243 * h_1) + -6.195613

    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)

# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()
    
    if not args.validate: # Then predict
        if not args.cleanfile: # File is not preprocessed
            tempdir=tempfile.gettempdir()
            cleanfile=tempdir+os.sep+"clean.csv"
            clean(args.csvfile,cleanfile, -1, args.headerless, True)
            test_tensor = np.loadtxt(cleanfile,delimiter=',',dtype='float64')
            os.remove(cleanfile)
        else: # File is already preprocessed
            test_tensor = np.loadtxt(args.File,delimiter = ',',dtype = 'float64')               
        test_tensor = Normalize(test_tensor)
        if transform_true:
            test_tensor = transform(test_tensor)
        with open(args.csvfile,'r') as csvinput:
            writer = csv.writer(sys.stdout, lineterminator='\n')
            reader = csv.reader(csvinput)
            if (not args.headerless):
                writer.writerow((next(reader, None)+['Prediction']))
            i=0
            for row in reader:
                if (classify(test_tensor[i])):
                    pred="1"
                else:
                    pred="0"
                row.append(pred)
                writer.writerow(row)
                i=i+1
    elif args.validate: # Then validate this predictor, always clean first.
        if n_classes==2:
            tempdir=tempfile.gettempdir()
            temp_name = next(tempfile._get_candidate_names())
            cleanfile=tempdir+os.sep+temp_name
            clean(args.csvfile,cleanfile, -1, args.headerless)
            val_tensor = np.loadtxt(cleanfile,delimiter = ',',dtype = 'float64')
            os.remove(cleanfile)
            val_tensor = Normalize(val_tensor)
            if transform_true:
                trans = transform(val_tensor[:,:-1])
                val_tensor = np.concatenate((trans,val_tensor[:,-1].reshape(-1,1)),axis = 1)
            count,correct_count,num_TP,num_TN,num_FP,num_FN,num_class_1,num_class_0 = 0,0,0,0,0,0,0,0
            for i,row in enumerate(val_tensor):
                if int(classify(val_tensor[i].tolist())) == int(float(val_tensor[i,-1])):
                    correct_count+=1
                    if int(float(row[-1]))==1:
                        num_class_1+=1
                        num_TP+=1
                    else:
                        num_class_0+=1
                        num_TN+=1
                else:
                    if int(float(row[-1]))==1:
                        num_class_1+=1
                        num_FN+=1
                    else:
                        num_class_0+=1
                        num_FP+=1
                count+=1
        else:
            tempdir=tempfile.gettempdir()
            temp_name = next(tempfile._get_candidate_names())
            cleanvalfile=tempdir+os.sep+temp_name
            clean(args.csvfile,cleanvalfile, -1, args.headerless)
            val_tensor = np.loadtxt(cleanfile,delimiter = ',',dtype = 'float64')
            os.remove(cleanfile)
            val_tensor = Normalize(val_tensor)
            if transform_true:
                trans = transform(val_tensor[:,:-1])
                val_tensor = np.concatenate((trans,val_tensor[:,-1].reshape(-1,1)),axis = 1)
            numeachclass={}
            count,correct_count = 0,0
            for i,row in enumerate(val_tensor):
                if int(classify(val_tensor[i].tolist())) == int(float(val_tensor[i,-1])):
                    correct_count+=1
                    if int(float(val_tensor[i,-1])) in numeachclass.keys():
                        numeachclass[int(float(val_tensor[i,-1]))]+=1
                    else:
                        numeachclass[int(float(val_tensor[i,-1]))]=0
                count+=1

        model_cap=249

        if n_classes==2:

            FN=float(num_FN)*100.0/float(count)
            FP=float(num_FP)*100.0/float(count)
            TN=float(num_TN)*100.0/float(count)
            TP=float(num_TP)*100.0/float(count)
            num_correct=correct_count

            if int(num_TP+num_FN)!=0:
                TPR=num_TP/(num_TP+num_FN) # Sensitivity, Recall
            if int(num_TN+num_FP)!=0:
                TNR=num_TN/(num_TN+num_FP) # Specificity, 
            if int(num_TP+num_FP)!=0:
                PPV=num_TP/(num_TP+num_FP) # Recall
            if int(num_FN+num_TP)!=0:
                FNR=num_FN/(num_FN+num_TP) # Miss rate
            if int(2*num_TP+num_FP+num_FN)!=0:
                FONE=2*num_TP/(2*num_TP+num_FP+num_FN) # F1 Score
            if int(num_TP+num_FN+num_FP)!=0:
                TS=num_TP/(num_TP+num_FN+num_FP) # Critical Success Index

            randguess=int(float(10000.0*max(num_class_1,num_class_0))/count)/100.0
            modelacc=int(float(num_correct*10000)/count)/100.0

            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc)+" ("+str(int(num_correct))+"/"+str(count)+" correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc-randguess)+" (of possible "+str(round(100-randguess,2))+"%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct*100)/model_cap)/100.0)+" bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100*(modelacc-randguess)/model_cap)/100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN)+" ("+str(int(num_TN))+"/"+str(count)+")")
            print("True Positives:                     {:.2f}%".format(TP)+" ("+str(int(num_TP))+"/"+str(count)+")")
            print("False Negatives:                    {:.2f}%".format(FN)+" ("+str(int(num_FN))+"/"+str(count)+")")
            print("False Positives:                    {:.2f}%".format(FP)+" ("+str(int(num_FP))+"/"+str(count)+")")
            if int(num_TP+num_FN)!=0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN+num_FP)!=0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP+num_FP)!=0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2*num_TP+num_FP+num_FN)!=0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP+num_FN)!=0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP+num_FN+num_FP)!=0:    
                print("Critical Success Index:             {:.2f}".format(TS))
        else:
            num_correct=correct_count
            modelacc=int(float(num_correct*10000)/count)/100.0
            randguess=round(max(numeachclass.values())/sum(numeachclass.values())*100,2)
            print("System Type:                        "+str(n_classes)+"-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc)+" ("+str(int(num_correct))+"/"+str(count)+" correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc-randguess)+" (of possible "+str(round(100-randguess,2))+"%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct*100)/model_cap)/100.0)+" bits/bit")






