#!/usr/bin/env python3
#
# This code is was produced by an alpha version of Brainome Daimensions(tm) and is 
# licensed under GNU GPL v2.0 or higher. For details, please see: 
# https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
#
#
# Output of Brainome Daimensions(tm) 0.93 Table Compiler v0.94.
# Invocation: btc https://www.openml.org/data/get_csv/13153954/speeddating.arff -o Predictors/SpeedDating_QC.py -target match -stopat 87.1 -f QC -e 100 --yes
# Total compiler execution time: 0:02:28.73. Finished on: Apr-22-2020 01:51:51.
# This source code requires Python 3.
#
"""
System Type:                        Binary classifier
Best-guess accuracy:                83.52%
Model accuracy:                     87.21% (7307/8378 correct)
Improvement over best guess:        3.69% (of possible 16.48%)
Model capacity (MEC):               1155 bits
Generalization ratio:               6.32 bits/bit
Model efficiency:                   0.00%/parameter
System behavior
True Negatives:                     77.57% (6499/8378)
True Positives:                     9.64% (808/8378)
False Negatives:                    6.83% (572/8378)
False Positives:                    5.96% (499/8378)
True Pos. Rate/Sensitivity/Recall:  0.59
True Neg. Rate/Specificity:         0.93
Precision:                          0.62
F-1 Measure:                        0.60
False Negative Rate/Miss Rate:      0.41
Critical Success Index:             0.43

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

from bisect import bisect_left
# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "speeddating.csv"


#Number of attributes
num_attr = 122
n_classes = 2


# Preprocessor for CSV files
def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    il=[]
    
    ignorelabels=[]
    ignorecolumns=[]
    target="match"


    if (testfile):
        target=''
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless==False):
                header=next(reader, None)
                try:
                    if (target!=''): 
                        hc=header.index(target)
                    else:
                        hc=len(header)-1
                        target=header[hc]
                except:
                    raise NameError("Target '"+target+"' not found! Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=header.index(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute '"+ignorecolumns[i]+"' is the target. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '"+ignorecolumns[i]+"' not found in header. Header must be same as in file passed to btc.")
                for i in range(0,len(header)):      
                    if (i==hc):
                        continue
                    if (i in il):
                        continue
                    print(header[i]+",", end = '', file=outputfile)
                print(header[hc],file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if (row[target] in ignorelabels):
                        continue
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name==target):
                            continue
                        if (',' in row[name]):
                            print ('"'+row[name]+'"'+",",end = '', file=outputfile)
                        else:
                            print (row[name]+",",end = '', file=outputfile)
                    print (row[target], file=outputfile)

            else:
                try:
                    if (target!=""): 
                        hc=int(target)
                    else:
                        hc=-1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0,len(ignorecolumns)):
                    try:
                        col=int(ignorecolumns[i])
                        if (col==hc):
                            raise ValueError("Attribute "+str(col)+" is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    if (hc==-1):
                        hc=len(row)-1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0,len(row)):
                        if (i in il):
                            continue
                        if (i==hc):
                            continue
                        if (',' in row[i]):
                            print ('"'+row[i]+'"'+",",end = '', file=outputfile)
                        else:
                            print(row[i]+",",end = '', file=outputfile)
                    print (row[hc], file=outputfile)

def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    # function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping

# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([118655981268.72, 123135666454.9, 123558353486.39499, 124489434133.035, 125401803164.94, 128709421613.08499, 128859291958.685, 128899501572.81, 129080867052.38, 130782333544.54001, 131275213863.875, 132001832730.36, 132083311252.045, 132535696935.34, 132570893593.485, 132603755766.73999, 132617406429.025, 133057832103.38, 133090966936.625, 133145184359.23001, 133191188886.66, 133601544206.39, 133771619281.09, 133860512798.27501, 134053948493.36, 134420771657.99, 134649800653.14499, 134898784901.215, 134979376048.0, 135660086148.725, 135827576807.965, 136071659490.23, 136122847361.545, 136170704154.85, 136233372785.13501, 136509454761.405, 136701151817.36499, 136848669192.66, 136874059940.1, 138204388877.46503, 138252369210.345, 138313244434.425, 138348701441.06, 138351479653.635, 138383800849.22498, 138446692384.985, 138549079535.77502, 138770806711.01, 138791371144.155, 138817735889.22, 138865923947.78998, 138897995525.22998, 139012545338.2, 139173123922.99, 139196222918.495, 139333845055.54498, 139351122250.91998, 139382287340.26, 139407800288.88, 139462955330.855, 139478426262.41, 139581368275.53, 139592076003.02502, 139690023684.505, 139904793866.53, 140204002841.76, 140233626157.52002, 140268845767.37, 140297509975.605, 140449683100.24, 140483407023.255, 140495154871.68, 140508903145.11, 140520204344.0, 140558514132.85498, 140717101706.635, 140719643758.3, 140837026349.225, 140852583427.075, 140907751088.68, 140932748256.88, 140993747628.27, 140994693765.41498, 141091502007.45502, 141129857853.625, 141137741687.87, 141170815458.77502, 141267548720.845, 141353825424.13, 141474620737.09998, 141509186059.595, 141745335591.86, 141761181080.37, 141775333666.315, 141784540995.09003, 141862952819.355, 141886807116.09003, 141900277036.47, 141908471664.26498, 141924358778.825, 141956365511.13, 142004088812.76, 142064671313.685, 142166423329.745, 142175295345.47498, 142257948789.76, 142337622503.795, 142373950776.23, 142417603845.095, 142436053370.97998, 142459658344.77002, 142545062693.36, 142546340787.95, 142557689859.815, 142569735799.375, 142798244442.665, 142810408034.54, 142815730326.13, 142846796625.93, 142886562451.375, 142910987678.09, 143102874074.03, 143135683564.96503, 143413024545.51498, 143429020254.16998, 143543484721.40503, 143576249495.14, 143779112761.56, 143816852976.97, 143851594930.27, 143903922759.195, 144004649205.29, 144010035666.61002, 144059189745.97, 144072247698.66498, 144202241975.155, 144210788886.775, 144295073886.045, 144337359906.81, 144455467373.195, 144493301178.19, 144519255084.60498, 144536947856.22, 144554407215.91498, 144562390059.375, 144567535624.555, 144600580053.60498, 144607607978.495, 144636904172.715, 144730941310.075, 144766201735.985, 145039049892.545, 145051064075.15, 145057111504.34, 145061537067.63, 145102362677.985, 145122370433.39, 145127124342.075, 145128815965.785, 145167172621.36, 145174867843.84, 145205821871.59, 145214960589.72, 145217498729.48502, 145240486746.805, 145314474898.60498, 145335718297.745, 145421242559.62, 145425796647.135, 145434593687.99, 145441828725.05, 145481841018.69, 145489607175.69, 145494983267.815, 145501496397.53998, 145525088145.25, 145559897593.54498, 145580486275.505, 145584214644.575, 145592861520.19, 145616230791.135, 145641529026.86, 145657465640.71002, 145702605816.05, 145717652216.77502, 145765099180.36, 145802549760.475, 145845079884.49, 145863558981.62, 145875172127.315, 145923021563.02502, 145959288072.45, 145973116122.47998, 145998953411.83502, 146016057109.11002, 146030043733.405, 146066286134.9, 146090411366.41, 146099239374.6, 146196784864.685, 146205518561.565, 146221564873.255, 146224344272.83002, 146494890483.43, 146522209586.18, 146566981330.515, 146604426484.84003, 146724385074.29, 146731829611.515, 146818735778.255, 146828734455.62, 146835861413.83002, 146836610508.54498, 146849335885.595, 146863971198.93, 146904995355.535, 146914572261.765, 146930145999.07, 146938344219.075, 146973992894.705, 147005560179.345, 147045188683.14, 147073279644.02002, 147081696220.215, 147097780047.95, 147170485503.37, 147196429135.18, 147276771972.915, 147353917594.81, 147420836592.19, 147431986423.6, 147471276108.385, 147497488287.72998, 147562750994.49, 147566108734.24, 147571648217.805, 147580785814.20502, 147671934422.43, 147684468685.6, 147746461717.1, 147754034468.285, 147878173172.10498, 147885153881.91498, 147956680015.65997, 147969006237.715, 147993027300.305, 148000840625.615, 148007672221.18, 148018815058.685, 148068880133.58002, 148070732577.125, 148084696513.325, 148114382463.42, 148139640881.135, 148176466045.89, 148186819287.41998, 148193720972.755, 148222172842.52, 148236902803.135, 148258592895.83502, 148314324993.035, 148400069259.315, 148421447721.68, 148433701477.625, 148435190977.02002, 148544363251.20502, 148559440926.445, 148609033403.47998, 148623570682.45502, 148626941692.04498, 148627835988.775, 148638580955.4, 148667270715.88, 148971419238.97, 148975624904.78, 148993945963.715, 149003134850.905, 149033301455.04498, 149041680300.245, 149079092356.18, 149106129286.53998, 149152030828.805, 149172692196.97998, 149288519724.8, 149294685319.975, 149363203768.87, 149375835814.675, 149408589842.31, 149442158193.805, 149513106521.53998, 149521476578.43, 149734972598.69, 149750973165.72, 149777571293.72998, 149785402996.46002, 149846355749.755, 149879254465.35498, 149962384664.615, 149965551153.23502, 150038765681.53, 150046461579.32, 150083390780.83002, 150084486347.885, 150091517328.59003, 150109873368.78998, 150172528098.195, 150175006404.485, 150191569211.26498, 150202345935.76498, 150251660960.765, 150255575732.035, 150305290196.28003, 150311017001.66998, 150429256396.16, 150438353181.77002, 150478973461.22498, 150483951300.09998, 150516845662.175, 150519269146.565, 150573966822.58502, 150585619656.97, 150593350116.865, 150600284489.9, 150623949052.24, 150637739099.755, 150686584317.17, 150689418029.45, 150698940242.3, 150704277956.32498, 150837004208.4, 150848125238.935, 150854950309.99, 150867728868.29498, 150909960289.20502, 150920838056.34003, 150950170225.22, 150953250001.285, 151114765444.83002, 151125912549.96, 151156243590.72498, 151157945117.15002, 151166780223.69, 151176300424.07, 151187246388.95502, 151191288887.38, 151222186066.305, 151237697658.49002, 151249250060.63, 151255565316.435, 151434646596.375, 151458264055.64, 151483097648.71, 151489178918.51498, 151497598271.895, 151510061339.655, 151520423650.62, 151523458406.675, 151541168361.955, 151572403362.91498, 151585874370.845, 151591198037.66, 151785920854.305, 151796488136.495, 151874370776.88, 151883931095.92, 151899225973.48, 151915124634.84, 151965871856.095, 151992313604.255, 152032209380.51, 152038061619.275, 152139807404.85, 152147747502.065, 152205237000.36, 152212373110.375, 152224753647.995, 152231361482.805, 152245922176.74, 152259849584.635, 152268946439.63998, 152271796312.47498, 152277780711.29, 152286088144.385, 152324802283.99, 152341151453.40997, 152351439068.185, 152354987774.995, 152404606098.065, 152410799647.13, 152482117560.575, 152491486745.64, 152582536650.27, 152591885435.35498, 152595618002.035, 152599525338.115, 152602872011.65, 152606193905.225, 152624816329.445, 152646163402.265, 152674874551.015, 152689434354.46002, 152705470848.41998, 152709015008.19, 152773075579.16998, 152788368437.62, 152832468698.72, 152844007385.63, 152848319093.87, 152856265799.14, 152865742167.535, 152891026164.515, 152935759558.78497, 152959485263.74, 152985348483.77002, 152997856288.08502, 153051510321.6, 153072102441.265, 153138765178.695, 153142139803.02002, 153158941494.225, 153163066660.955, 153204940879.425, 153206273237.785, 153239104901.235, 153248757436.32, 153262972245.13, 153270465856.375, 153273894000.975, 153277015556.84003, 153289286537.76, 153296162872.09497, 153304362905.445, 153321057145.115, 153362188068.935, 153372918840.125, 153428695568.615, 153445800224.66003, 153458496533.495, 153464333782.49, 153579917902.61, 153581635519.22498, 153593236128.06, 153599388607.8, 153614092057.72998, 153619099973.45, 153642122037.09003, 153649680734.045, 153698532360.235, 153705368152.305, 153723769994.525, 153735336980.83502, 153786788642.115, 153793457613.74, 153884728458.08002, 153894183208.035, 153946544109.40997, 153954408101.63, 153960992017.65, 153964137486.40002, 154099448945.805, 154101998703.495, 154197200050.52502, 154210896336.65002, 154290768254.835, 154308961607.175, 154372025329.685, 154380402792.95, 154423165083.27502, 154431592477.165, 154440179349.125, 154442906459.125, 154449226819.005, 154458717632.39, 154475007046.21, 154484136480.275, 154500662662.36, 154520094063.94, 154535447370.02002, 154540890603.495, 154559987532.485, 154566380531.735, 154590898177.62, 154618432045.93, 154671690920.685, 154699986858.34998, 154719154063.215, 154726201684.94, 154794137270.55, 154801568894.155, 155019796965.61, 155023701025.615, 155122178883.07, 155139361792.81, 155147796053.235, 155153671652.385, 155197720399.89, 155207938873.27002, 155222825060.34998, 155230212150.975, 155236025271.505, 155240248688.665, 155265448457.175, 155277390970.895, 155286418432.84497, 155309337088.675, 155332327461.91998, 155335544260.895, 155394650579.005, 155414019194.83002, 155450047155.43, 155470714749.66, 155474932825.265, 155481257457.635, 155527955150.40002, 155530519012.72, 155534313086.505, 155538648270.65, 155559403843.45, 155565147068.035, 155646495328.76, 155649172433.995, 155653168669.63, 155660501268.43, 155693025708.24, 155697135459.405, 155713384569.295, 155716170779.785, 155729391394.07, 155731599295.99, 155737498906.51, 155743076468.925, 155771832285.05, 155779170524.425, 155872787013.115, 155877429807.055, 155928824995.47, 155946805764.08502, 156144725621.84, 156147616931.81, 156244644690.695, 156247687733.95, 156261047664.59, 156271301579.435, 156285356591.2, 156292310933.94, 156302887126.035, 156356355943.6, 156377907945.75, 156396970466.935, 156432800859.535, 156439535557.915, 156453067020.13, 156461776865.01, 156473140843.57498, 156476945513.79498, 156481589039.615, 156487950151.885, 156517362087.195, 156541946466.365, 156592580883.46002, 156600311689.71, 156608011702.87, 156609876838.62, 156672763880.715, 156690315984.79, 156718247297.075, 156723502927.26, 156784073569.78, 156785872431.90002, 156805818172.485, 156813968908.86, 156818889554.315, 156832201107.445, 156914310376.44998, 156923445426.85498, 156932541052.085, 156951603786.56, 156986941314.62, 156988301530.04498, 156991295074.86, 156994707915.84, 157002423636.335, 157011587457.615, 157049060859.15, 157063739180.855, 157081020147.97998, 157087404234.72498, 157115293524.95, 157119793822.65002, 157125880297.66, 157141643643.995, 157176827141.85, 157181332596.845, 157248872946.37, 157256077823.435, 157374452669.73502, 157376146166.66, 157425536636.72, 157429918659.345, 157434111467.195, 157438805900.015, 157489692968.03, 157502482060.59998, 157604278445.33, 157606710064.435, 157638339767.41, 157648914862.625, 157655792164.58002, 157660962562.57, 157730423686.075, 157735349384.485, 157773135052.93, 157784201145.88, 157887688721.805, 157889736933.185, 158029661160.155, 158032044622.685, 158071843372.15, 158073716533.985, 158092577168.98, 158096912052.385, 158103922861.965, 158115640722.585, 158156428850.84003, 158158192626.04, 158164466439.225, 158171682407.64, 158223509869.875, 158227371633.245, 158240904604.16, 158242239007.245, 158248038094.025, 158255555814.49, 158333938810.36, 158342551444.25, 158426669879.85498, 158431025245.87, 158434363753.66, 158436173975.33502, 158440506860.795, 158450485273.72998, 158459032679.675, 158464939842.53, 158471592020.82, 158474734825.71, 158677293611.375, 158680161165.64, 158791690038.76, 158799870015.87, 158809379544.36, 158814483932.255, 158850359882.05, 158853703379.78998, 158907511716.39, 158909783036.57, 158991274530.5, 158992930784.255, 158994810009.765, 159008851557.175, 159115073238.59998, 159122245325.655, 159127721522.63, 159133377875.87, 159152339578.27002, 159153975984.01, 159189366616.36, 159192920411.84998, 159200643877.655, 159207124962.64, 159246398884.725, 159260151290.365, 159299218061.685, 159300949210.90002, 159305244918.86, 159320187797.84, 159369324230.035, 159388365143.365, 159474321093.27002, 159493260899.35498, 159611201696.41998, 159613592755.52, 159704186574.965, 159726545416.615, 159748576115.75, 159757249109.215, 159811957153.16998, 159816600524.25, 159823626188.25, 159826577511.88498, 159911962699.16498, 159917258064.99, 159918138654.51, 159918966190.27002, 159927919075.2, 159930128727.26, 159937520575.15, 159945601031.96, 159968974772.2, 159975464210.64502, 160039681287.485, 160049426709.91998, 160054923156.72, 160056578949.665, 160065478524.18, 160073947460.85498, 160109633054.945, 160126194590.93, 160128934400.35, 160137544441.13, 160245592807.03, 160253207445.35498, 160269126884.41998, 160275678922.8, 160284491672.37, 160286541937.41998, 160419800282.88, 160431632043.15997, 160443973758.86, 160445764861.63, 160451743333.235, 160452056939.18, 160476328847.235, 160488720262.695, 160518299742.81, 160521645979.85, 160524740229.76, 160527965948.595, 160594984707.44, 160602169341.185, 160615958953.27502, 160627685050.94, 160698791840.655, 160709727216.40997, 160805854321.195, 160806789382.46, 160814816114.23, 160822422702.735, 160830709000.88, 160841009302.78003, 160883872784.3, 160893559493.925, 161042285652.675, 161049476501.18, 161065125133.695, 161072238293.08002, 161129471682.26, 161134929022.39, 161158577588.56, 161172240813.26, 161203522182.325, 161209536748.925, 161232648377.47498, 161236428623.63, 161238875001.03, 161242933024.385, 161268031184.48502, 161277476640.215, 161290842447.885, 161302442825.78, 161313982927.66498, 161320530561.635, 161367366717.755, 161372391697.385, 161388577376.77502, 161391728781.135, 161401828958.495, 161405276890.185, 161419415970.08002, 161420258218.995, 161436480908.145, 161441260248.34998, 161451258931.945, 161459513207.055, 161637095635.71503, 161661178305.10498, 161872779565.975, 161887279536.985, 161913828091.6, 161937421911.995, 161952189189.3, 161962663966.085, 162016153521.62, 162020715050.845, 162034407806.66, 162047557080.555, 162098263993.195, 162124423590.58002, 162182535264.38, 162187113203.47998, 162218673863.185, 162233467229.515, 162281239435.655, 162290125118.82, 162309176740.45502, 162312627786.15002, 162321659957.83502, 162341261777.3, 162387520696.825, 162407589975.52002, 162492970209.38, 162496553485.43, 162532075666.175, 162534110620.53, 162540073332.95502, 162560632933.97, 162667539746.16, 162677073570.09, 162691281222.93, 162693833674.22998, 162714230365.765, 162722908396.075, 162740722728.605, 162753035563.81, 162764824537.64, 162770260701.99, 162884881715.95502, 162893934474.45502, 162922734315.275, 162925640902.21, 162972495007.19, 162984307381.16498, 163023929251.72998, 163057902000.70502, 163068760787.93, 163079609952.70502, 163106330740.72998, 163110576440.47, 163135541701.35, 163143480683.445, 163215440470.87, 163218757631.61002, 163301751371.12, 163321314011.78003, 163342011304.66998, 163348866731.85498, 163372451600.04498, 163385785217.66, 163393807224.475, 163397093122.71002, 163458391908.345, 163459282001.98, 163538045717.81, 163552887315.42, 163623686214.8, 163625762632.31, 163676538167.20502, 163688787907.84, 163717340979.66998, 163720062415.58002, 163834320447.005, 163863650937.78998, 164039447272.345, 164049274815.60498, 164062677446.49, 164071110968.66498, 164144451437.845, 164148490386.985, 164167679720.01, 164169920347.90002, 164175657107.14, 164188653901.145, 164203323283.165, 164211329758.715, 164245895634.16, 164253140615.47, 164444694565.175, 164478694153.89, 164539318432.16, 164543760684.375, 164624134618.945, 164634776142.59, 164637033703.59, 164639275894.68, 164666403113.97, 164674727132.70502, 164678917394.79498, 164696823483.195, 164744005456.885, 164746953633.08002, 164779295985.69, 164788129412.40997, 164831157003.55, 164841630280.735, 164885971082.08002, 164890350009.47003, 164947734160.04, 164950771284.5, 164996986226.53998, 165000023025.78, 165119297236.72498, 165135821520.535, 165198698073.775, 165201450868.37, 165208656818.95502, 165214299519.755, 165223537668.57, 165225704111.66998, 165231710581.74, 165239343654.315, 165243097649.02502, 165249972341.515, 165278288742.255, 165292406845.41998, 165360093340.84998, 165365364094.16498, 165407291232.22498, 165414108195.8, 165455481630.09998, 165458400013.565, 165568454406.615, 165577132912.565, 165665225480.87, 165676702573.755, 165739279148.58502, 165761622640.63, 166143354509.22998, 166149151547.35498, 166215267032.78497, 166223406727.16998, 166228595191.485, 166231319927.565, 166403811016.18, 166410113487.995, 166433846506.49, 166440322079.24, 166515628533.21002, 166521162709.86, 166554395286.29498, 166571150002.60498, 166586692752.91498, 166598036232.435, 166602656930.89502, 166623770517.02502, 166645663199.83502, 166653372696.04498, 166677564472.685, 166698249742.865, 166830126442.75, 166835560587.47, 166947311421.52002, 166951096515.48, 167200531060.79, 167207041149.685, 167272676524.11, 167279098080.89502, 167292594355.78998, 167304594719.445, 167445052064.41498, 167452633362.885, 167604521778.97998, 167607007400.68, 167665161868.78998, 167675626803.865, 167686502598.995, 167690038978.865, 167787040585.71503, 167797590335.605, 167825671075.44, 167843069278.93, 167898023935.495, 167905103959.055, 167938887643.025, 167941020045.555, 168011862934.2, 168020210585.495, 168029319629.705, 168034734883.005, 168163029818.73, 168179147814.47, 168191883212.61, 168195331404.385, 168206935774.7, 168210718173.53998, 168302453516.855, 168318370632.09, 168399021573.03998, 168400939042.22998, 168485254147.785, 168508005225.915, 168843783292.705, 168847793211.685, 168993388975.675, 168996960333.76, 169018993005.865, 169030275378.995, 169251570748.09, 169269843263.395, 169369760738.005, 169375382667.49, 169379359923.87, 169391589120.095, 169434607629.21002, 169440469014.14, 169687082479.73, 169708450748.51, 169836335262.21002, 169841127343.96503, 169896470860.22998, 169903754397.90497, 169949186094.185, 169952690012.685, 170101738251.945, 170115630138.125, 170276924622.66998, 170281030389.41998, 170299404599.685, 170302505189.07, 170305596143.525, 170311341813.19, 170341070934.285, 170344414756.66, 170467661609.735, 170489817860.19, 170525312767.75, 170531733995.99, 170616510908.405, 170625725420.5, 170713239973.085, 170728713921.20502, 170746996158.27002, 170764165467.325, 170806244023.70502, 170823334842.33502, 171038024181.91, 171058814567.045, 171181012797.75, 171206798196.41, 171230571348.735, 171254377122.025, 171316286757.785, 171318310381.84003, 171432462288.175, 171444677616.7, 171498285005.35498, 171500837083.84, 171721772284.88, 171727453343.61, 171904504591.41, 171913410284.155, 172021138324.465, 172046584785.06, 172123026258.405, 172144350076.59998, 172172243375.525, 172190153544.175, 172383543021.815, 172395793869.22498, 172536184141.66498, 172550524469.065, 172569699210.35498, 172579582866.03998, 172647454312.82, 172661612615.055, 172722034376.46503, 172723826848.79, 172740639979.41998, 172759189185.065, 172788581524.97998, 172793865609.10498, 172937997820.0, 172952334203.89, 172986357479.38, 172992003014.265, 173019135069.59, 173025819972.09, 173218516592.42, 173225282391.44, 173519723157.26, 173522553917.875, 173545243344.65, 173556333098.78497, 173674265724.29, 173678549470.84003, 173753468169.005, 173812690110.725, 173968316354.29498, 173968908300.525, 174551753973.115, 174565757819.31, 174739972672.88, 174741372452.58, 174762380975.685, 174775650251.43, 174901697591.16998, 174927775920.495, 174951298728.075, 174962709964.02, 175021104107.935, 175024685976.565, 175194899680.85, 175202342280.90002, 175542676517.485, 175566457151.635, 175723188423.34, 175772877142.37, 175828840923.49, 175851176434.58002, 176112633916.375, 176126377523.76, 176375367684.385, 176385498422.0, 176570448382.7, 176594739025.51, 176620121252.125, 176640273540.79, 176652869585.105, 176659772907.925, 177478789664.895, 177490471787.685, 178124359221.335, 178128949712.585, 178603485307.79, 178625870996.585, 179203719761.995, 179218589688.83002, 179760425485.79498, 179778244680.685, 181344753649.24, 181361010832.18, 182345212600.865, 182382000121.01, 183708291361.45502, 183738274431.32, 184977475687.99, 185051767804.065, 185145593015.295, 185310755661.12, 187061397250.25, 187073323612.615, 187910186549.065, 187920846792.44, 187927503412.66498, 187971144729.93, 188009019190.61502, 188014475244.83002, 188220113289.95, 188309910095.80002, 190913842796.63, 191066031481.96002, 191401279283.095, 191623911820.36, 192030615480.325, 192318547685.28, 193802629220.59, 193916556802.01498, 194039721896.93, 194098103119.96002, 230770932903.635, 234085731128.5, 240373997771.995, 241273667709.5, 244232127142.0, 244389011476.5, 247429318387.0, 248369421110.0])
def eqenergy(rows):
    return np.sum(rows, axis=1)
def classify(rows):
    energys = eqenergy(rows)
    start_label = 0
    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = (numers[indys] + start_label) % 2
        outputs[defaultindys]=1
        return outputs
    return thresh_search(energys)

numthresholds=1155



# Main method
model_cap = numthresholds


def Validate(file):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 0
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:,-1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    with open(preprocessedfile, 'r') as csvinput:
        dirtyreader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(dirtyreader, None) + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            print(str(','.join(str(j) for j in ([i for i in row]))) + ',' + str(get_key(int(outputs[k]), classmapping)))



#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()

    #clean if not already clean
    if not args.cleanfile:
        tempdir = tempfile.gettempdir()
        cleanfile = tempdir + os.sep + "clean.csv"
        preprocessedfile = tempdir + os.sep + "prep.csv"
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x,y: x
        classmapping = {}

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile, classmapping)


    else:
        
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
            print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
            print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
            print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
            if int(num_TP + num_FN) != 0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN + num_FP) != 0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP + num_FP) != 0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2 * num_TP + num_FP + num_FN) != 0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP + num_FN) != 0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP + num_FN + num_FP) != 0:
                print("Critical Success Index:             {:.2f}".format(TS))

        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            print("System Type:                        " + str(n_classes) + "-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")





            def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
                #check for numpy/scipy is imported
                try:
                    from scipy.sparse import coo_matrix #required for multiclass metrics
                    try:
                        np.array
                    except:
                        import numpy as np
                except:
                    raise ValueError("Scipy and Numpy Required for Multiclass Metrics")
                # Compute confusion matrix to evaluate the accuracy of a classification.
                # By definition a confusion matrix :math:C is such that :math:C_{i, j}
                # is equal to the number of observations known to be in group :math:i and
                # predicted to be in group :math:j.
                # Thus in binary classification, the count of true negatives is
                # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
                # :math:C_{1,1} and false positives is :math:C_{0,1}.
                # Read more in the :ref:User Guide <confusion_matrix>.
                # Parameters
                # ----------
                # y_true : array-like of shape (n_samples,)
                # Ground truth (correct) target values.
                # y_pred : array-like of shape (n_samples,)
                # Estimated targets as returned by a classifier.
                # labels : array-like of shape (n_classes), default=None
                # List of labels to index the matrix. This may be used to reorder
                # or select a subset of labels.
                # If None is given, those that appear at least once
                # in y_true or y_pred are used in sorted order.
                # sample_weight : array-like of shape (n_samples,), default=None
                # Sample weights.
                # normalize : {'true', 'pred', 'all'}, default=None
                # Normalizes confusion matrix over the true (rows), predicted (columns)
                # conditions or all the population. If None, confusion matrix will not be
                # normalized.
                # Returns
                # -------
                # C : ndarray of shape (n_classes, n_classes)
                # Confusion matrix.
                # References
                # ----------
                if labels is None:
                    labels = np.array(list(set(list(y_true.astype('int')))))
                else:
                    labels = np.asarray(labels)
                    if np.all([l not in y_true for l in labels]):
                        raise ValueError("At least one label specified must be in y_true")


                if sample_weight is None:
                    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
                else:
                    sample_weight = np.asarray(sample_weight)
                if y_true.shape[0]!=y_pred.shape[0]:
                    raise ValueError("y_true and y_pred must be of the same length")

                if normalize not in ['true', 'pred', 'all', None]:
                    raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


                n_labels = labels.size
                label_to_ind = {y: x for x, y in enumerate(labels)}
                # convert yt, yp into index
                y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
                y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
                # intersect y_pred, y_true with labels, eliminate items not in labels
                ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
                y_pred = y_pred[ind]
                y_true = y_true[ind]
                # also eliminate weights of eliminated items
                sample_weight = sample_weight[ind]
                # Choose the accumulator dtype to always have high precision
                if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                    dtype = np.int64
                else:
                    dtype = np.float64
                cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


                with np.errstate(all='ignore'):
                    if normalize == 'true':
                        cm = cm / cm.sum(axis=1, keepdims=True)
                    elif normalize == 'pred':
                        cm = cm / cm.sum(axis=0, keepdims=True)
                    elif normalize == 'all':
                        cm = cm / cm.sum()
                    cm = np.nan_to_num(cm)
                return cm


            print("Confusion Matrix:")
            mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])



    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        os.remove(preprocessedfile)


