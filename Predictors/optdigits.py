#!/usr/bin/env python3
#
# This code is was produced by an alpha version of Brainome Daimensions(tm) and is
# licensed under GNU GPL v2.0 or higher. For details, please see:
# https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
#
#
# Output of Brainome Daimensions(tm) 0.96 Table Compiler v0.96.
# Invocation: btc https://www.openml.org/data/get_csv/28/dataset_28_optdigits.arff -o Predictors/optdigits_NN.py -target class -stopat 98.4 -f NN -e 20 --yes --runlocalonly
# Total compiler execution time: 6:50:48.44. Finished on: May-30-2020 06:07:00.
# This source code requires Python 3.
#
"""
Classifier Type: Neural Network
System Type:                        10-way classifier
Best-guess accuracy:                10.22%
Model accuracy:                     97.31% (5469/5620 correct)
Improvement over best guess:        87.09% (of possible 89.78%)
Model capacity (MEC):               334 bits
Generalization ratio:               16.37 bits/bit
Confusion Matrix:
 [9.66% 0.00% 0.00% 0.02% 0.02% 0.00% 0.12% 0.00% 0.02% 0.02%]
 [0.00% 9.95% 0.02% 0.04% 0.05% 0.02% 0.04% 0.00% 0.05% 0.00%]
 [0.00% 0.05% 9.73% 0.02% 0.00% 0.05% 0.02% 0.00% 0.02% 0.02%]
 [0.00% 0.02% 0.11% 9.80% 0.00% 0.07% 0.00% 0.02% 0.07% 0.09%]
 [0.02% 0.00% 0.00% 0.00% 9.93% 0.00% 0.05% 0.00% 0.02% 0.09%]
 [0.00% 0.07% 0.07% 0.11% 0.05% 9.47% 0.02% 0.00% 0.04% 0.11%]
 [0.00% 0.00% 0.00% 0.00% 0.07% 0.02% 9.77% 0.00% 0.07% 0.00%]
 [0.00% 0.00% 0.02% 0.00% 0.02% 0.04% 0.00% 9.95% 0.02% 0.04%]
 [0.00% 0.12% 0.04% 0.09% 0.02% 0.02% 0.05% 0.02% 9.50% 0.00%]
 [0.02% 0.05% 0.00% 0.07% 0.05% 0.05% 0.00% 0.16% 0.04% 9.56%]

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "dataset_28_optdigits.csv"


#Number of output logits
num_output_logits = 10

#Number of attributes
num_attr = 64
n_classes = 10

mappings = []
list_of_cols_to_normalize = []

transform_true = True

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values())) + 1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize, mappings):
            if i >= data_arr.shape[1]:
                break
            col = data_arr[:, i]
            normcol = column_norm(col,mapping)
            data_arr[:, i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([0.0, 0.3106761565836299, 5.459074733096085, 11.809252669039147, 11.571530249110321, 5.608185053380783, 1.387900355871886, 0.1398576512455516, 0.003914590747330961, 1.9501779359430604, 10.5067615658363, 11.697508896797153, 10.417081850533808, 8.272241992882563, 2.020996441281139, 0.12099644128113879, 0.003914590747330961, 2.6423487544483986, 9.669395017793594, 6.677224199288256, 7.11067615658363, 8.016014234875446, 1.9377224199288257, 0.046263345195729534, 0.0014234875444839859, 2.4508896797153024, 9.263701067615658, 8.872953736654804, 9.633096085409253, 7.790747330960854, 2.3462633451957293, 0.002135231316725979, 0.00035587188612099647, 2.219217081850534, 7.786832740213523, 9.106049822064056, 10.230960854092526, 9.057651245551602, 2.9548042704626334, 0.0, 0.02099644128113879, 1.4768683274021353, 6.7149466192170815, 7.25373665480427, 7.724199288256227, 8.472597864768684, 3.5, 0.018505338078291814, 0.018149466192170817, 0.7903914590747331, 7.851245551601424, 9.772597864768683, 9.591103202846975, 9.087544483985765, 3.7444839857651244, 0.16227758007117438, 0.00035587188612099647, 0.299644128113879, 5.822064056939502, 11.959430604982206, 11.507829181494662, 6.572597864768683, 2.025622775800712, 0.24092526690391458])
        components = np.array([array([ 4.55779407e-17,  1.46389642e-02,  1.44558127e-01,  6.65329912e-02,
        1.04596813e-01,  2.24735166e-01,  9.56071658e-02,  1.13859343e-02,
        3.64423920e-05,  5.93856076e-02,  7.62643380e-02, -1.02053972e-01,
        7.30955596e-02,  2.60200406e-01,  1.21726462e-01,  7.30575775e-03,
        6.40325510e-05,  1.89930689e-03, -2.07342484e-01, -1.46984426e-01,
        2.13166856e-01,  2.06365868e-01,  2.62165796e-02, -2.56936696e-04,
       -3.31452591e-05, -7.55737325e-02, -2.61288922e-01,  1.04261473e-01,
        3.37916511e-01,  1.23019532e-01, -3.29149670e-02, -1.58274421e-04,
       -1.56910728e-05, -1.30033047e-01, -2.81553630e-01,  5.90045202e-02,
        1.62553263e-01, -1.43788329e-02, -5.14658942e-02,  0.00000000e+00,
       -1.76589662e-03, -9.96647193e-02, -3.30885517e-01, -2.63051102e-02,
        5.70769411e-02, -1.41228647e-01, -1.32297301e-01, -1.57566832e-03,
       -1.36541108e-03, -1.29917806e-02, -1.21740699e-01, -1.58200909e-02,
       -2.87697780e-02, -1.39474927e-01, -1.22398718e-01, -8.14000344e-03,
        1.00175983e-05,  1.38856644e-02,  1.72775423e-01,  4.66737377e-02,
       -1.88745541e-01, -1.32075296e-01, -4.32716068e-02,  2.42684861e-04]), array([-3.76978681e-17, -1.50109517e-02, -2.01533559e-01, -1.70444996e-01,
        6.54822472e-02,  1.11742181e-01,  7.48928095e-02,  1.34565755e-02,
       -4.00266796e-05, -1.12033520e-01, -2.93805471e-01,  6.23139644e-02,
        1.23653471e-02, -1.85646076e-02,  9.73107097e-02,  1.22714808e-02,
        7.56752532e-05, -1.08825663e-01, -9.44346173e-02,  1.77855751e-01,
       -4.40892327e-02, -8.19350334e-03,  7.80678798e-02,  5.39266672e-03,
        1.09584588e-04,  1.02871975e-02,  1.01558759e-01,  6.05891686e-02,
       -2.31782494e-02,  8.66630095e-02,  5.43902112e-02,  2.59698507e-04,
        6.18502482e-05,  8.56350440e-02,  2.42116372e-01,  2.15976263e-01,
        2.08766861e-01,  7.50836423e-02, -1.85308332e-02, -0.00000000e+00,
        2.70457836e-03,  6.13547841e-02,  9.04882374e-02,  2.62249521e-01,
        3.60632943e-01, -2.92643297e-02, -1.58084670e-01, -7.35521390e-05,
        1.97538097e-03, -1.49076549e-02, -2.17017893e-01,  6.17510067e-02,
        1.33176906e-01, -2.59584572e-01, -1.83963419e-01, -3.01257487e-03,
        1.94246022e-05, -1.25836771e-02, -1.97287495e-01, -1.83691163e-01,
       -1.43533329e-01, -1.98211573e-01, -8.54240717e-02, -1.11393779e-02]), array([ 2.21601985e-17,  1.83077672e-02,  1.14338256e-01,  8.87660097e-02,
       -1.11456024e-01, -1.59296963e-01, -6.28382240e-02, -7.49520400e-03,
        2.99551612e-04,  6.36503402e-02,  3.63346813e-02,  7.18390864e-02,
        3.14983619e-02, -2.20665784e-01, -1.03761223e-01, -9.04637771e-03,
        3.06951606e-04,  8.40077606e-03, -7.17267521e-02,  9.31245418e-02,
        1.61183823e-01, -2.70177874e-01, -1.34880715e-01, -3.75438854e-03,
        1.36097965e-04, -7.15987939e-02, -1.44231616e-01,  1.00080081e-01,
        1.01356940e-01, -3.26504191e-01, -1.63329508e-01, -1.22869679e-04,
       -2.34661878e-05, -7.11559361e-02, -3.52886826e-02,  2.75640856e-01,
        1.74630790e-01, -2.97128144e-01, -1.58444032e-01, -0.00000000e+00,
       -5.75892160e-04, -3.52755462e-02,  7.72755722e-02,  3.39154051e-01,
        1.10891118e-01, -2.38922679e-01, -7.25064577e-02,  1.29754525e-03,
       -3.28643915e-04,  1.38514710e-02,  1.32754964e-01,  2.20575000e-01,
        6.44820313e-03, -7.26133083e-02,  1.10437764e-01,  1.37622925e-02,
        8.70885071e-06,  1.85136860e-02,  1.16904152e-01,  4.83275424e-02,
        1.60362524e-02,  1.14793917e-01,  1.33655461e-01,  2.01547894e-02]), array([-1.92362820e-17,  1.13734739e-02,  1.31006374e-01,  1.66638175e-01,
        9.67984404e-03,  7.14184827e-02,  8.07287846e-02,  1.32833994e-02,
       -1.28206655e-04,  3.71048516e-02,  2.10511146e-01,  1.68595637e-02,
       -1.73904490e-01,  4.79468568e-02,  9.80324207e-02,  8.66403421e-03,
        8.88247505e-05,  7.29726377e-02,  1.10368998e-01, -2.98321987e-01,
       -3.05476058e-01,  2.61412774e-02,  7.52368798e-02,  9.76117446e-04,
        9.21740385e-06,  6.30402151e-02,  4.28301818e-02, -2.71189792e-01,
       -2.18587815e-01, -2.16644385e-02,  7.94279378e-02,  1.06833167e-04,
       -2.93489760e-06,  5.78474507e-02,  1.48567860e-01,  5.02099399e-02,
       -1.07926660e-01, -1.35541634e-01,  4.16989576e-02, -0.00000000e+00,
       -1.47930550e-03,  1.97419091e-02,  1.51885331e-01,  1.35939835e-01,
       -7.03285771e-02, -2.38292638e-01, -7.22838115e-02, -1.25190072e-03,
       -1.39647079e-03,  1.70277496e-03,  1.47320095e-01,  1.61119907e-01,
       -1.33544762e-01, -2.78957645e-01, -1.29561759e-01, -1.03552877e-02,
        1.32194737e-06,  1.22416834e-02,  1.60032660e-01,  1.49415290e-01,
       -2.36126893e-01, -2.73075504e-01, -8.57436877e-02, -1.42682916e-02]), array([-7.57314249e-17,  5.53224455e-03,  6.18212036e-02,  9.42010963e-03,
        1.51603317e-02,  1.27496880e-01,  4.32739482e-02, -1.70765723e-03,
        3.15588018e-04,  5.24643695e-02,  1.74671487e-01,  1.18711791e-01,
       -2.63464317e-02, -8.84958357e-02,  6.46303219e-03,  5.61525621e-04,
        6.40600736e-04,  1.50240910e-01,  3.87249957e-01,  1.68371253e-01,
       -4.86845498e-02, -2.43847677e-01, -1.62166687e-02,  1.12245144e-03,
        3.72983955e-04,  1.49187001e-01,  3.25846223e-01,  2.30554172e-01,
        8.74397791e-03, -1.62758578e-01, -3.77938624e-02,  9.53459231e-05,
       -1.79052095e-05, -2.97000604e-02, -1.06170764e-01, -8.96159123e-02,
       -6.73293737e-02, -1.74806360e-01, -1.16734061e-01, -0.00000000e+00,
       -2.13278723e-03, -7.60225614e-02, -2.75638375e-01, -1.93075326e-01,
        1.37457373e-01,  1.62003737e-02, -1.69113172e-01, -2.63885189e-03,
       -1.61580486e-03, -1.98492112e-02, -1.37736249e-01, -6.87363718e-02,
        3.03621857e-01,  8.82828710e-03, -1.80975223e-01, -7.07417110e-03,
       -1.83482272e-05,  3.93010488e-03,  6.02634638e-02,  1.13240121e-01,
        2.20984547e-02, -2.00179620e-01, -8.54828303e-02,  3.22042069e-03]), array([ 5.87196640e-17, -1.71547808e-03, -1.42586866e-02, -1.95478369e-02,
       -1.79304877e-02,  1.25357392e-01,  9.67462263e-02,  1.32942147e-02,
       -1.70036760e-04, -6.46246867e-03, -4.44646655e-02, -1.14965136e-01,
       -3.34894738e-01, -1.64794584e-01,  7.73018605e-02,  9.76675746e-03,
       -8.20211808e-05, -7.31265367e-03, -3.26119769e-03, -1.74239329e-01,
       -2.98890666e-01, -2.63395562e-01, -1.62863127e-02,  2.27630497e-03,
       -1.00280533e-06,  2.11416692e-02,  1.37714304e-01,  1.09870987e-01,
        7.16567138e-02, -9.12474784e-02, -8.53157954e-02, -1.88218230e-05,
       -1.81607438e-05, -3.67353864e-02,  5.42332691e-02,  2.34331459e-01,
        2.76273202e-01,  1.86948965e-01, -1.59637867e-02, -0.00000000e+00,
       -3.08529118e-03, -9.44045316e-02, -2.34977764e-01, -1.04391306e-01,
       -1.07011552e-01, -1.55411508e-03,  1.65881977e-01,  3.22451067e-03,
       -3.21983485e-03, -5.15479920e-02, -2.35111233e-01, -1.69977537e-01,
       -3.41169500e-01, -1.03577014e-01,  2.18101728e-01,  1.08358235e-02,
       -8.69817781e-05, -3.53303424e-03, -8.80783202e-03, -5.75286417e-03,
       -2.89102058e-02,  1.38526612e-01,  7.09607247e-02, -3.15644698e-03]), array([ 9.90437774e-18, -4.83565434e-03, -5.72717048e-02, -3.82793748e-02,
       -9.05727105e-02, -7.80040013e-02,  5.08615995e-02,  2.06911289e-02,
       -1.73318992e-04,  1.28855409e-02, -6.58268914e-02,  8.93889100e-02,
       -2.58012557e-02, -2.10534828e-01,  4.48712115e-02,  1.98331746e-02,
        1.88303797e-04,  1.53871252e-02, -6.28967784e-02, -7.88473331e-02,
       -1.43739698e-01, -9.78129517e-02,  6.69929053e-02,  5.92529106e-03,
        1.92433890e-05, -7.14462449e-04, -2.45515098e-01, -4.52437338e-01,
       -2.21862146e-01,  4.92412170e-02,  3.03894497e-02,  2.34912956e-04,
        2.55358417e-05, -1.04703549e-02, -2.85129608e-01, -3.32659697e-01,
        2.75531384e-02,  1.29975411e-02, -3.06262399e-02,  0.00000000e+00,
        3.40875746e-04, -1.54659413e-02, -2.80693493e-01,  1.39165080e-02,
        2.55511529e-01, -6.35391524e-02, -4.47818311e-02,  9.00592708e-04,
        9.08678961e-05, -4.06304002e-02, -2.42861271e-01,  1.32929073e-01,
        1.93186862e-01, -6.56329715e-02,  1.47790232e-01,  3.02521849e-02,
        2.38635734e-05, -7.79364280e-03, -7.28151815e-02,  9.93567636e-03,
       -1.32428926e-02,  8.01503227e-02,  2.27330370e-01,  6.64373540e-02]), array([-3.82409583e-17,  3.37517051e-02,  1.18245690e-01, -9.25966970e-02,
        1.51248837e-01,  2.75952049e-01,  7.96355719e-02,  7.15504386e-03,
       -1.07126608e-04,  2.12139641e-02, -1.45265426e-01, -1.56515024e-01,
       -8.74156105e-03, -1.92650526e-02,  3.80487423e-02,  8.19146106e-03,
       -3.31704160e-04, -1.28742813e-01, -3.98985049e-01, -7.97997967e-03,
       -6.60995494e-02, -3.50961871e-01, -2.83164792e-02,  3.32205910e-03,
       -2.76654419e-04, -9.77038123e-02, -2.38702706e-01, -6.05054804e-03,
       -2.31284535e-01, -3.67477180e-01, -4.36230016e-02, -4.85633030e-05,
       -2.11841471e-05,  3.34147333e-02,  2.33382871e-02, -1.16077186e-01,
       -1.21228580e-01, -5.82061440e-02, -4.98647178e-02,  0.00000000e+00,
        5.42176918e-03,  1.16012135e-01,  7.40512023e-02, -1.25383495e-01,
       -2.80617706e-02,  2.04132702e-01, -6.15558125e-02, -1.75647977e-03,
        5.82642614e-03,  9.15555796e-02,  8.07995705e-02, -1.74916418e-01,
        5.75276765e-02,  1.04156128e-01, -1.40048460e-01, -1.23251680e-02,
        8.04589605e-06,  3.71746763e-02,  1.62742514e-01, -8.67560024e-02,
       -2.34158673e-02, -1.25671763e-01, -1.15411998e-01, -2.29337377e-02]), array([ 4.63260593e-17,  2.18382795e-02,  1.88045554e-01,  1.15961280e-01,
       -1.69022398e-01, -3.17390047e-01, -8.25165329e-02, -4.67739314e-03,
       -2.02942646e-04,  3.92173356e-02,  1.40961522e-02,  7.20052650e-03,
        7.48791019e-02, -3.88451009e-01, -1.44571858e-01, -1.62748159e-03,
       -1.97133064e-04, -3.18673648e-02, -1.28349738e-01,  5.10009459e-02,
        1.77134611e-01, -2.10851149e-01, -4.55143882e-02,  4.02432453e-03,
        1.09838454e-04,  2.23482175e-03, -1.84922821e-02,  6.06190782e-02,
        8.70877644e-02,  1.74001642e-01,  1.06238906e-01,  4.46311904e-04,
        5.20388317e-05,  8.86427273e-02,  6.17910481e-02, -4.64862651e-02,
       -3.92353008e-02,  4.10922335e-01,  1.89360960e-01,  0.00000000e+00,
        3.53582258e-03,  7.08587026e-02, -9.98366589e-02, -4.85212021e-04,
       -1.21212814e-01,  5.55282626e-02,  8.31914987e-02, -4.09240215e-05,
        2.26575026e-03,  2.19793270e-02, -7.99652192e-02,  2.94406636e-03,
        4.39648572e-02, -1.30267231e-01, -8.52068773e-02, -8.57637102e-03,
       -1.27733916e-04,  1.97336718e-02,  2.02341740e-01,  5.71542510e-02,
       -1.78259900e-01, -3.14915733e-01, -1.24651578e-01, -1.42298471e-02]), array([ 2.51334743e-17,  3.02413860e-03, -8.73822570e-02, -1.45482499e-01,
       -2.03874421e-02,  5.19743194e-02,  6.33416247e-02,  1.02291978e-02,
       -1.81863540e-04, -1.58764385e-02, -1.79698313e-01, -4.95114558e-02,
        1.30552560e-02, -8.76018626e-02,  4.18393412e-02,  9.33703201e-03,
        9.54997402e-04,  1.72620310e-02, -3.99841866e-02,  2.10066568e-01,
        1.47567579e-01, -7.60974366e-02,  3.44681946e-02,  3.59806163e-03,
        3.17718428e-04,  6.59210344e-03,  6.12647398e-02,  1.69109152e-01,
       -2.60164276e-02,  2.50744408e-02,  1.02202942e-01,  9.90770298e-05,
       -7.46260922e-06, -3.42847606e-02, -8.49480227e-02, -2.02643570e-01,
       -4.55447999e-01, -2.10050872e-01,  6.34907170e-02, -0.00000000e+00,
        8.41280043e-04, -2.22289067e-02, -7.61951812e-02,  6.18256216e-03,
       -2.67960281e-01, -4.23814280e-01,  1.87588793e-02,  2.60724680e-03,
        1.69999677e-03,  4.75445618e-03, -6.79118991e-02, -6.40866427e-02,
       -2.46038305e-01, -3.10863473e-01,  2.43376187e-02,  7.14640597e-03,
        3.97601417e-05,  2.30775983e-03, -9.14445234e-02, -2.09387508e-01,
       -8.74025924e-02,  6.81241873e-02,  7.82680538e-02, -2.00524818e-03]), array([ 1.68173825e-17,  2.51363404e-02,  1.62065782e-01, -2.02295242e-01,
       -2.78790341e-01, -1.74911200e-01, -1.05963302e-01, -2.00935038e-02,
       -1.24751151e-04,  1.06706365e-01, -3.48660506e-02, -3.86101571e-01,
       -1.71814004e-01, -1.00843107e-01, -5.44125534e-02, -1.04439146e-02,
       -3.07025215e-04,  1.07554086e-01, -3.42986548e-02, -2.11316250e-01,
        1.11681096e-01,  1.27453804e-01,  3.30509323e-02,  1.00148193e-03,
       -1.04403037e-04,  1.25969832e-01,  1.42901688e-01, -1.15284895e-01,
        6.15740579e-02,  9.79770300e-02,  4.37542238e-02,  2.04648144e-04,
        6.06968552e-05,  1.49609711e-01,  1.00186278e-01, -1.40833651e-02,
        6.62011766e-02, -1.85133128e-01, -1.04316673e-01,  0.00000000e+00,
        2.44082802e-03,  1.02886203e-01,  2.26562490e-02, -5.72283000e-02,
        2.23241091e-01, -1.04785151e-01, -2.24364074e-01, -5.47890505e-03,
        1.75311653e-03,  5.99900715e-02,  4.73704913e-02, -3.91224021e-01,
        3.54662546e-02,  2.01000616e-02, -1.19142865e-01, -1.45246408e-02,
       -2.33447725e-05,  2.61426384e-02,  1.36005939e-01, -1.88487446e-01,
        5.97343250e-02,  1.07854947e-01,  5.38496350e-02,  1.40336328e-02]), array([-3.52642375e-17,  2.75027255e-03,  1.05650162e-01,  1.81161861e-02,
       -2.78134616e-01, -6.22027867e-02,  1.56927191e-01,  3.70127274e-02,
       -7.54420052e-04, -9.78482911e-02, -3.74210503e-02,  4.60949198e-02,
       -3.62674076e-01, -1.12287105e-01,  2.73974728e-01,  3.60252737e-02,
       -1.58706113e-03, -2.08764061e-01,  1.44481830e-02,  2.79393341e-01,
       -2.38897020e-01,  9.28791691e-02,  2.54047670e-01,  1.15156337e-02,
       -4.89227056e-04, -2.02978565e-01, -1.22128878e-01,  2.75707702e-01,
        2.89432332e-02,  2.16403113e-01,  7.73098217e-02,  5.19540939e-04,
        6.64013891e-05, -1.29661014e-01, -2.00261711e-01, -1.48293120e-02,
       -3.24365031e-02,  8.87609952e-03, -8.63578193e-02,  0.00000000e+00,
        2.10088659e-03,  3.73661296e-02,  1.09022462e-01, -2.64552487e-02,
        7.76421790e-02,  6.08632065e-02, -5.51444010e-02,  1.56372202e-03,
        1.82494480e-03,  8.87191203e-02,  2.87426181e-01, -1.30335822e-02,
        5.15915910e-02,  2.48500530e-02,  3.72462374e-03,  1.13671319e-02,
        1.75889574e-05,  1.28836787e-02,  9.40914827e-02,  1.01669800e-01,
        4.43522504e-02, -7.48025675e-03,  3.43151851e-02,  2.34828484e-02]), array([-8.95236952e-17,  3.28471812e-02,  2.01098276e-01, -1.54173662e-01,
       -7.08557447e-02,  3.23965291e-01,  1.35189175e-01, -2.31628124e-03,
        3.35927819e-04,  8.52952076e-02,  3.01680804e-02, -6.95103138e-02,
        6.41297302e-02,  4.75410993e-02,  8.06261492e-02, -3.72474475e-03,
       -4.42734676e-04,  1.18781588e-01,  9.36273706e-02,  2.71233815e-02,
        6.71887646e-02, -9.19912643e-02, -6.61425710e-02, -5.99270112e-03,
       -4.36732077e-04,  6.75898181e-02,  2.53785947e-01,  1.82057414e-02,
        3.23803448e-02,  1.24313320e-01, -6.98519460e-02, -3.41480537e-04,
       -1.10864899e-04, -3.22608303e-03,  3.48722404e-02, -1.99661556e-01,
       -1.00748519e-02,  2.13041203e-01, -7.16285105e-03, -0.00000000e+00,
        1.73770532e-03,  3.79745548e-02, -1.82807429e-02,  2.35854344e-01,
        7.36552718e-02,  1.68410374e-01,  2.95333154e-02,  3.04211975e-04,
        2.89825754e-03,  4.64397977e-02,  2.12889623e-01,  3.12475184e-01,
       -5.75577680e-04,  1.06304089e-01,  1.64575986e-01,  1.33150049e-02,
        7.98584051e-05,  3.68444431e-02,  2.25880364e-01, -3.05710375e-01,
       -2.88768021e-01,  1.58718661e-01,  1.77467592e-01,  2.37512527e-02]), array([-1.84373303e-17, -4.22439887e-02, -2.25404197e-01, -5.85336792e-02,
       -2.39097999e-01, -6.47755752e-02, -3.13936342e-02, -1.18532558e-02,
       -2.94118067e-04, -1.67985932e-01,  1.53004846e-02, -4.68031843e-02,
       -2.47745988e-01,  4.34186989e-02, -4.88030288e-02, -1.90513065e-02,
       -1.71812813e-04, -1.32769672e-01,  1.83482223e-01, -4.80517785e-02,
        1.83276207e-01,  1.73653092e-02, -1.84443489e-01, -1.11819656e-02,
        3.44645848e-05, -4.55773166e-02, -3.55755139e-02, -1.27265766e-01,
        2.29064628e-01, -2.60553633e-01, -2.45200688e-01, -1.11621821e-04,
       -8.16201872e-05, -1.35931967e-02, -1.21500026e-01, -3.27599213e-01,
        1.06079952e-01,  2.58525843e-02, -1.56992442e-01, -0.00000000e+00,
       -4.55934777e-03,  1.40079463e-02,  1.35131705e-01, -8.91590562e-02,
       -2.68769019e-02,  1.65701098e-01, -3.77861667e-02, -2.96153614e-03,
       -2.68578987e-03, -3.15049827e-02,  9.02115149e-02,  5.72812367e-02,
       -1.81214552e-01, -1.16460943e-01, -5.75777748e-02, -5.75323828e-03,
        6.55101235e-05, -3.50337289e-02, -2.25316118e-01, -5.62238307e-02,
       -2.70374306e-01, -2.21128563e-01, -9.38995602e-02, -1.26060021e-02]), array([ 7.23545794e-17,  2.90932103e-03, -7.54081170e-02, -2.48118053e-01,
       -4.37454887e-01, -1.88885306e-02,  2.38160669e-01,  5.37619244e-02,
        2.52294325e-05,  2.28206164e-02,  6.65042088e-02,  1.76921282e-01,
        2.13819594e-01, -6.25718296e-02,  2.61724717e-01,  4.15172769e-02,
        3.08449275e-04,  7.16645086e-02,  5.14180686e-02, -1.11771889e-01,
        9.73635560e-02, -3.14700415e-02,  1.55998245e-01,  1.18719207e-02,
        2.99384156e-05,  2.42288351e-02, -1.08409365e-01, -1.40837493e-01,
       -5.79184147e-02, -9.16714353e-02,  7.40671868e-02, -1.64418501e-04,
       -7.99510110e-05, -4.27964764e-02,  6.79670175e-03,  3.24157562e-01,
        2.38413963e-02, -4.38550537e-02, -1.26010264e-02, -0.00000000e+00,
       -4.31037816e-03, -1.03708292e-01, -6.37460593e-02, -4.01469262e-02,
       -3.25072307e-01, -4.76646957e-03, -5.31219110e-02, -3.41255857e-03,
       -2.86698789e-03, -3.64432506e-02, -4.83553130e-02,  4.87582793e-03,
        6.05220866e-02,  3.06538462e-01, -5.95113635e-02, -2.59081417e-02,
        2.83730419e-05,  7.07641699e-03, -9.15703314e-02, -1.81219993e-01,
       -1.20115584e-01, -6.10991825e-02, -1.36740391e-01, -4.90838509e-02]), array([ 9.75584187e-17, -6.49078988e-03, -1.42270894e-01, -1.17850081e-01,
        2.71230093e-01,  1.60078014e-01,  2.19016827e-02, -5.19434824e-04,
        1.50282856e-03,  1.28308403e-01,  1.14564036e-01, -6.81204108e-02,
       -1.47725848e-01, -2.43362310e-01,  2.37837849e-02,  9.63501836e-03,
        1.46754279e-03,  2.25557132e-01,  1.60958876e-01, -9.25754138e-02,
        2.22208637e-02, -1.98714443e-01,  9.12966133e-02,  7.57714183e-03,
        3.43282726e-04,  5.82167789e-02, -1.56082413e-01, -9.43777060e-02,
        2.58908353e-01,  1.36729734e-01,  3.23185141e-02,  4.52641696e-04,
        2.91822522e-05,  1.08065229e-02, -3.13454052e-01, -3.73223685e-04,
       -7.39626345e-03,  1.27602819e-01,  5.24625289e-02,  0.00000000e+00,
        8.58955924e-03,  1.50136530e-01,  1.62268328e-01,  3.72865316e-01,
       -1.20217545e-01, -3.07325028e-03, -3.98411723e-02, -3.13110434e-03,
        7.01502515e-03,  7.33434765e-02,  7.66165438e-02, -9.17666507e-02,
        5.65026254e-03,  5.01390272e-02, -1.40207809e-01, -4.41003741e-02,
        1.53149820e-04, -7.39536592e-03, -1.47719223e-01, -4.12169350e-02,
        2.72686567e-01, -1.89676457e-02, -1.59147054e-01, -6.38543201e-02]), array([-1.02629385e-16,  1.77920345e-02,  1.33309336e-01,  3.46872763e-03,
        4.62486819e-03,  1.48005225e-01,  1.61521997e-01,  2.84746845e-02,
       -8.54638761e-04,  1.34131076e-02,  1.36656153e-01,  2.52095232e-01,
        1.93061821e-01,  4.84471685e-02,  1.36387657e-01,  2.75164399e-02,
       -2.06873865e-04, -5.59011303e-02,  2.24093713e-01,  2.66493161e-01,
        1.87337015e-01, -6.18481360e-02,  6.51972155e-02,  7.51768917e-03,
       -5.26609325e-05,  2.31392429e-02, -1.19162715e-01, -2.48566310e-01,
        3.89362372e-02, -9.39215744e-02,  4.80956618e-04, -4.22320527e-05,
        1.32288054e-04,  1.37567066e-01,  6.26080570e-02, -3.91190185e-02,
        2.47814527e-01,  1.58191312e-01,  7.50896960e-02, -0.00000000e+00,
       -3.41913566e-03,  7.57967110e-02,  2.17738268e-01, -2.02841596e-01,
        5.87056108e-02, -1.09452815e-01,  5.56292470e-02,  1.75400367e-03,
       -4.58695163e-03,  1.66577359e-02,  8.07863581e-02, -3.68138177e-01,
       -6.72370709e-02, -3.18646022e-01,  6.83592899e-02,  5.10225168e-03,
        1.12796293e-05,  2.63348312e-02,  1.34874812e-01,  3.54486973e-03,
        1.01289086e-01,  9.88759199e-02,  1.24781607e-01,  1.85710534e-02]), array([-9.44314508e-17,  2.16456586e-02,  8.61776353e-02, -5.63825686e-02,
       -4.54281524e-02, -8.69242746e-02, -1.00946728e-01, -2.30322741e-02,
        9.59508800e-04,  1.16573153e-01,  1.16121768e-01, -6.72889619e-02,
       -2.92134385e-03,  1.04526110e-01, -1.08268880e-01, -1.99725513e-02,
        1.14709438e-03,  9.45444044e-02,  1.79054035e-01,  2.63665306e-01,
        1.91254986e-02,  1.49007010e-01, -3.23062696e-02, -3.75555755e-03,
        1.05031225e-04, -1.14410174e-01, -7.83755548e-02, -2.54563999e-02,
       -3.96758761e-01, -3.34008373e-02, -4.56019897e-02, -8.55231138e-05,
       -2.58916214e-05, -1.42686175e-01, -2.48315725e-01,  1.64864300e-01,
       -9.28640380e-02, -1.16380949e-02, -5.88468158e-04,  0.00000000e+00,
       -2.73407552e-04, -4.61407704e-02, -1.60340565e-02,  3.26891749e-01,
        1.10031089e-01,  3.07916778e-01,  4.73569578e-02,  1.58940549e-03,
       -3.23369675e-03, -4.22151731e-02, -2.42410748e-01, -2.59593810e-01,
       -2.94777229e-01, -4.68443987e-02,  1.21869286e-02, -2.13144136e-03,
       -7.63374523e-05,  1.87251034e-02,  1.07915126e-01, -6.71952175e-02,
       -1.30746566e-01, -5.84593683e-02, -8.78023418e-02, -3.16398820e-02]), array([ 4.32387832e-17,  1.22929273e-02,  4.97415750e-02,  1.50649542e-01,
       -1.61940959e-01,  1.89152149e-01,  1.41154032e-01,  1.06667953e-02,
        9.11630319e-04,  5.53086782e-02, -1.12859940e-01, -3.01787208e-02,
       -4.00845268e-01,  3.89475890e-02,  1.49882157e-01,  1.91752935e-03,
        1.61725654e-04,  1.17937643e-01, -1.14161042e-01,  2.12956594e-01,
        2.75926867e-01,  1.43881664e-01, -1.52236374e-02, -5.71108229e-03,
        1.49667022e-04,  1.75852303e-01, -1.91558936e-02, -1.25307445e-01,
        1.01254299e-01, -1.19254930e-01, -6.03742692e-02, -1.63257816e-04,
        7.08345559e-05,  1.58806230e-01,  1.63918174e-01, -5.53069353e-02,
       -1.24735581e-01, -8.10435556e-02,  1.69537964e-01, -0.00000000e+00,
       -3.94698104e-03,  1.01388011e-02,  5.53731460e-02,  1.65469066e-01,
        3.71668900e-02, -3.97491811e-02,  2.32660766e-01,  1.28848978e-03,
       -7.13494130e-03, -9.26742180e-02, -2.93938320e-01,  3.09097367e-02,
        1.02305819e-01,  2.07261208e-01,  1.17580530e-01,  2.03024624e-03,
       -2.00228247e-04,  8.02336309e-03,  6.98467413e-02,  2.57129170e-01,
        6.94099113e-02, -3.13527992e-02, -1.11898676e-01, -2.13679402e-02]), array([ 1.33623628e-16,  1.71156944e-02, -6.46671427e-02, -2.82461075e-01,
       -1.23303929e-01,  5.70278152e-02, -1.38661077e-01, -2.79863603e-02,
        1.09560214e-03,  2.61821602e-01,  1.72615564e-01, -3.82715372e-02,
       -1.78042322e-01,  1.69220369e-01, -1.15421169e-01, -2.49596069e-02,
        1.20755170e-03,  2.48202950e-01, -3.46042297e-02, -3.40974786e-02,
       -8.43467999e-02,  2.64797549e-02, -1.08012304e-01, -5.48407672e-03,
       -2.00546752e-04, -9.14924868e-02, -2.21349953e-01,  1.56310151e-01,
       -3.54776349e-02, -3.52912446e-02, -1.09447022e-01, -2.52633927e-04,
       -2.53658596e-05, -1.23561519e-01,  4.06303187e-02,  1.60523314e-01,
       -1.41617599e-01,  2.03265348e-01,  5.99760184e-02,  0.00000000e+00,
        2.11668289e-03,  6.46910278e-03,  1.73731172e-01, -2.02173571e-01,
        4.39353035e-02, -1.39102396e-01,  1.75818582e-01,  3.80823632e-03,
        3.71590041e-03,  4.52766154e-02, -4.83760673e-03,  1.83903657e-02,
        4.09447961e-01, -2.21883763e-01,  1.87583259e-01,  1.61810937e-02,
        2.07814955e-05,  7.38995837e-03, -1.06425266e-01, -6.83016206e-02,
       -5.88976632e-02, -1.03406885e-01,  8.58315548e-02,  1.12642678e-02]), array([-2.45699148e-17,  3.07845369e-02,  1.34078163e-01, -8.99670958e-02,
       -5.00535186e-02,  1.01676237e-01, -1.67059650e-01, -6.37693142e-02,
       -1.33142678e-03, -7.75731437e-02, -1.47644573e-01,  1.90153076e-01,
       -1.83574235e-01,  1.28825489e-01, -2.68034564e-01, -6.07979400e-02,
       -2.02542309e-04, -1.24541035e-01,  3.62477051e-03,  2.93175243e-01,
       -2.85452687e-01,  6.07383987e-02, -2.79704936e-01, -2.31679918e-02,
       -5.51845476e-05,  6.99369146e-02, -5.17749053e-02, -2.27420409e-01,
        3.26641995e-02,  1.28744839e-01, -1.75021026e-01, -7.76025900e-04,
       -4.58624996e-06,  7.69243111e-02, -5.01000079e-02,  3.25012540e-02,
        1.34236669e-01,  5.80111512e-02,  1.78948008e-02, -0.00000000e+00,
       -7.95754927e-03, -1.06154832e-01, -1.36761546e-01,  6.69052632e-02,
       -3.17690046e-01, -2.38261732e-01, -1.01703323e-01, -1.21912284e-03,
       -5.98200653e-03, -1.55706566e-02,  4.33045492e-02, -3.38182124e-02,
        1.07403567e-01,  1.26023771e-01, -2.00967815e-01, -2.03999196e-02,
        4.26108406e-04,  3.65116686e-02,  1.40900916e-01, -1.77120545e-01,
        5.19507532e-02,  1.79626366e-02, -6.71907013e-02,  3.23807803e-03]), array([ 1.74043381e-17, -1.94070329e-02, -2.64100862e-02,  6.87332729e-02,
       -4.76019861e-02,  2.25384130e-01,  3.95239308e-02, -2.53078895e-04,
       -1.51118148e-03, -8.03177649e-02,  2.56315095e-01,  4.80136699e-02,
       -8.84710660e-02, -1.47612689e-01, -1.02317003e-01, -1.00151897e-02,
       -8.54879376e-04, -1.41093402e-01,  6.35437387e-02, -4.86301503e-02,
        1.20139764e-01, -8.51008742e-02, -1.49670725e-01, -5.45873915e-03,
       -5.24373258e-04, -1.46265536e-01, -1.91484652e-01, -1.90899643e-02,
        2.47025685e-02,  2.50759164e-01, -8.05430033e-02, -5.28852271e-04,
       -1.45691736e-04, -6.24260163e-02,  3.08978763e-01,  9.01963470e-02,
       -3.07781162e-01,  7.24987315e-02, -1.39018092e-01,  0.00000000e+00,
       -1.60761174e-03, -6.11088843e-02,  1.10330168e-01, -2.99045823e-02,
       -2.18488417e-02,  1.12973589e-01, -3.60646249e-01, -4.05051922e-03,
       -3.65465193e-03, -4.72354726e-02, -2.59055188e-01, -1.52703439e-02,
       -5.27173669e-02,  4.43789130e-02, -1.77532210e-01,  8.13560513e-03,
       -3.23559894e-04, -2.35542335e-02, -4.43771584e-02,  1.23456542e-01,
       -4.46015044e-02,  2.25635654e-01,  2.39759539e-01,  7.14467651e-02]), array([-1.48442237e-17, -3.83345487e-02, -4.24170382e-02, -3.37557187e-02,
        5.25263329e-02,  1.60050196e-01, -1.13100311e-01, -2.85566170e-02,
       -2.27319746e-03, -2.01000641e-01,  2.24670184e-01, -5.49684939e-02,
       -7.01179467e-02, -3.34518191e-02, -1.88539628e-01, -2.22179120e-02,
       -1.94335343e-03, -2.47680858e-01,  1.90375110e-01, -1.00878640e-02,
        3.10437240e-02, -1.40377785e-01, -2.19722329e-02,  2.70560454e-03,
       -6.90903926e-04, -1.05006407e-01, -1.57363401e-01,  4.68656568e-02,
        2.54306412e-02,  6.62654040e-02,  3.39444916e-01,  5.64382161e-04,
       -2.32963246e-04,  3.97970998e-02,  1.09975455e-02,  1.84029988e-02,
        9.12280363e-02, -2.15203979e-01,  3.32691143e-01, -0.00000000e+00,
        2.80399060e-03,  5.82271126e-02, -3.35315299e-02, -3.59585679e-02,
        2.37629637e-01, -2.09828799e-01,  1.51606308e-01,  2.65426527e-03,
        2.60470675e-03,  3.87737374e-03,  2.29091509e-02, -8.37448122e-02,
       -9.20284105e-03,  3.25759531e-01,  1.74003148e-01,  1.64030651e-02,
        2.01335463e-05, -2.87042552e-02, -2.66373037e-02, -1.23792808e-01,
       -2.69206009e-01, -6.31883052e-02, -7.81144714e-02,  1.12874819e-02]), array([ 4.95106617e-17,  2.73527341e-02, -3.26190729e-02,  3.81488190e-02,
        2.07792114e-01, -8.97608084e-02, -1.92958860e-01, -2.76227194e-02,
        1.38017116e-03,  2.26197825e-01,  4.54442036e-02,  2.13339247e-01,
       -2.00208517e-01, -3.91803082e-02, -3.68151082e-02, -3.10950801e-03,
        1.94809684e-03,  2.98296948e-01, -1.17636997e-01,  2.42509719e-01,
       -4.87790240e-02,  4.20543553e-02,  2.29010002e-01,  1.45357905e-02,
        4.21909647e-04,  1.70565799e-01, -1.90654451e-01,  2.29854398e-02,
        1.43719132e-01, -2.93221393e-02,  2.64599960e-01,  1.14354545e-03,
        7.21640760e-05,  1.27024293e-01,  1.24270907e-01, -3.99884004e-02,
        1.41278372e-01, -1.60221877e-01, -1.19954760e-01, -0.00000000e+00,
        6.96560901e-04,  2.93342478e-02,  2.75590159e-02, -1.88019807e-01,
       -1.27448278e-01,  2.17679665e-01, -1.83546783e-01, -2.11271907e-03,
       -8.51960717e-04, -5.00074399e-02, -3.68262546e-02,  1.08762261e-01,
       -1.63593873e-01,  6.20486032e-02,  5.73739150e-02,  1.32712386e-02,
       -2.29699781e-04,  1.71497234e-02, -1.72355704e-03, -1.65866101e-01,
       -2.37501287e-01, -3.67535086e-02,  1.60654887e-01,  3.46188816e-02]), array([ 7.74849925e-18, -4.29030983e-02, -4.10152068e-02,  5.86201610e-02,
       -1.64913791e-01,  5.12269471e-02, -1.25926132e-01, -1.48489382e-02,
        3.60482342e-04, -1.12945803e-01, -5.94002542e-03, -1.96133842e-01,
       -1.74312055e-01,  2.67515460e-01, -5.36318585e-02, -9.53059989e-03,
        3.80283128e-04,  6.38480506e-02,  5.72157670e-02,  7.69066532e-02,
        2.24898439e-01, -1.49194880e-01,  1.08605676e-01,  9.49466798e-03,
        2.42367169e-04,  1.25159100e-01, -6.00323702e-02,  4.33380124e-02,
       -2.57194279e-01, -1.90635044e-01,  2.71240923e-01,  8.82378899e-04,
       -1.65518821e-04,  1.11956912e-01, -1.12720828e-01,  1.43792684e-01,
        7.26633936e-02,  2.77290730e-01,  1.58617250e-01, -0.00000000e+00,
        5.32399443e-03,  7.16570425e-02, -2.00327397e-01, -4.84322475e-02,
       -1.42791645e-02, -1.92462484e-02, -1.58217379e-01, -1.99411214e-03,
        6.71958049e-03,  5.09287062e-02,  1.48939429e-01,  2.40165318e-01,
       -5.15251801e-02, -9.17638432e-02, -2.72601884e-01, -1.88803270e-02,
       -4.83112104e-05, -3.80436817e-02, -1.11611969e-01,  1.24226913e-01,
        8.31663480e-02,  2.44264576e-01,  6.31611905e-02,  3.34103960e-02])])
        whiten = False
        explained_variance = np.array([178.33659177363282, 162.4151484000601, 143.8206742315699, 102.97441670364137, 68.12348645348392, 61.07147826971476, 54.912125256721296, 44.36596326529317, 42.143196220585345, 37.73074709381276, 29.214140865657583, 26.900199369910332, 22.36577618414611, 18.96143023241057, 17.2557096715803, 16.87942688827794, 15.447267690893666, 14.44570188367582, 12.033518543404835, 11.09529595657714, 10.734149904024262, 10.076075143358976, 9.13161755349822, 8.486062206050772, 8.030190137341924])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files
def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    il=[]
    
    ignorelabels=[]
    ignorecolumns=[]
    target="class"


    # if (testfile):
    #     target = ''
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if (target != ''): 
                        hc = header.index(target)
                    else:
                        hc = len(header) - 1
                        target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                for i in range(0, len(header)):      
                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    print(header[i] + ",", end='', file=outputfile)
                print(header[hc], file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if (row[target] in ignorelabels):
                        continue
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name==target):
                            continue
                        if (',' in row[name]):
                            print ('"' + row[name] + '"' + ",", end='', file=outputfile)
                        else:
                            print (row[name] + ",", end='', file=outputfile)
                    print (row[target], file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc =- 1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    if (hc == -1):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if (',' in row[i]):
                            print ('"' + row[i] + '"'+",", end='', file=outputfile)
                        else:
                            print(row[i]+",", end = '', file=outputfile)
                    print (row[hc], file=outputfile)

def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    # function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping

# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)
# Classifier
def single_classify(row):
    #inits
    x = row
    o = [0] * num_output_logits


    #Nueron Equations
    h_0 = max((((1.6670955 * float(x[0]))+ (0.4788265 * float(x[1]))+ (-0.6966118 * float(x[2]))+ (-0.3806394 * float(x[3]))+ (0.22098224 * float(x[4]))+ (-0.025440464 * float(x[5]))+ (-0.14372465 * float(x[6]))+ (-1.0624768 * float(x[7]))+ (0.69774556 * float(x[8]))+ (0.7466435 * float(x[9]))+ (0.47773 * float(x[10]))+ (-0.07871684 * float(x[11]))+ (0.2294896 * float(x[12]))+ (-0.5987303 * float(x[13]))+ (-0.1574835 * float(x[14]))+ (-0.623084 * float(x[15]))+ (0.676302 * float(x[16]))+ (0.4534151 * float(x[17]))+ (-0.7274995 * float(x[18]))+ (-0.69025123 * float(x[19]))+ (-0.20435452 * float(x[20]))+ (0.6472998 * float(x[21]))+ (-0.043571755 * float(x[22]))+ (0.87169075 * float(x[23]))+ (0.674609 * float(x[24]))) + 1.3347461), 0)
    h_1 = max((((1.8720791 * float(x[0]))+ (-1.2464523 * float(x[1]))+ (0.09160898 * float(x[2]))+ (-0.15106139 * float(x[3]))+ (0.7371993 * float(x[4]))+ (0.6897619 * float(x[5]))+ (0.47455567 * float(x[6]))+ (-0.3783464 * float(x[7]))+ (-0.3380101 * float(x[8]))+ (-0.0751036 * float(x[9]))+ (-0.58749324 * float(x[10]))+ (-0.06326497 * float(x[11]))+ (-0.5039947 * float(x[12]))+ (-0.4043777 * float(x[13]))+ (0.19531558 * float(x[14]))+ (-0.76805633 * float(x[15]))+ (0.20317474 * float(x[16]))+ (-0.7017039 * float(x[17]))+ (-0.0740223 * float(x[18]))+ (-0.3327423 * float(x[19]))+ (0.07044361 * float(x[20]))+ (0.5625441 * float(x[21]))+ (-0.6476653 * float(x[22]))+ (0.50400424 * float(x[23]))+ (-0.9095916 * float(x[24]))) + 8.206443), 0)
    h_2 = max((((1.0010566 * float(x[0]))+ (1.9410353 * float(x[1]))+ (0.80815077 * float(x[2]))+ (0.03282332 * float(x[3]))+ (2.3115103 * float(x[4]))+ (-0.4615175 * float(x[5]))+ (-0.72136664 * float(x[6]))+ (-0.64168197 * float(x[7]))+ (-0.2793076 * float(x[8]))+ (-0.40822014 * float(x[9]))+ (2.0310183 * float(x[10]))+ (0.8834967 * float(x[11]))+ (-0.42577347 * float(x[12]))+ (0.440866 * float(x[13]))+ (-0.95677763 * float(x[14]))+ (0.027196983 * float(x[15]))+ (-0.9332201 * float(x[16]))+ (1.5153824 * float(x[17]))+ (0.41121837 * float(x[18]))+ (-0.47447652 * float(x[19]))+ (-0.45361054 * float(x[20]))+ (0.73091596 * float(x[21]))+ (-1.2014318 * float(x[22]))+ (0.86246157 * float(x[23]))+ (-0.24451092 * float(x[24]))) + -4.518817), 0)
    h_3 = max((((1.9190073 * float(x[0]))+ (0.41383806 * float(x[1]))+ (0.32951152 * float(x[2]))+ (0.04116866 * float(x[3]))+ (0.10617708 * float(x[4]))+ (0.60990465 * float(x[5]))+ (-0.2040372 * float(x[6]))+ (-0.8717511 * float(x[7]))+ (0.02057437 * float(x[8]))+ (0.045353744 * float(x[9]))+ (0.06968807 * float(x[10]))+ (0.3779279 * float(x[11]))+ (-0.47520983 * float(x[12]))+ (0.3480085 * float(x[13]))+ (-0.3802794 * float(x[14]))+ (-0.18677883 * float(x[15]))+ (0.24303365 * float(x[16]))+ (0.22538844 * float(x[17]))+ (-0.13863687 * float(x[18]))+ (-0.62230533 * float(x[19]))+ (-0.6837254 * float(x[20]))+ (0.06714314 * float(x[21]))+ (-0.83765745 * float(x[22]))+ (0.5103969 * float(x[23]))+ (-0.7194672 * float(x[24]))) + 11.55394), 0)
    h_4 = max((((-1.9535648 * float(x[0]))+ (-0.8281373 * float(x[1]))+ (0.46122324 * float(x[2]))+ (-6.780389 * float(x[3]))+ (1.7515031 * float(x[4]))+ (3.1261208 * float(x[5]))+ (1.41067 * float(x[6]))+ (0.2680632 * float(x[7]))+ (-1.453559 * float(x[8]))+ (-0.39918455 * float(x[9]))+ (-0.6496826 * float(x[10]))+ (1.1743476 * float(x[11]))+ (0.28272158 * float(x[12]))+ (1.5527349 * float(x[13]))+ (-2.181594 * float(x[14]))+ (4.5009384 * float(x[15]))+ (-0.17310855 * float(x[16]))+ (0.05756638 * float(x[17]))+ (0.1268734 * float(x[18]))+ (-0.5895301 * float(x[19]))+ (-2.758263 * float(x[20]))+ (0.0147838555 * float(x[21]))+ (-1.3261963 * float(x[22]))+ (-0.8798592 * float(x[23]))+ (2.3480504 * float(x[24]))) + -5.8926272), 0)
    h_5 = max((((-0.5111296 * float(x[0]))+ (0.107826 * float(x[1]))+ (0.037302718 * float(x[2]))+ (-2.6984951 * float(x[3]))+ (1.0592146 * float(x[4]))+ (1.4869007 * float(x[5]))+ (0.74020964 * float(x[6]))+ (1.3549824 * float(x[7]))+ (0.47864148 * float(x[8]))+ (0.07717238 * float(x[9]))+ (-0.62041503 * float(x[10]))+ (-0.15320009 * float(x[11]))+ (1.1290314 * float(x[12]))+ (0.45041278 * float(x[13]))+ (-1.5924629 * float(x[14]))+ (1.8933713 * float(x[15]))+ (-0.09256186 * float(x[16]))+ (0.30477673 * float(x[17]))+ (-0.20800322 * float(x[18]))+ (-0.27408186 * float(x[19]))+ (-1.6955588 * float(x[20]))+ (-0.22316277 * float(x[21]))+ (1.0485436 * float(x[22]))+ (-0.040681474 * float(x[23]))+ (1.1639965 * float(x[24]))) + 0.68213964), 0)
    h_6 = max((((0.054425623 * float(x[0]))+ (-0.07801112 * float(x[1]))+ (-0.2468571 * float(x[2]))+ (-0.39317402 * float(x[3]))+ (-0.6426439 * float(x[4]))+ (-0.78651786 * float(x[5]))+ (0.5049648 * float(x[6]))+ (0.25981894 * float(x[7]))+ (0.35542977 * float(x[8]))+ (-0.33570293 * float(x[9]))+ (0.5389604 * float(x[10]))+ (-0.60831404 * float(x[11]))+ (0.042479753 * float(x[12]))+ (-0.20132317 * float(x[13]))+ (-0.04371282 * float(x[14]))+ (-0.3690106 * float(x[15]))+ (-0.31491765 * float(x[16]))+ (0.07541386 * float(x[17]))+ (0.027270751 * float(x[18]))+ (-0.1341633 * float(x[19]))+ (-0.17880258 * float(x[20]))+ (-0.11857447 * float(x[21]))+ (0.36345255 * float(x[22]))+ (0.58932674 * float(x[23]))+ (0.41871873 * float(x[24]))) + 4.6669354), 0)
    h_7 = max((((1.0324978 * float(x[0]))+ (1.1965268 * float(x[1]))+ (0.77280515 * float(x[2]))+ (0.27497452 * float(x[3]))+ (0.55256426 * float(x[4]))+ (0.35533848 * float(x[5]))+ (-0.09240804 * float(x[6]))+ (-0.30795044 * float(x[7]))+ (0.13937683 * float(x[8]))+ (-0.13313814 * float(x[9]))+ (0.30080038 * float(x[10]))+ (0.32334933 * float(x[11]))+ (-0.24184778 * float(x[12]))+ (0.93318695 * float(x[13]))+ (-0.38298386 * float(x[14]))+ (0.21669243 * float(x[15]))+ (0.052418076 * float(x[16]))+ (0.29223832 * float(x[17]))+ (-0.013485274 * float(x[18]))+ (-0.66302645 * float(x[19]))+ (-0.16519402 * float(x[20]))+ (0.28113127 * float(x[21]))+ (-1.0978574 * float(x[22]))+ (0.12689966 * float(x[23]))+ (-0.84286505 * float(x[24]))) + -5.323165), 0)
    h_8 = max((((0.69294125 * float(x[0]))+ (0.062766016 * float(x[1]))+ (0.6093903 * float(x[2]))+ (-0.29686722 * float(x[3]))+ (0.18441036 * float(x[4]))+ (-0.13129354 * float(x[5]))+ (0.26163858 * float(x[6]))+ (-0.39638144 * float(x[7]))+ (-0.4105321 * float(x[8]))+ (0.15844777 * float(x[9]))+ (0.12454173 * float(x[10]))+ (0.17233211 * float(x[11]))+ (0.18817754 * float(x[12]))+ (-0.105846636 * float(x[13]))+ (-0.24396941 * float(x[14]))+ (-0.11130836 * float(x[15]))+ (0.111213155 * float(x[16]))+ (-0.006097943 * float(x[17]))+ (0.11228152 * float(x[18]))+ (-0.1815497 * float(x[19]))+ (-0.1272553 * float(x[20]))+ (0.53914666 * float(x[21]))+ (-0.6474057 * float(x[22]))+ (-0.02014307 * float(x[23]))+ (-0.19215736 * float(x[24]))) + 6.928645), 0)
    o[0] = (15.622788 * h_0)+ (-35.599064 * h_1)+ (-10.167472 * h_2)+ (-16.959093 * h_3)+ (-15.69563 * h_4)+ (-26.313993 * h_5)+ (10.688268 * h_6)+ (-16.099022 * h_7)+ (-34.837128 * h_8) + 5.841299
    o[1] = (5.4839463 * h_0)+ (1.6621474 * h_1)+ (4.5582833 * h_2)+ (-0.39156368 * h_3)+ (2.0140862 * h_4)+ (2.7233598 * h_5)+ (3.7554562 * h_6)+ (5.979729 * h_7)+ (7.7924843 * h_8) + -20.905167
    o[2] = (-0.748629 * h_0)+ (5.796452 * h_1)+ (0.13728708 * h_2)+ (0.9501637 * h_3)+ (0.48455656 * h_4)+ (1.2905629 * h_5)+ (3.4816527 * h_6)+ (4.378959 * h_7)+ (13.114702 * h_8) + -28.401123
    o[3] = (1.8434584 * h_0)+ (6.991647 * h_1)+ (5.189926 * h_2)+ (4.3900146 * h_3)+ (1.2638044 * h_4)+ (3.0799437 * h_5)+ (3.6916718 * h_6)+ (0.7731585 * h_7)+ (0.5660229 * h_8) + -18.472908
    o[4] = (5.7645264 * h_0)+ (-0.2763646 * h_1)+ (6.374654 * h_2)+ (5.241244 * h_3)+ (0.71242 * h_4)+ (5.785462 * h_5)+ (3.435139 * h_6)+ (5.9458537 * h_7)+ (-10.830616 * h_8) + -14.070772
    o[5] = (4.456524 * h_0)+ (9.203566 * h_1)+ (5.9649596 * h_2)+ (-2.4564185 * h_3)+ (-1.2712456 * h_4)+ (7.818339 * h_5)+ (-15.973888 * h_6)+ (3.282939 * h_7)+ (4.5711246 * h_8) + -6.066612
    o[6] = (-17.873865 * h_0)+ (-36.65739 * h_1)+ (5.722273 * h_2)+ (-17.969196 * h_3)+ (4.2641306 * h_4)+ (-0.9154838 * h_5)+ (3.3298438 * h_6)+ (-4.1748137 * h_7)+ (6.2053957 * h_8) + 5.870583
    o[7] = (7.6604757 * h_0)+ (0.6015432 * h_1)+ (-0.8694572 * h_2)+ (7.3084655 * h_3)+ (-10.845544 * h_4)+ (4.0385466 * h_5)+ (3.5051763 * h_6)+ (5.587141 * h_7)+ (-1.0092542 * h_8) + -17.644167
    o[8] = (0.9407424 * h_0)+ (0.04700577 * h_1)+ (6.4269094 * h_2)+ (10.182937 * h_3)+ (1.2279725 * h_4)+ (4.297512 * h_5)+ (0.7959537 * h_6)+ (-5.3138757 * h_7)+ (2.3106172 * h_8) + 5.08672
    o[9] = (7.3519793 * h_0)+ (7.0102396 * h_1)+ (5.770672 * h_2)+ (1.2172029 * h_3)+ (1.9992875 * h_4)+ (1.1679404 * h_5)+ (0.8533041 * h_6)+ (1.8356054 * h_7)+ (-1.0303391 * h_8) + -7.322043

    

    #Output Decision Rule
    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)


#for classifying batches
def classify(arr):
    outputs = []
    for row in arr:
        outputs.append(single_classify(row))
    return outputs


def Predict(arr,headerless,csvfile, get_key, classmapping):
    with open(csvfile, 'r') as csvinput:
        #readers and writers
        writer = csv.writer(sys.stdout, lineterminator=os.linesep)
        reader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            writer.writerow(','.join(next(reader, None) + ["Prediction"]))
        
        
        for i, row in enumerate(reader):
            #use the transformed array as input to predictor
            pred = str(get_key(int(single_classify(arr[i])), classmapping))
            #use original untransformed line to write out
            row.append(pred)
            writer.writerow(row)


def Validate(arr):
    if n_classes == 2:
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        outputs=[]
        for i, row in enumerate(arr):
            outputs.append(int(single_classify(arr[i, :-1].tolist())))
        outputs=np.array(outputs)
        correct_count = int(np.sum(outputs.reshape(-1) == arr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, arr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, arr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(arr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(arr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0
    else:
        numeachclass = {}
        count, correct_count = 0, 0
        preds = []
        for i, row in enumerate(arr):
            pred = int(single_classify(arr[i].tolist()))
            preds.append(pred)
            if pred == int(float(arr[i, -1])):
                correct_count += 1
                if int(float(arr[i, -1])) in numeachclass.keys():
                    numeachclass[int(float(arr[i, -1]))] += 1
                else:
                    numeachclass[int(float(arr[i, -1]))] = 0
            count += 1
        return count, correct_count, numeachclass, preds
    


# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()


    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}


    #load file
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')


    #Normalize
    cleanarr = Normalize(cleanarr)


    #Transform
    if transform_true:
        if args.validate:
            trans = transform(cleanarr[:, :-1])
            cleanarr = np.concatenate((trans, cleanarr[:, -1].reshape(-1, 1)), axis = 1)
        else:
            cleanarr = transform(cleanarr)


    #Predict
    if not args.validate:
        Predict(cleanarr, args.headerless, preprocessedfile, get_key, classmapping)


    #Validate
    else:
        print("Classifier Type: Neural Network")
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = Validate(cleanarr)
        else:
            count, correct_count, numeachclass, preds = Validate(cleanarr)
            #Correct Labels
            true_labels = cleanarr[:, -1]


        #Report Metrics
        model_cap = 334
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
            print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
            print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
            print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
            if int(num_TP + num_FN) != 0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN + num_FP) != 0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP + num_FP) != 0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2 * num_TP + num_FP + num_FN) != 0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP + num_FN) != 0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP + num_FN + num_FP) != 0:
                print("Critical Success Index:             {:.2f}".format(TS))

        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            print("System Type:                        " + str(n_classes) + "-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            try:
                import numpy as np # For numpy see: http://numpy.org
                from numpy import array
            except:
                print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

            def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
                #check for numpy/scipy is imported
                try:
                    from scipy.sparse import coo_matrix #required for multiclass metrics
                except:
                    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                    sys.exit()
                # Compute confusion matrix to evaluate the accuracy of a classification.
                # By definition a confusion matrix :math:C is such that :math:C_{i, j}
                # is equal to the number of observations known to be in group :math:i and
                # predicted to be in group :math:j.
                # Thus in binary classification, the count of true negatives is
                # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
                # :math:C_{1,1} and false positives is :math:C_{0,1}.
                # Read more in the :ref:User Guide <confusion_matrix>.
                # Parameters
                # ----------
                # y_true : array-like of shape (n_samples,)
                # Ground truth (correct) target values.
                # y_pred : array-like of shape (n_samples,)
                # Estimated targets as returned by a classifier.
                # labels : array-like of shape (n_classes), default=None
                # List of labels to index the matrix. This may be used to reorder
                # or select a subset of labels.
                # If None is given, those that appear at least once
                # in y_true or y_pred are used in sorted order.
                # sample_weight : array-like of shape (n_samples,), default=None
                # Sample weights.
                # normalize : {'true', 'pred', 'all'}, default=None
                # Normalizes confusion matrix over the true (rows), predicted (columns)
                # conditions or all the population. If None, confusion matrix will not be
                # normalized.
                # Returns
                # -------
                # C : ndarray of shape (n_classes, n_classes)
                # Confusion matrix.
                # References
                # ----------
                if labels is None:
                    labels = np.array(list(set(list(y_true.astype('int')))))
                else:
                    labels = np.asarray(labels)
                    if np.all([l not in y_true for l in labels]):
                        raise ValueError("At least one label specified must be in y_true")


                if sample_weight is None:
                    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
                else:
                    sample_weight = np.asarray(sample_weight)
                if y_true.shape[0]!=y_pred.shape[0]:
                    raise ValueError("y_true and y_pred must be of the same length")

                if normalize not in ['true', 'pred', 'all', None]:
                    raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


                n_labels = labels.size
                label_to_ind = {y: x for x, y in enumerate(labels)}
                # convert yt, yp into index
                y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
                y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
                # intersect y_pred, y_true with labels, eliminate items not in labels
                ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
                y_pred = y_pred[ind]
                y_true = y_true[ind]
                # also eliminate weights of eliminated items
                sample_weight = sample_weight[ind]
                # Choose the accumulator dtype to always have high precision
                if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                    dtype = np.int64
                else:
                    dtype = np.float64
                cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


                with np.errstate(all='ignore'):
                    if normalize == 'true':
                        cm = cm / cm.sum(axis=1, keepdims=True)
                    elif normalize == 'pred':
                        cm = cm / cm.sum(axis=0, keepdims=True)
                    elif normalize == 'all':
                        cm = cm / cm.sum()
                    cm = np.nan_to_num(cm)
                return cm


            print("Confusion Matrix:")
            mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #Clean Up
    if not args.cleanfile:
        os.remove(cleanfile)
        os.remove(preprocessedfile)
