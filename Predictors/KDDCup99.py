#!/usr/bin/env python3
#
# This code is was produced by an alpha version of Brainome Daimensions(tm) and is
# licensed under GNU GPL v2.0 or higher. For details, please see:
# https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html
#
#
# Output of Brainome Daimensions(tm) 0.96 Table Compiler v0.96.
# Invocation: btc https://www.openml.org/data/get_csv/53996/KDDCup99.arff -o Predictors/KDDCup99_QC.py -target label -stopat 99.93 -f QC -e 100 --yes --runlocalonly
# Total compiler execution time: 0:02:23.75. Finished on: May-23-2020 06:47:19.
# This source code requires Python 3.
#
"""
Classifier Type: Quick Clustering
System Type:                        23-way classifier
Best-guess accuracy:                56.84%
Model accuracy:                     99.97% (493912/494020 correct)
Improvement over best guess:        43.13% (of possible 43.16%)
Model capacity (MEC):               2175 bits
Generalization ratio:               227.08 bits/bit
Confusion Matrix:
 [19.68% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.01% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 21.70% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 56.84% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.01% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.05% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.20% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.21% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.01% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.24% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.45% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.32% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.05% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.21% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]
 [0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%
  0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00% 0.00%]

"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "KDDCup99.csv"


#Number of attributes
num_attr = 41
n_classes = 23


# Preprocessor for CSV files
def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    il=[]
    
    ignorelabels=[]
    ignorecolumns=[]
    target="label"


    if (testfile):
        target = ''
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if (target != ''): 
                        hc = header.index(target)
                    else:
                        hc = len(header) - 1
                        target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il=il+[col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                for i in range(0, len(header)):      
                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    print(header[i] + ",", end='', file=outputfile)
                print(header[hc], file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if (row[target] in ignorelabels):
                        continue
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name==target):
                            continue
                        if (',' in row[name]):
                            print ('"' + row[name] + '"' + ",", end='', file=outputfile)
                        else:
                            print (row[name] + ",", end='', file=outputfile)
                    print (row[target], file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc =- 1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    if (hc == -1):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if (',' in row[i]):
                            print ('"' + row[i] + '"'+",", end='', file=outputfile)
                        else:
                            print(row[i]+",", end = '', file=outputfile)
                    print (row[hc], file=outputfile)

def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    
    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'normal': 0, 'buffer_overflow': 1, 'loadmodule': 2, 'perl': 3, 'neptune': 4, 'smurf': 5, 'guess_passwd': 6, 'pod': 7, 'teardrop': 8, 'portsweep': 9, 'ipsweep': 10, 'land': 11, 'ftp_write': 12, 'back': 13, 'imap': 14, 'satan': 15, 'phf': 16, 'nmap': 17, 'multihop': 18, 'warezmaster': 19, 'warezclient': 20, 'spy': 21, 'rootkit': 22}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    # function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")

    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result

    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping

# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([3423724706.4799995, 3423728640.9900002, 3581096047.515, 3581096067.9749994, 3581096095.7249994, 3581096165.105, 3581096167.1, 3581096199.105, 3581096200.105, 3581096221.66, 3581096549.13, 3581098193.79, 3581109988.705, 3665478035.1499996, 3749629639.36, 4214602298.3900003, 4394881190.6, 4394881770.795, 4394881776.115, 4394883273.03, 4394883273.355, 4452228249.43, 4509458416.870001, 4521704677.184999, 4541486828.799999, 4552252417.934999, 4552252601.76, 4552252731.23, 4552252788.605, 4552252874.615, 4552252991.545, 4552253321.004999, 4552253583.74, 4552253794.805, 4552253825.875, 4552253857.925, 4552253885.12, 4552253964.57, 4552254053.985001, 4552254608.525, 4552254650.23, 4552254759.395, 4552254793.385, 4552254995.26, 4552255347.045, 4552255507.63, 4552255586.55, 4552256716.684999, 4552256879.775, 4552256920.135, 4552257026.96, 4552257104.625, 4552257163.805, 4552257170.555, 4552257202.32, 4552258518.29, 4552259350.555, 4552260530.145, 4552260851.514999, 4552261628.27, 4552261671.52, 4552261951.535, 4552262260.014999, 4552262351.67, 4552275291.195, 4552276047.535, 4552278120.015, 4552279182.065001, 4552279812.059999, 4552280371.225, 4552322056.555, 4552325647.83, 4655281932.285001, 4804568776.955, 4890549181.585, 4971784648.955, 5017155696.875, 5050988742.185, 5112570945.47, 5157942682.965, 5174527205.65, 5192243030.68, 5209920927.610001, 5218690492.040001, 5231480711.325001, 5259320526.355, 5333991623.795, 5400172411.915, 5415461636.08, 5447971245.960001, 5491179041.85, 5505290209.365, 5508703007.900001, 5513448484.01, 5518194039.07, 5659988883.119999, 5675382146.065, 5692238911.879999, 5705728852.51, 5717714848.135, 5717714864.650001, 5758660407.369999, 5783242104.98, 5783242128.05, 5783242134.88, 5783242136.82, 5783242139.78, 5783242143.539999, 5783242156.405001, 5783242158.405001, 5783242160.715, 5783242169.615, 5783242172.585, 5783242175.505, 5783242186.795, 5783242187.875, 5783242340.934999, 5783242341.58, 5783242342.094999, 5783242342.59, 5783242345.985001, 5783242346.43, 5783242347.745, 5783242348.175, 5783242348.92, 5783242349.215, 5783242354.735001, 5783242354.825001, 5783242355.014999, 5783242355.26, 5783242356.82, 5783242357.285, 5783242359.485, 5783242360.35, 5783242362.054999, 5783242362.205, 5783242362.77, 5783242363.380001, 5783242363.93, 5783242364.594999, 5783242365.065001, 5783242365.415, 5783242366.87, 5783242367.365, 5783242368.940001, 5783242369.254999, 5783242371.07, 5783242372.21, 5783242374.280001, 5783242375.095, 5783242376.21, 5783242376.434999, 5783242379.12, 5783242379.35, 5783242382.834999, 5783242383.16, 5783242392.96, 5783242393.225, 5783242393.84, 5783242394.33, 5783242417.530001, 5783242418.094999, 5783242419.815001, 5783242420.304999, 5783242420.639999, 5783242420.7, 5783242421.96, 5783242422.485, 5783242422.585, 5783242422.67, 5783242423.805, 5783242424.065001, 5783242424.21, 5783242424.345, 5783242424.42, 5783242424.66, 5783242425.094999, 5783242425.715, 5783242425.915, 5783242426.334999, 5783242426.405001, 5783242427.025, 5783242427.87, 5783242428.075001, 5783242428.195, 5783242428.5, 5783242428.835, 5783242429.23, 5783242429.295, 5783242429.305, 5783242429.32, 5783242429.575, 5783242429.91, 5783242430.02, 5783242430.095, 5783242430.225, 5783242430.24, 5783242430.559999, 5783242431.639999, 5783242432.530001, 5783242433.0, 5783242433.515, 5783242433.88, 5783242434.56, 5783242435.11, 5783242436.38, 5783242437.004999, 5783242438.055, 5783242438.115, 5783242438.67, 5783242439.8, 5783242440.395, 5783242440.825, 5783242441.15, 5783242441.74, 5783242442.235, 5783242442.860001, 5783242444.365, 5783242444.95, 5783242445.615, 5783242446.05, 5783242446.219999, 5783242446.245, 5783242446.4800005, 5783242446.99, 5783242447.264999, 5783242448.05, 5783242448.12, 5783242448.13, 5783242448.389999, 5783242448.995, 5783242449.275, 5783242449.594999, 5783242450.195, 5783242450.225, 5783242451.309999, 5783242451.865, 5783242453.450001, 5783242453.91, 5783242454.31, 5783242455.83, 5783242456.205, 5783242456.355, 5783242456.495, 5783242456.945, 5783242457.6, 5783242458.055, 5783242458.205, 5783242458.495, 5783242458.799999, 5783242458.965, 5783242459.185, 5783242459.635, 5783242459.684999, 5783242459.88, 5783242461.355, 5783242461.695, 5783242462.200001, 5783242462.865, 5783242463.344999, 5783242463.76, 5783242464.255, 5783242465.075001, 5783242465.25, 5783242465.8949995, 5783242467.494999, 5783242467.98, 5783242468.14, 5783242468.145, 5783242468.235001, 5783242468.73, 5783242469.164999, 5783242470.005, 5783242470.175, 5783242470.735, 5783242472.2699995, 5783242473.09, 5783242473.21, 5783242474.045, 5783242474.17, 5783242474.895, 5783242475.44, 5783242475.99, 5783242476.309999, 5783242477.085, 5783242477.094999, 5783242477.1, 5783242477.33, 5783242478.045, 5783242478.425, 5783242479.0, 5783242479.29, 5783242479.875, 5783242480.309999, 5783242481.139999, 5783242481.18, 5783242482.89, 5783242483.545, 5783242484.155, 5783242484.275, 5783242487.335, 5783242488.389999, 5783242491.275001, 5783242491.84, 5783242494.355, 5783242494.405001, 5783242496.985001, 5783242497.375, 5783242646.67, 5783242646.8949995, 5783242649.445001, 5783242650.51, 5783242652.365, 5783242652.715, 5783242653.305, 5783242653.75, 5783242665.795, 5783242666.37, 5783242717.514999, 5783242718.215, 5783242720.23, 5783242720.505, 5783242721.735001, 5783242722.055, 5783242723.345, 5783242723.535, 5783242725.6, 5783242726.125, 5783242728.93, 5783242730.295, 5783242737.49, 5783242737.715, 5783242737.765, 5783242738.065001, 5783242739.395, 5783242740.264999, 5783242740.764999, 5783242740.835001, 5783242742.4, 5783242743.025, 5783242752.795, 5783242753.045, 5783242753.535, 5783242753.835, 5783242754.725, 5783242755.115, 5783242756.684999, 5783242758.219999, 5783242762.4800005, 5783242763.424999, 5783242764.725, 5783242765.075, 5783242768.28, 5783242769.219999, 5783242772.765, 5783242773.190001, 5783242782.450001, 5783242783.175, 5783242931.43, 5783242932.455, 5783242934.795, 5783242935.07, 5783242935.33, 5783242936.190001, 5783242936.41, 5783242937.825, 5783242938.335, 5783242939.585, 5783242940.785, 5783242942.86, 5783242943.655001, 5783242948.855, 5783242949.09, 5783242949.48, 5783242951.02, 5783242951.745001, 5783242952.96, 5783242953.115, 5783242954.014999, 5783242954.65, 5783242956.085, 5783242956.275, 5783242958.094999, 5783242958.3949995, 5783242959.0, 5783242959.2699995, 5783242960.855, 5783242961.309999, 5783242962.13, 5783242963.135, 5783242967.06, 5783242967.110001, 5783242970.21, 5783242970.685, 5783242973.11, 5783242973.855, 5783242976.9, 5783242977.385, 5783242980.405001, 5783242980.88, 5783242988.68, 5783242989.365, 5783243235.295, 5783243235.969999, 5783243240.66, 5783243241.219999, 5783243256.135, 5783243256.445, 5783244166.785, 5783244168.825, 5783244171.41, 5783244247.82, 5783244248.33, 5783247091.34, 5783247107.709999, 5783247110.789999, 5783247116.514999, 5783247189.88, 5783247192.835, 5783247763.99, 5783247766.809999, 5783247772.425, 5783247778.385, 5783247851.8, 5783247868.885, 5783247912.795, 5783247924.815, 5783248006.95, 5783248043.684999, 5783248088.385, 5783248098.43, 5783249145.305, 5783249150.52, 5783249153.41, 5783249173.63, 5783249175.139999, 5783249182.255, 5783249185.72, 5783249189.710001, 5783249190.795, 5783249191.535, 5783249194.155001, 5783249195.2699995, 5783249197.224999, 5783249205.305, 5783249208.055, 5783249209.82, 5783249263.549999, 5783249264.889999, 5783249453.155001, 5783249457.38, 5783249460.85, 5783249465.795, 5783249477.63, 5783249479.385, 5783563935.395, 5783726999.969999, 5783822864.365, 5784065802.845, 5784315655.5, 5784873953.094999, 5786907834.08, 5788381509.35, 5788382241.440001, 5788387392.895, 5810077490.775, 5889448934.379999, 5947305635.84, 5947305712.545, 5947350652.469999, 5947660220.455, 5957512325.855, 6000739204.775001, 6047339592.895, 6060273642.05, 6065406294.935, 6072271715.875, 6091408478.385, 6134759749.48, 6173923243.35, 6187139439.66, 6187139525.47, 6187139620.780001, 6187146778.425, 6192786811.4, 6202532338.66, 6206644723.575001, 6206644725.02, 6206644727.05, 6206644728.045, 6207619170.73, 6208812513.38, 6209031378.395, 6223775898.535, 6241198350.845, 6251027360.580001, 6258097123.045, 6280879677.525, 6334896741.785, 6370819448.49, 6396067728.61, 6406789134.315001, 6429429964.445, 6611999720.1449995, 6629139928.165, 6642002360.73, 6642002540.325, 6645147733.035, 6655130284.945, 6661932246.985001, 6661932646.125, 6671072677.855, 6684834066.58, 6710666986.54, 6735016143.715, 6749633400.705, 6749633400.975, 6780627272.325001, 6781490084.17, 6782352762.825001, 6782352907.43, 6782353189.11, 6815738724.565001, 6853936181.74, 6867305461.815001, 6875863053.235001, 6875871535.120001, 6875871537.014999, 6875871549.495, 6875871553.755, 6875925686.035, 6875925877.115001, 6875925877.285001, 6875926289.805, 6883458702.105, 6888183347.139999, 6898999602.030001, 6910949673.75, 6913130397.63, 6961011671.799999, 7009404212.74, 7031215343.27, 7059413996.625, 7066313564.575001, 7073141266.360001, 7086606939.735001, 7120379586.969999, 7147514202.014999, 7152905487.880001, 7158296182.045, 7158296250.92, 7158296323.745, 7158296438.51, 7158296560.040001, 7158296651.155001, 7158296700.125, 7158296751.025, 7158296794.695, 7158299527.26, 7158299530.695, 7158299536.175, 7158299540.265, 7158299541.27, 7158299543.34, 7158299563.235001, 7158299577.29, 7158299578.245, 7158299581.245001, 7158299584.255001, 7158299589.245001, 7158299593.76, 7158299601.25, 7158299607.305, 7158299613.795, 7158299615.690001, 7158299619.735001, 7158299621.24, 7158299695.650001, 7158299698.14, 7158299754.245, 7158299756.275, 7158299762.295001, 7158299764.27, 7158299772.79, 7158299773.735, 7158299784.764999, 7158299785.745, 7158299788.805, 7158299789.210001, 7158299811.595, 7158299812.135, 7158299952.04, 7158303121.235, 7172827117.285, 7189051861.2, 7190725630.915, 7190725640.73, 7190725668.835, 7190725675.610001, 7190725688.725, 7190725705.150001, 7190725725.88, 7190725748.65, 7190725786.365, 7190725787.834999, 7190725791.125, 7190725824.095, 7190725830.415, 7190725833.13, 7190725840.225, 7213155041.69, 7235584081.155, 7239995333.775, 7264314287.57, 7285605193.355, 7285605417.665, 7285605424.13, 7285605556.139999, 7310183034.605, 7321219043.13, 7329853587.309999, 7340131832.424999, 7348096949.495, 7348097201.400001, 7348097213.940001, 7348097395.235001, 7348097398.195, 7356923357.965, 7365749294.58, 7386573313.49, 7427786457.280001, 7452439279.940001, 7465362933.64, 7498144858.135, 7498178418.245001, 7498178659.5, 7506324692.325001, 7525547725.14, 7548240942.505, 7562017490.0, 7562017491.0, 7562017511.565001, 7631951389.120001, 7705163805.594999, 7708496094.440001, 7708534831.7, 7751783574.965, 7751783604.27, 7751783638.475, 7751783653.51, 7751783810.84, 7751783811.295, 7751783812.95, 7751783813.71, 7766177416.309999, 7802781250.235001, 7841642927.139999, 7862783302.475, 7871123069.645, 7874973377.755001, 7874973378.01, 7874973572.5, 7874973572.5, 7874973687.6050005, 7874973688.540001, 7874973775.93, 7874973779.425, 7874973785.450001, 7874973786.450001, 7874973791.9800005, 7874973795.52, 7874973802.93, 7874973821.615, 7874973824.440001, 7874973991.615, 7893137181.595, 7969256757.190001, 7981072083.275001, 7981072157.7300005, 7981072391.555, 7981072567.165, 7983686683.14, 7988546030.379999, 7993016482.709999, 8012437206.635, 8019108522.0, 8019108526.0, 8019108531.0, 8019108533.0, 8019108548.0, 8019108548.0, 8019108552.0, 8019108553.0, 8019109528.5, 8019109986.5, 8019109996.27, 8019109997.77, 8019110016.525001, 8019110017.025001, 8019110018.0, 8019110018.094999, 8019110024.0, 8019110024.0, 8019110026.405, 8019110027.309999, 8019110041.0, 8019110076.535, 8019110077.535, 8019110080.545, 8019110081.545, 8019110083.555, 8019110085.555, 8019110088.575001, 8019110089.575001, 8019110092.58, 8019110094.58, 8019110096.59, 8019110097.59, 8019110100.6, 8019110105.610001, 8019110108.62, 8019110109.62, 8019110111.63, 8019110114.63, 8019110117.07, 8019110117.639999, 8019110119.639999, 8019110121.639999, 8019110124.65, 8019110126.65, 8019110128.66, 8019110129.66, 8019110131.67, 8019110134.67, 8019110136.68, 8019110137.68, 8019110139.690001, 8019110141.690001, 8019110144.7, 8019110146.17, 8019110147.33, 8019110147.905001, 8019110148.27, 8019110149.905001, 8019110150.1, 8019110151.6, 8019110152.094999, 8019110153.969999, 8019110154.43, 8019110155.865, 8019110156.360001, 8019110158.360001, 8019110245.51, 8019110247.51, 8019110248.360001, 8019110249.52, 8019110250.02, 8019110251.855, 8019110252.03, 8019110255.53, 8019110256.03, 8019110259.53, 8019110260.360001, 8019110265.54, 8019110266.365, 8019110267.174999, 8019110268.04, 8019110271.549999, 8019110272.375, 8019110273.549999, 8019110274.049999, 8019110275.87, 8019110276.059999, 8019110277.549999, 8019110278.38, 8019110280.23, 8019110281.045, 8019110283.559999, 8019110284.375, 8019110285.57, 8019110286.059999, 8019110291.58, 8019110292.07, 8019110293.889999, 8019110294.08, 8019110297.235, 8019110297.735, 8019110299.885, 8019110300.09, 8019110303.59, 8019110304.09, 8019110305.889999, 8019110306.1, 8019110307.1, 8029277243.365, 8043608258.85, 8082761632.14, 8086802008.120001, 8115443678.635, 8145354776.09, 8165007072.745, 8192446541.695, 8208533440.645, 8231588560.045, 8315867228.76, 8335464115.075001, 8348203235.88, 8359490258.175, 8359490266.15, 8359490266.65, 8359490274.139999, 8359490274.389999, 8359490274.764999, 8359490275.0, 8359490275.0, 8359490275.0, 8359490275.0, 8359490275.5, 8359490277.0, 8359490277.0, 8359490278.125, 8359490278.415, 8359490278.79, 8359490279.0, 8359490279.0, 8359490279.135, 8359490281.0, 8359490281.0, 8359490281.0, 8359490281.125, 8359490282.75, 8359490283.25, 8359490284.77, 8359490285.0, 8359490285.0, 8359490285.0, 8359490285.0, 8359490285.125, 8359490288.835, 8359490289.13, 8359490289.265, 8359490292.76, 8359490293.25, 8359490294.76, 8359490295.0, 8359490295.0, 8359490295.0, 8359490297.0, 8359490297.13, 8359490298.5, 8359490299.0, 8359490299.0, 8359490299.13, 8359490299.265, 8359490299.385, 8359490301.0, 8359490301.0, 8359490301.0, 8359490301.0, 8359490302.125, 8359490302.385, 8359490302.835, 8359490303.0, 8359490303.0, 8359490303.255, 8359490304.125, 8359490304.375, 8359490304.76, 8359490305.0, 8359490305.0, 8359490305.0, 8359490305.0, 8359490305.0, 8359490308.265, 8359490308.4, 8359490308.764999, 8359490309.255, 8359490310.335, 8359490310.455, 8359490312.809999, 8359490313.5, 8359490314.125, 8359490314.375, 8359490314.75, 8359490315.0, 8359490315.0, 8359490315.0, 8359490315.0, 8359490315.125, 8359490315.255, 8359490315.38, 8359490316.764999, 8359490317.0, 8359490318.13, 8359490318.63, 8359490319.125, 8359490320.75, 8359490321.0, 8359490321.0, 8359490321.0, 8359490321.0, 8359490321.0, 8359490321.265, 8359490321.385, 8359490322.780001, 8359490323.13, 8359490323.385, 8359490324.125, 8359490324.385, 8359490324.76, 8359490325.0, 8359490325.0, 8359490325.0, 8359490325.0, 8359490325.075, 8359490325.205, 8359490325.265, 8359490325.385, 8359490326.125, 8359490326.375, 8359490329.0, 8359490329.0, 8359490329.915001, 8359490330.385, 8359490332.755, 8359490333.125, 8359490333.25, 8359490333.385, 8359490334.5, 8359490335.0, 8359490335.0, 8359490335.125, 8359490335.625, 8359490336.2, 8359490336.82, 8359490337.0, 8359490337.125, 8359490338.580001, 8359490339.125, 8359490339.25, 8359490341.0, 8359490341.13, 8359490341.88, 8359490342.385, 8359490342.755, 8359490343.25, 8359490344.75, 8359490345.13, 8359490346.13, 8359490346.385, 8359490347.26, 8359490347.63, 8359490348.13, 8359490348.63, 8359490349.25, 8359490351.125, 8359490351.385, 8359490352.755, 8359490353.125, 8359490353.255, 8359490353.38, 8359490353.89, 8359490354.385, 8359490354.755, 8359490355.25, 8359490356.125, 8359490356.375, 8359490356.764999, 8359490357.125, 8359490358.125, 8359490358.625, 8359490359.125, 8359490359.88, 8359490360.38, 8359490361.0, 8359490361.0, 8359490361.285, 8359490361.405001, 8359490362.255, 8359490363.25, 8359490364.25, 8359490365.125, 8359490365.25, 8359490365.255, 8359490365.385, 8359490366.13, 8359490366.385, 8359490366.89, 8359490367.26, 8359490368.75, 8359490369.495, 8359490369.885, 8359490370.38, 8359490372.755, 8359490373.125, 8359490374.255, 8359490375.255, 8359490376.755, 8359490377.125, 8359490378.255, 8359490379.125, 8359490379.275, 8359490379.465, 8359490379.88, 8359490380.38, 8359490380.76, 8359490381.0, 8359490381.25, 8359490381.38, 8359490381.88, 8359490382.375, 8359490382.755, 8359490383.25, 8359490384.255, 8359490385.125, 8359490385.25, 8359490388.755, 8359490389.13, 8359490389.315001, 8359490389.434999, 8359490389.875, 8359490390.38, 8359490390.885, 8359490391.385, 8359490392.255, 8359490393.125, 8359490393.365, 8359490393.49, 8359490394.255, 8359490395.25, 8359490396.125, 8359490396.375, 8359490396.75, 8359490397.125, 8359490397.915, 8359490399.125, 8359490399.25, 8359490399.705, 8359490400.33, 8359490400.75, 8359490401.125, 8359490402.255, 8359490403.75, 8359490404.75, 8359490405.125, 8359490405.25, 8359490405.255, 8359490405.38, 8359490406.625, 8359490407.255, 8359490407.885, 8359490408.385, 8359490408.755, 8359490409.690001, 8359490410.875, 8359490411.875, 8359490412.375, 8359490412.755, 8359490413.125, 8359490413.344999, 8359490413.844999, 8359490414.625, 8359490415.25, 8359490416.75, 8359490417.125, 8359490417.73, 8359490419.125, 8359490419.88, 8359490420.38, 8359490420.755, 8359490421.125, 8359490421.75, 8359490422.375, 8359490422.75, 8359490423.66, 8359490424.375, 8359490424.75, 8359490425.125, 8359490427.7, 8359490428.325, 8359490428.88, 8359490429.38, 8359490430.255, 8359490431.125, 8359490431.325, 8359490433.125, 8359490433.875, 8359490434.625, 8359490435.25, 8359490436.145, 8359490437.125, 8359490437.805, 8359490438.305, 8359490438.75, 8359490439.125, 8359490439.75, 8359490440.625, 8359490441.125, 8359490441.309999, 8359490441.434999, 8359490442.25, 8359490443.75, 8359490444.75, 8359490445.125, 8359490445.25, 8359490445.625, 8359490447.645, 8359490448.145, 8359490448.375, 8359490448.75, 8359490450.125, 8359490451.29, 8359490451.415, 8359490452.75, 8359490453.125, 8359490453.875, 8359490454.625, 8359490455.5, 8359490456.125, 8359490456.375, 8359490456.75, 8359490457.125, 8359490457.75, 8359490458.625, 8359490459.125, 8359490459.665, 8359490460.165, 8359490460.625, 8359490461.125, 8359490461.25, 8359490461.27, 8359490462.125, 8359490462.375, 8359490462.75, 8359490463.25, 8359490463.780001, 8359490464.155001, 8359490464.625, 8359490465.125, 8359490466.125, 8359490466.875, 8359490468.75, 8359490473.125, 8359490473.25, 8359490473.375, 8359490474.25, 8359490475.5, 8359490476.125, 8359490476.625, 8359490479.125, 8359490479.25, 8359490479.875, 8359490480.625, 8359490481.23, 8359490481.75, 8359490482.125, 8359490482.625, 8359490483.125, 8359490483.875, 8359490484.75, 8359490485.125, 8359490486.530001, 8359490489.75, 8359490490.84, 8359490491.715, 8359490492.625, 8359490495.5, 8359490496.5, 8359490497.125, 8359490497.25, 8359490497.29, 8359490499.125, 8359490499.875, 8359490500.75, 8359490502.25, 8359490502.75, 8359490503.125, 8359490503.625, 8359490505.125, 8359490508.875, 8359490509.375, 8359490510.75, 8359490511.3, 8359490512.75, 8359490513.125, 8359490513.31, 8359490513.435, 8359490513.875, 8359490514.375, 8359490514.75, 8359490515.25, 8359490516.125, 8359490516.375, 8359490517.0, 8359490517.0, 8359490517.01, 8359490517.135, 8359490518.0, 8359490518.0, 8359490518.875, 8359490519.375, 8359490520.75, 8359490521.155001, 8359490521.375, 8359490521.75, 8359490522.125, 8359490522.75, 8359490523.25, 8359490523.75, 8359490524.25, 8359490524.75, 8359490525.25, 8359490527.765, 8359490528.455, 8359490528.765, 8359490529.32, 8359490529.52, 8359490530.335, 8359490530.52, 8359490531.47, 8359490531.525001, 8359490532.365, 8359490532.525001, 8359490533.07, 8359490533.63, 8359490534.335, 8359490534.525001, 8359490535.25, 8359490535.515, 8359490536.025001, 8359490536.55, 8359490537.08, 8359490537.75, 8359490538.25, 8359490540.655001, 8359490541.030001, 8359490541.375, 8359490542.875, 8359490543.375, 8359490544.75, 8359490545.25, 8359490546.835, 8359490547.21, 8359490547.375, 8359490548.75, 8359490549.25, 8359490550.530001, 8359490551.030001, 8359490551.875, 8359490552.375, 8359490552.555, 8359490553.055, 8359490555.88, 8359490556.455, 8359490557.955, 8359490558.455, 8359490558.620001, 8359490559.120001, 8359490559.875, 8359490560.375, 8359490561.495, 8382199817.094999, 8417999112.049999, 8434912925.82, 8478746406.93, 8495294752.805, 8513860689.645, 8523166188.3550005, 8557346960.164999, 8570604291.325001, 8581724622.620001, 8593854403.175, 8624609773.5, 8710848136.630001, 8710848156.155, 8810775626.79, 8847642075.66, 8857404162.14, 8857404189.27, 8857404200.355, 8857404213.080002, 8857404236.84, 8857404253.085001, 8857404262.505001, 8857404272.529999, 8857404273.985, 8857404281.775, 8857404289.82, 8857404296.46, 8857404313.495, 8857404318.065002, 8857404319.975, 8857404320.125, 8857404325.205, 8857404328.244999, 8857404333.77, 8857404339.875, 8857404342.994999, 8857404346.075, 8857404353.3, 8857404354.125, 8857404357.02, 8857404358.425, 8857404366.475, 8857404366.990002, 8857404367.265, 8857404367.395, 8857404367.579998, 8857404368.465, 8857404369.36, 8857404369.85, 8857404377.345001, 8857404377.975, 8857404386.545, 8857404387.035, 8857404387.445, 8857404387.585, 8857404387.75, 8857404388.005001, 8857404393.625, 8857404394.029999, 8857404397.515, 8857404398.064999, 8857404403.650002, 8857404404.035, 8857404406.625, 8857404407.025, 8857404407.759998, 8857404408.064999, 8857404413.73, 8857404414.06, 8857404417.56, 8857404418.095, 8857404423.73, 8857404424.099998, 8857404426.59, 8857404426.855, 8857404427.05, 8857404427.8, 8857404428.125, 8857404438.16, 8857404438.16, 8857404438.65, 8857404439.0, 8857404444.2, 8857404444.2, 8857404446.715, 8857404447.064999, 8857404448.210001, 8857404448.215, 8857404448.67, 8857404449.035, 8857404457.725, 8857404458.1, 8857404458.289999, 8857404458.289999, 8857404458.7, 8857404459.08, 8857404467.759998, 8857404468.16, 8857404468.349998, 8857404468.36, 8857404468.73, 8857404469.105, 8857404478.240002, 8857404478.245, 8857404478.404999, 8857404478.79, 8857404479.15, 8857404479.16, 8857404484.43, 8857404484.814999, 8857404487.29, 8857404487.295, 8857404488.32, 8857404488.32, 8857404489.195, 8857404489.220001, 8857404498.375, 8857404498.849998, 8857404499.300001, 8857404499.305, 8857404508.42, 8857404508.875, 8857404509.355, 8857404509.77, 8857404518.425, 8857404518.9, 8857404519.39, 8857404519.8, 8857404528.48, 8857404528.935, 8857404529.43, 8857404529.85, 8857404539.445, 8857404539.88, 8857404548.59, 8857404548.99, 8857404549.505, 8857404549.935, 8857404551.544998, 8857404551.954998, 8857404554.615, 8857404555.035, 8857404558.61, 8857404559.02, 8857404559.539999, 8857404559.965, 8857404561.59, 8857404562.005, 8857404568.63, 8857404569.05, 8857404569.57, 8857404570.0, 8857404571.6, 8857404572.02, 8857404574.68, 8857404575.1, 8857404578.605, 8857404579.064999, 8857404579.605, 8857404580.02, 8857404584.675, 8857404585.1, 8857404588.599998, 8857404589.029999, 8857404589.61, 8857404590.03, 8857404594.675, 8857404595.105, 8857404598.61, 8857404599.07, 8857404599.605, 8857404600.04, 8857404608.61, 8857404609.09, 8857404609.599998, 8857404610.05, 8857404614.67, 8857404615.125, 8857404618.599998, 8857404619.09, 8857404619.614998, 8857404620.07, 8857404628.619999, 8857404629.09, 8857404629.609999, 8857404630.055, 8857404634.665, 8857404635.115002, 8857404638.635, 8857404639.09, 8857404639.609999, 8857404639.974998, 8857404640.125, 8857404648.645, 8857404649.115, 8857404649.650002, 8857404650.130001, 8857404658.66, 8860226288.335, 8867860236.215, 8926427461.345001, 8980182830.005001, 8991336300.205002, 9002489503.775, 9043347822.355, 9065283917.655, 9095377959.76, 9108416312.17, 9108416345.355, 9108416347.365, 9108416349.715, 9108416351.725, 9108416353.195, 9108416355.205002, 9108416384.67, 9108416385.78, 9108416387.54, 9108416388.75, 9108416390.759998, 9108416405.205, 9108416407.215, 9108416408.744999, 9108416410.755001, 9108416412.93, 9108416413.68, 9108416415.3, 9108416421.8, 9108416423.8, 9108416429.3, 9108416431.31, 9108416449.535, 9108416450.5, 9108416452.645, 9108416463.55, 9108416464.355, 9108416467.53, 9108416468.345001, 9108416468.869999, 9108416469.5, 9108416470.205, 9108416471.245, 9108416472.05, 9108416472.84, 9108416473.135, 9108416474.380001, 9108416475.685001, 9108416476.32, 9108416476.57, 9108416476.75, 9108416476.880001, 9108416477.715, 9108416479.295, 9108416480.755001, 9108416481.025002, 9108416481.390001, 9108416482.329998, 9108416483.165, 9108416484.529999, 9108416484.824999, 9108416486.285, 9108416487.630001, 9108416488.11, 9108416488.735, 9108416489.02, 9108416489.509998, 9108416490.099998, 9108416490.525, 9108416490.635, 9108416491.080002, 9108416491.54, 9108416492.740002, 9108416492.805, 9108416492.985, 9108416493.435001, 9108416494.075, 9108416494.68, 9108416495.130001, 9108416495.400002, 9108416496.45, 9108416497.18, 9108416497.895, 9108416498.275002, 9108416498.655003, 9108416499.035, 9108416499.42, 9108416499.975, 9108416500.175, 9108416500.775002, 9108416501.01, 9108416501.6, 9108416501.935, 9108416502.45, 9108416502.725, 9108416503.095001, 9108416503.61, 9108416504.11, 9108416504.585, 9108416504.91, 9108416505.02, 9108416505.119999, 9108416505.24, 9108416505.545002, 9108416505.885, 9108416506.044998, 9108416506.285, 9108416506.625, 9108416507.055, 9108416507.154999, 9108416507.244999, 9108416507.654999, 9108416508.045, 9108416508.115, 9108416508.665, 9108416508.95, 9108416509.625, 9108416509.985, 9108416510.135, 9108416510.3, 9108416510.705, 9108416511.005001, 9108416511.935001, 9108416512.435001, 9108416512.895, 9108416514.045002, 9108416514.064999, 9108416514.26, 9108416514.895, 9108416515.099998, 9108416515.255001, 9108416516.445, 9108416517.145, 9108416517.295, 9108416517.59, 9108416517.95, 9108416518.27, 9108416518.835001, 9108416519.220001, 9108416519.535, 9108416519.955, 9108416520.855, 9108416521.025, 9108416521.404999, 9108416521.955, 9108416522.36, 9108416523.145, 9108416523.23, 9108416523.385, 9108416523.675, 9108416523.85, 9108416524.625, 9108416526.199999, 9108416526.385002, 9108416526.96, 9108416527.16, 9108416527.805, 9108416528.615002, 9108416529.925, 9108416530.235, 9108416530.595, 9108416530.985, 9108416531.515, 9108416531.949999, 9108416532.06, 9108416532.23, 9108416532.599998, 9108416532.9, 9108416533.14, 9108416533.915, 9108416534.720001, 9108416535.045, 9108416535.91, 9108416536.805, 9108416537.135, 9108416538.01, 9108416538.615, 9108416538.8, 9108416538.864998, 9108416538.939999, 9108416539.185001, 9108416539.575, 9108416540.395, 9108416540.98, 9108416541.08, 9108416541.224998, 9108416541.55, 9108416541.81, 9108416541.97, 9108416542.28, 9108416544.96, 9108416545.075, 9108416545.8, 9108416546.759998, 9108416547.095, 9108416547.935, 9108416548.01, 9108416548.165, 9108416548.595, 9108416548.904999, 9108416548.96, 9108416549.955, 9108416550.310001, 9108416550.470001, 9108416550.66, 9108416550.865, 9108416551.615002, 9108416551.98, 9108416552.49, 9108416553.29, 9108416553.65, 9108416553.89, 9108416554.285, 9108416554.525, 9108416555.150002, 9108416555.425, 9108416555.875, 9108416556.945, 9108416557.68, 9108416558.259998, 9108416559.91, 9108416560.73, 9108416561.4, 9108416561.99, 9108416563.45, 9108416563.785, 9108416567.02, 9108416567.185001, 9108416568.555, 9108416568.939999, 9108416569.46, 9108416569.965, 9108416571.050001, 9108416571.130001, 9108416571.675001, 9108416572.295, 9108416574.735, 9108416575.17, 9108416577.380001, 9108416578.005001, 9108416581.015, 9108416581.305, 9108416583.385002, 9108416584.005001, 9108416584.775, 9108416584.974998, 9108416585.44, 9108416585.975, 9108416586.76, 9108416587.2, 9108416588.01, 9108416588.990002, 9108416589.449999, 9108416590.06, 9108416591.18, 9108416591.365002, 9108416591.7, 9108416592.060001, 9108416592.59, 9108416592.965, 9108416593.645, 9108416593.84, 9108416594.02, 9108416594.265, 9108416594.75, 9108416595.05, 9108416601.524998, 9108416601.919998, 9108416605.759998, 9108416605.955, 9108416607.015, 9108416607.06, 9108416607.619999, 9108416607.904999, 9108416608.33, 9108416608.79, 9108416608.970001, 9108416613.51, 9108416614.095001, 9108416614.77, 9108416614.93, 9108416615.01, 9108416615.555, 9108416615.855, 9108416617.345, 9108416617.654999, 9108416625.619999, 9108416626.2, 9108416626.86, 9108416627.029999, 9108416627.715, 9108416628.015, 9108416628.650002, 9108416629.035, 9108416633.404999, 9108416633.880001, 9108416634.669998, 9108416635.2, 9108416636.485, 9108416636.965, 9108416638.855, 9108416639.009998, 9108416643.02, 9108416643.04, 9108416643.71, 9108416643.985, 9108416644.27, 9108416645.485, 9108416645.94, 9108416647.86, 9108416648.009998, 9108416648.494999, 9108416648.970001, 9108416649.45, 9108416650.025002, 9108416650.89, 9108416651.044998, 9108416651.695, 9108416651.970001, 9108416653.665, 9108416653.765, 9108416654.525, 9108416654.994999, 9108416658.71, 9108416659.265, 9108416662.845001, 9108416662.99, 9108416664.0, 9108416664.029999, 9108416664.720001, 9108416665.035, 9108416665.84, 9108416665.99, 9108416666.445, 9108416666.91, 9108416667.025, 9108416667.055, 9108416667.32, 9108416667.41, 9108416667.744999, 9108416668.3, 9108416668.78, 9108416669.045, 9108416671.755001, 9108416671.985, 9108416672.085, 9108416672.56, 9108416676.73, 9108416677.039999, 9108416678.465, 9108416678.92, 9108416679.715, 9108416680.27, 9108416684.525002, 9108416684.975, 9108416685.605, 9108416685.825, 9108416686.25, 9108416686.75, 9108416686.935001, 9108416687.060001, 9108416687.545, 9108416688.005001, 9108416688.755001, 9108416689.045, 9108416689.814999, 9108416690.04, 9108416692.795, 9108416693.015, 9108416700.795002, 9108416701.085, 9108416702.154999, 9108416702.55, 9108416702.965, 9108416703.8, 9108416704.29, 9108416705.425, 9108416705.83, 9108416705.985, 9108416706.814999, 9108416707.305, 9108416707.845001, 9108416708.06, 9108416708.545, 9108416708.975, 9108416709.73, 9108416709.875, 9108416710.27, 9108416711.130001, 9108416711.974998, 9108416712.675, 9108416712.955, 9108416714.34, 9108416714.975, 9108416716.855, 9108416717.5, 9108416718.765, 9108416719.255001, 9108416723.350002, 9108416723.975, 9108416724.0, 9108416724.025, 9108416725.665, 9108416725.730001, 9108416726.385002, 9108416727.02, 9108416727.83, 9108416728.32, 9108416728.85, 9108416729.48, 9108416730.215, 9108416730.35, 9108416730.795, 9108416731.285, 9108416733.83, 9108416734.32, 9108416734.825, 9108416735.44, 9108416737.83, 9108416738.435, 9108416739.720001, 9108416740.205, 9108416742.815, 9108416743.3, 9108416743.905, 9108416744.985, 9108416745.715, 9108416745.779999, 9108416746.25, 9108416746.675, 9108416746.745, 9108416748.84, 9108416749.325, 9108416749.9, 9108416750.975, 9108416751.740002, 9108416752.225, 9108416752.869999, 9108416753.46, 9108416754.800001, 9108416755.285, 9108416755.705002, 9108416755.77, 9108416758.885, 9108416759.494999, 9108416760.98, 9108416761.465, 9108416762.385, 9108416762.965, 9108416763.880001, 9108416764.37, 9108416764.92, 9108416766.015, 9108416768.060001, 9108416768.615, 9108416768.985, 9108416769.865, 9108416770.35, 9108416773.875, 9108416774.439999, 9108416777.265, 9108416777.735, 9108416777.965, 9108416778.795002, 9108416779.28, 9108416779.78, 9108416779.835001, 9108416780.335001, 9108416780.96, 9108416781.81, 9108416782.295, 9108416782.795, 9108416782.84, 9108416782.93, 9108416783.895, 9108416784.814999, 9108416785.515, 9108416785.8, 9108416785.845001, 9108416785.935001, 9108416786.93, 9108416787.83, 9108416788.315, 9108416788.82, 9108416788.86, 9108416789.36, 9108416789.925, 9108416790.835, 9108416791.79, 9108416791.86, 9108416792.975, 9108416793.84, 9108416794.2, 9108416794.865, 9108416795.305, 9108416795.59, 9108416795.95, 9108416796.86, 9108416797.29, 9108416797.75, 9108416797.869999, 9108416799.0, 9108416799.59, 9108416801.98, 9108416802.470001, 9108416804.970001, 9108416805.2, 9108416807.970001, 9108416808.48, 9108416810.98, 9108416811.205002, 9108416813.98, 9108416814.505001, 9108416816.985, 9108416817.375, 9108416817.830002, 9108416820.005001, 9108416820.51, 9108416822.994999, 9108416823.0, 9108416823.02, 9108416823.51, 9108416825.18, 9108416825.67, 9108416826.015, 9108416826.515, 9108416828.41, 9108416828.905, 9108416829.0, 9108416829.0, 9108416829.035, 9108416829.53, 9108416831.725, 9108416832.245, 9108416834.739998, 9108416835.98, 9108416837.6, 9108416837.865, 9108416838.989998, 9108416840.994999, 9108416841.489998, 9108416842.72, 9108416843.75, 9108416844.28, 9108416846.75, 9108416847.210001, 9108416848.525002, 9108416849.825, 9108416850.0, 9108416850.24, 9108416852.26, 9108416854.47, 9108416855.96, 9108416856.794998, 9108416858.805, 9108416859.810001, 9108416861.985, 9108416862.305, 9108416863.814999, 9108416865.529999, 9108416866.7, 9108416867.665, 9108416867.915, 9108416868.36, 9108416869.175, 9108416870.135, 9108416870.925001, 9108416872.425001, 9108416873.935001, 9108416874.985, 9108416876.51, 9108416877.32, 9108416879.84, 9108416880.5, 9108416882.52, 9108416884.529999, 9108416886.02, 9108416887.35, 9108416888.86, 9108416889.365002, 9108416890.330002, 9108416890.78, 9108416891.455, 9108416892.36, 9108416894.869999, 9108416895.31, 9108416895.64, 9108416897.885, 9108416899.384998, 9108416900.880001, 9108416901.380001, 9108416920.425, 9108416920.84, 9108416949.52, 9108416949.75, 9108416950.585, 9108416950.810001, 9108416975.77, 9108416975.86, 9145439871.369999, 9198568545.64, 9222934352.25, 9231194716.0, 9231194919.78, 9231194934.21, 9231194934.925, 9231194940.130001, 9231194943.415, 9231194943.585, 9231194943.94, 9231194944.355, 9231194944.94, 9231194947.355, 9231194949.7, 9231194950.325, 9231194951.425, 9231194952.384998, 9231194960.365, 9231194964.329998, 9231194970.875, 9231194972.27, 9231195016.24, 9231195018.25, 9231195075.759998, 9231195083.954998, 9269665990.95, 9338231787.775002, 9399111724.17, 9515343386.14, 9555077131.07, 9671707473.619999, 9671707486.41, 9671707560.72, 9671707614.849998, 9671707633.755001, 9671707657.95, 9671707715.100002, 9671707727.645, 9671707728.14, 9671707732.66, 9671707733.21, 9671707735.215, 9671707739.82, 9671707745.39, 9671707763.130001, 9671707764.580002, 9671707770.505001, 9671707771.580002, 9671707772.07, 9671707773.665, 9671707782.95, 9671707784.060001, 9671707818.055, 9671707818.720001, 9671707834.79, 9671707836.350002, 9671707858.16, 9671707862.135002, 9671707968.74, 9671707969.1, 9671707975.869999, 9671707977.875, 9734073849.325, 9734073851.34, 10527605318.64, 10547628126.595001, 10547628129.42, 10547628161.990002, 10547628194.195, 10547628213.895, 10547628251.064999, 10547628262.025, 10654138467.795, 10654138468.04, 10654138469.46, 10654138469.545, 10654138470.615, 10654138470.86, 10654138480.69, 10654138480.740002, 10654138493.555, 10654138493.615002, 10654138502.825, 10654138502.945, 10654138512.9, 10654138513.05, 10654138530.875, 10654138531.075, 10654138543.130001, 10654138543.155, 10654138571.364998, 10654138571.419998, 10654138583.46, 10654138583.54, 10654138614.779999, 10654138615.115, 10654138625.2, 10654138625.2, 10654138634.45, 10654138634.794998, 10654138652.19, 10654138652.19, 10654138672.369999, 10654138672.744999, 10654138684.525002, 10654138684.95, 10654138692.455, 10654138692.845, 10654138734.66, 10654138735.039999, 10654138818.634998, 10715527957.154999, 10776917261.634998, 10806102652.085, 10835288011.385])
labels = array([4.0, 0.0, 9.0, 0.0, 6.0, 4.0, 6.0, 4.0, 6.0, 4.0, 0.0, 1.0, 0.0, 9.0, 4.0, 10.0, 0.0, 15.0, 0.0, 15.0, 0.0, 4.0, 0.0, 4.0, 10.0, 0.0, 15.0, 6.0, 0.0, 22.0, 0.0, 2.0, 0.0, 2.0, 0.0, 1.0, 21.0, 2.0, 0.0, 6.0, 0.0, 21.0, 0.0, 3.0, 0.0, 3.0, 0.0, 1.0, 22.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 22.0, 0.0, 18.0, 0.0, 1.0, 0.0, 22.0, 0.0, 18.0, 0.0, 1.0, 0.0, 15.0, 20.0, 0.0, 9.0, 0.0, 6.0, 9.0, 6.0, 0.0, 10.0, 15.0, 0.0, 4.0, 9.0, 0.0, 9.0, 4.0, 15.0, 0.0, 15.0, 0.0, 10.0, 4.0, 15.0, 4.0, 9.0, 4.0, 9.0, 4.0, 12.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 12.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 12.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 12.0, 0.0, 20.0, 0.0, 19.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 18.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 1.0, 2.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 1.0, 0.0, 22.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 2.0, 0.0, 1.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 19.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 18.0, 0.0, 9.0, 18.0, 19.0, 0.0, 20.0, 0.0, 20.0, 19.0, 9.0, 0.0, 15.0, 0.0, 13.0, 0.0, 4.0, 17.0, 15.0, 0.0, 4.0, 9.0, 4.0, 15.0, 10.0, 9.0, 0.0, 4.0, 20.0, 4.0, 0.0, 10.0, 0.0, 10.0, 0.0, 4.0, 10.0, 4.0, 9.0, 10.0, 4.0, 9.0, 0.0, 9.0, 15.0, 9.0, 20.0, 4.0, 0.0, 10.0, 9.0, 4.0, 9.0, 0.0, 15.0, 0.0, 4.0, 9.0, 4.0, 0.0, 15.0, 0.0, 4.0, 10.0, 15.0, 4.0, 15.0, 4.0, 9.0, 10.0, 0.0, 16.0, 0.0, 16.0, 0.0, 13.0, 0.0, 13.0, 0.0, 4.0, 0.0, 9.0, 10.0, 4.0, 9.0, 4.0, 0.0, 10.0, 4.0, 15.0, 9.0, 10.0, 0.0, 15.0, 0.0, 19.0, 12.0, 22.0, 18.0, 2.0, 1.0, 18.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 19.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 20.0, 12.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 15.0, 4.0, 0.0, 9.0, 4.0, 9.0, 4.0, 15.0, 4.0, 9.0, 17.0, 11.0, 4.0, 0.0, 4.0, 9.0, 4.0, 10.0, 9.0, 0.0, 9.0, 4.0, 9.0, 13.0, 9.0, 13.0, 4.0, 9.0, 7.0, 0.0, 7.0, 0.0, 9.0, 0.0, 13.0, 0.0, 10.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 9.0, 4.0, 0.0, 4.0, 0.0, 10.0, 0.0, 9.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 15.0, 4.0, 15.0, 20.0, 0.0, 20.0, 0.0, 20.0, 0.0, 4.0, 10.0, 9.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 7.0, 5.0, 14.0, 4.0, 10.0, 4.0, 17.0, 4.0, 0.0, 4.0, 0.0, 4.0, 9.0, 0.0, 17.0, 10.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 15.0, 10.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 17.0, 10.0, 17.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 17.0, 10.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 0.0, 17.0, 0.0, 10.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 17.0, 0.0, 10.0, 17.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 17.0, 10.0, 0.0, 17.0, 0.0, 10.0, 0.0, 10.0, 17.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 0.0, 10.0, 0.0, 17.0, 0.0, 17.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 0.0, 17.0, 10.0, 17.0, 0.0, 10.0, 0.0, 17.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 17.0, 0.0, 10.0, 0.0, 10.0, 17.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 15.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 0.0, 10.0, 17.0, 10.0, 0.0, 4.0, 9.0, 4.0, 9.0, 0.0, 20.0, 4.0, 17.0, 4.0, 0.0, 9.0, 4.0, 9.0, 4.0, 0.0, 10.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 9.0, 4.0, 9.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 9.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 14.0, 0.0, 15.0, 0.0, 4.0, 0.0, 4.0, 15.0, 8.0, 15.0, 8.0, 17.0, 8.0, 0.0, 8.0, 0.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 0.0, 8.0, 0.0, 8.0, 17.0, 8.0, 17.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 17.0, 8.0, 17.0, 8.0, 15.0, 0.0, 8.0, 15.0, 0.0, 15.0, 8.0, 15.0, 8.0, 15.0, 8.0, 0.0, 15.0, 8.0, 15.0, 8.0, 15.0, 8.0, 0.0, 15.0, 8.0, 15.0, 0.0, 8.0, 15.0, 0.0, 8.0, 15.0, 8.0, 15.0, 0.0, 15.0, 8.0, 0.0, 8.0, 15.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 15.0, 8.0, 15.0, 8.0, 15.0, 17.0, 15.0, 8.0, 15.0, 8.0, 0.0, 15.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 0.0, 15.0, 8.0, 15.0, 0.0, 8.0, 15.0, 8.0, 0.0, 15.0, 8.0, 15.0, 0.0, 15.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 15.0, 8.0, 0.0, 8.0, 15.0, 8.0, 15.0, 8.0, 15.0, 0.0, 17.0, 15.0, 8.0, 15.0, 8.0, 15.0, 8.0, 15.0, 8.0, 15.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 15.0, 8.0, 15.0, 17.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 15.0, 8.0, 15.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 15.0, 17.0, 8.0, 0.0, 8.0, 0.0, 15.0, 8.0, 0.0, 8.0, 15.0, 0.0, 15.0, 0.0, 8.0, 15.0, 8.0, 15.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 15.0, 8.0, 15.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 17.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 8.0, 15.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 17.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 0.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 17.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 15.0, 0.0, 8.0, 0.0, 17.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 17.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 15.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 17.0, 8.0, 0.0, 8.0, 0.0, 15.0, 8.0, 0.0, 8.0, 15.0, 17.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 15.0, 0.0, 8.0, 15.0, 17.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 8.0, 0.0, 15.0, 0.0, 8.0, 0.0, 8.0, 15.0, 0.0, 8.0, 0.0, 8.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 4.0, 15.0, 22.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 22.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 15.0, 0.0, 4.0, 17.0, 4.0, 14.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 0.0, 4.0, 9.0, 4.0, 11.0, 0.0, 11.0, 4.0, 11.0, 4.0, 0.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 15.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 9.0, 4.0, 15.0, 4.0, 9.0, 4.0, 9.0, 4.0, 15.0, 4.0, 9.0, 4.0, 15.0, 4.0, 15.0, 4.0, 15.0, 14.0, 4.0])
def eqenergy(rows):
    return np.sum(rows, axis=1)
def classify(rows):
    energys = eqenergy(rows)

    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = labels[numers[indys]]
        outputs[defaultindys] = 0.0
        return outputs
    return thresh_search(energys)

numthresholds = 2175



# Main method
model_cap = numthresholds


def Validate(file):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 0
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:, -1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    with open(preprocessedfile, 'r') as csvinput:
        dirtyreader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(dirtyreader, None) + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            print(str(','.join(str(j) for j in ([i for i in row]))) + ',' + str(get_key(int(outputs[k]), classmapping)))



#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    args = parser.parse_args()
    faulthandler.enable()

    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile, classmapping)


    else:
        print("Classifier Type: Quick Clustering")
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count

            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            print("System Type:                        Binary classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
            print("System behavior")
            print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
            print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
            print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
            print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
            if int(num_TP + num_FN) != 0:
                print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
            if int(num_TN + num_FP) != 0:
                print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
            if int(num_TP + num_FP) != 0:
                print("Precision:                          {:.2f}".format(PPV))
            if int(2 * num_TP + num_FP + num_FN) != 0:
                print("F-1 Measure:                        {:.2f}".format(FONE))
            if int(num_TP + num_FN) != 0:
                print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
            if int(num_TP + num_FN + num_FP) != 0:
                print("Critical Success Index:             {:.2f}".format(TS))

        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            print("System Type:                        " + str(n_classes) + "-way classifier")
            print("Best-guess accuracy:                {:.2f}%".format(randguess))
            print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
            print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
            print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
            print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
            try:
                import numpy as np # For numpy see: http://numpy.org
                from numpy import array
            except:
                print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

            def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
                #check for numpy/scipy is imported
                try:
                    from scipy.sparse import coo_matrix #required for multiclass metrics
                except:
                    print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                    sys.exit()
                # Compute confusion matrix to evaluate the accuracy of a classification.
                # By definition a confusion matrix :math:C is such that :math:C_{i, j}
                # is equal to the number of observations known to be in group :math:i and
                # predicted to be in group :math:j.
                # Thus in binary classification, the count of true negatives is
                # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
                # :math:C_{1,1} and false positives is :math:C_{0,1}.
                # Read more in the :ref:User Guide <confusion_matrix>.
                # Parameters
                # ----------
                # y_true : array-like of shape (n_samples,)
                # Ground truth (correct) target values.
                # y_pred : array-like of shape (n_samples,)
                # Estimated targets as returned by a classifier.
                # labels : array-like of shape (n_classes), default=None
                # List of labels to index the matrix. This may be used to reorder
                # or select a subset of labels.
                # If None is given, those that appear at least once
                # in y_true or y_pred are used in sorted order.
                # sample_weight : array-like of shape (n_samples,), default=None
                # Sample weights.
                # normalize : {'true', 'pred', 'all'}, default=None
                # Normalizes confusion matrix over the true (rows), predicted (columns)
                # conditions or all the population. If None, confusion matrix will not be
                # normalized.
                # Returns
                # -------
                # C : ndarray of shape (n_classes, n_classes)
                # Confusion matrix.
                # References
                # ----------
                if labels is None:
                    labels = np.array(list(set(list(y_true.astype('int')))))
                else:
                    labels = np.asarray(labels)
                    if np.all([l not in y_true for l in labels]):
                        raise ValueError("At least one label specified must be in y_true")


                if sample_weight is None:
                    sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
                else:
                    sample_weight = np.asarray(sample_weight)
                if y_true.shape[0]!=y_pred.shape[0]:
                    raise ValueError("y_true and y_pred must be of the same length")

                if normalize not in ['true', 'pred', 'all', None]:
                    raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


                n_labels = labels.size
                label_to_ind = {y: x for x, y in enumerate(labels)}
                # convert yt, yp into index
                y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
                y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
                # intersect y_pred, y_true with labels, eliminate items not in labels
                ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
                y_pred = y_pred[ind]
                y_true = y_true[ind]
                # also eliminate weights of eliminated items
                sample_weight = sample_weight[ind]
                # Choose the accumulator dtype to always have high precision
                if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                    dtype = np.int64
                else:
                    dtype = np.float64
                cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


                with np.errstate(all='ignore'):
                    if normalize == 'true':
                        cm = cm / cm.sum(axis=1, keepdims=True)
                    elif normalize == 'pred':
                        cm = cm / cm.sum(axis=0, keepdims=True)
                    elif normalize == 'all':
                        cm = cm / cm.sum()
                    cm = np.nan_to_num(cm)
                return cm


            print("Confusion Matrix:")
            mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])



    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        os.remove(preprocessedfile)


