#!/usr/bin/env python3
#
# This code has been produced by an evaluation version of Daimensions(tm).
# Portions of this code copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.
# Distribution of this code in binary form or commercial use of any kind is forbidden.
# For a detailed license agreement see: http://brainome.ai/license
# Use of predictions results at your own risk.
#
# Output of Brainome Daimensions(tm) 0.98 Table Compiler v0.98.
# Invocation: btc -f NN -target class BNG-letter-nominal-1000000.csv -o BNG-letter-nominal-1000000_NN.py -nsamples 0 --yes -nsamples 0 -e 20
# Total compiler execution time: 6:37:34.07. Finished on: Sep-03-2020 18:50:56.
# This source code requires Python 3.
#
"""
Classifier Type:                     Neural Network
System Type:                         26-way classifier
Training/Validation Split:           60:40%
Best-guess accuracy:                 4.08%
Overall Model accuracy:              51.77% (517791/1000000 correct)
Overall Improvement over best guess: 47.69% (of possible 95.92%)
Model capacity (MEC):                680 bits
Generalization ratio:                761.45 bits/bit
Model efficiency:                    0.07%/parameter
Confusion Matrix:
 [2.82% 0.08% 0.01% 0.05% 0.03% 0.10% 0.03% 0.03% 0.01% 0.13% 0.03% 0.15%
  0.02% 0.02% 0.07% 0.00% 0.01% 0.12% 0.12% 0.04% 0.03% 0.02% 0.01% 0.03%
  0.08% 0.02%]
 [0.01% 1.54% 0.02% 0.02% 0.05% 0.39% 0.05% 0.03% 0.01% 0.00% 0.17% 0.03%
  0.11% 0.01% 0.30% 0.00% 0.01% 0.17% 0.02% 0.18% 0.05% 0.05% 0.01% 0.02%
  0.47% 0.12%]
 [0.00% 0.00% 2.75% 0.02% 0.00% 0.04% 0.01% 0.02% 0.43% 0.02% 0.00% 0.01%
  0.02% 0.18% 0.01% 0.09% 0.02% 0.01% 0.01% 0.05% 0.04% 0.08% 0.07% 0.09%
  0.01% 0.02%]
 [0.12% 0.02% 0.03% 2.58% 0.02% 0.04% 0.06% 0.01% 0.04% 0.03% 0.07% 0.08%
  0.02% 0.06% 0.03% 0.01% 0.01% 0.10% 0.27% 0.02% 0.04% 0.03% 0.13% 0.02%
  0.12% 0.06%]
 [0.01% 0.22% 0.02% 0.02% 0.76% 0.71% 0.22% 0.02% 0.01% 0.01% 0.28% 0.05%
  0.02% 0.03% 0.10% 0.05% 0.13% 0.29% 0.05% 0.07% 0.03% 0.14% 0.01% 0.04%
  0.27% 0.16%]
 [0.01% 0.07% 0.12% 0.05% 0.03% 1.85% 0.18% 0.03% 0.04% 0.01% 0.06% 0.03%
  0.01% 0.04% 0.18% 0.01% 0.02% 0.18% 0.01% 0.02% 0.01% 0.40% 0.04% 0.06%
  0.27% 0.10%]
 [0.01% 0.06% 0.04% 0.19% 0.07% 0.19% 1.79% 0.17% 0.01% 0.01% 0.20% 0.03%
  0.06% 0.05% 0.12% 0.02% 0.06% 0.15% 0.03% 0.04% 0.15% 0.16% 0.14% 0.15%
  0.10% 0.05%]
 [0.01% 0.16% 0.09% 0.04% 0.06% 0.39% 0.08% 0.67% 0.06% 0.02% 0.02% 0.03%
  0.38% 0.06% 0.03% 0.08% 0.05% 0.13% 0.01% 0.09% 0.10% 0.13% 0.45% 0.05%
  0.43% 0.27%]
 [0.05% 0.01% 0.37% 0.01% 0.00% 0.04% 0.01% 0.01% 2.69% 0.07% 0.00% 0.08%
  0.01% 0.14% 0.01% 0.01% 0.01% 0.02% 0.01% 0.03% 0.04% 0.03% 0.01% 0.05%
  0.03% 0.03%]
 [0.09% 0.01% 0.07% 0.16% 0.02% 0.04% 0.02% 0.01% 0.12% 2.42% 0.01% 0.31%
  0.01% 0.05% 0.02% 0.01% 0.02% 0.04% 0.12% 0.02% 0.02% 0.02% 0.02% 0.02%
  0.13% 0.03%]
 [0.02% 0.08% 0.01% 0.15% 0.05% 0.13% 0.04% 0.01% 0.01% 0.01% 2.27% 0.02%
  0.11% 0.01% 0.15% 0.05% 0.11% 0.08% 0.06% 0.01% 0.03% 0.06% 0.02% 0.01%
  0.20% 0.06%]
 [0.42% 0.03% 0.02% 0.07% 0.02% 0.02% 0.03% 0.01% 0.02% 0.43% 0.01% 2.29%
  0.03% 0.02% 0.02% 0.02% 0.04% 0.04% 0.06% 0.03% 0.11% 0.01% 0.02% 0.02%
  0.01% 0.09%]
 [0.01% 0.21% 0.03% 0.02% 0.03% 0.04% 0.04% 0.16% 0.02% 0.01% 0.03% 0.04%
  2.47% 0.01% 0.02% 0.01% 0.02% 0.03% 0.02% 0.23% 0.09% 0.01% 0.02% 0.01%
  0.08% 0.05%]
 [0.01% 0.02% 0.42% 0.12% 0.02% 0.06% 0.09% 0.03% 0.44% 0.05% 0.05% 0.03%
  0.02% 1.26% 0.03% 0.01% 0.01% 0.16% 0.04% 0.08% 0.10% 0.18% 0.20% 0.20%
  0.22% 0.03%]
 [0.02% 0.12% 0.01% 0.01% 0.04% 0.15% 0.02% 0.01% 0.01% 0.01% 0.19% 0.02%
  0.01% 0.01% 2.32% 0.05% 0.04% 0.11% 0.03% 0.02% 0.02% 0.08% 0.01% 0.01%
  0.21% 0.13%]
 [0.01% 0.01% 0.07% 0.02% 0.03% 0.04% 0.01% 0.01% 0.02% 0.01% 0.16% 0.03%
  0.01% 0.02% 0.03% 2.96% 0.09% 0.07% 0.01% 0.02% 0.02% 0.16% 0.01% 0.02%
  0.07% 0.06%]
 [0.01% 0.05% 0.02% 0.01% 0.03% 0.05% 0.03% 0.03% 0.03% 0.00% 0.06% 0.05%
  0.04% 0.01% 0.05% 0.09% 2.92% 0.07% 0.01% 0.03% 0.04% 0.01% 0.01% 0.02%
  0.04% 0.05%]
 [0.02% 0.18% 0.02% 0.02% 0.20% 0.28% 0.22% 0.01% 0.02% 0.01% 0.12% 0.18%
  0.02% 0.10% 0.52% 0.01% 0.02% 0.86% 0.01% 0.11% 0.07% 0.04% 0.02% 0.09%
  0.78% 0.02%]
 [0.19% 0.09% 0.02% 0.38% 0.02% 0.10% 0.05% 0.03% 0.03% 0.07% 0.10% 0.07%
  0.02% 0.04% 0.04% 0.02% 0.01% 0.14% 1.99% 0.05% 0.10% 0.07% 0.01% 0.03%
  0.12% 0.06%]
 [0.01% 0.13% 0.06% 0.03% 0.03% 0.13% 0.23% 0.02% 0.05% 0.01% 0.03% 0.03%
  0.15% 0.04% 0.04% 0.01% 0.06% 0.32% 0.01% 1.65% 0.09% 0.27% 0.00% 0.20%
  0.10% 0.03%]
 [0.01% 0.12% 0.14% 0.03% 0.06% 0.06% 0.06% 0.23% 0.07% 0.02% 0.01% 0.07%
  0.11% 0.10% 0.03% 0.04% 0.06% 0.24% 0.05% 0.28% 1.69% 0.03% 0.16% 0.11%
  0.17% 0.15%]
 [0.01% 0.04% 0.14% 0.04% 0.04% 0.34% 0.29% 0.03% 0.03% 0.02% 0.06% 0.02%
  0.02% 0.05% 0.06% 0.01% 0.01% 0.06% 0.02% 0.05% 0.02% 1.98% 0.20% 0.08%
  0.08% 0.05%]
 [0.01% 0.01% 0.17% 0.04% 0.03% 0.05% 0.07% 0.32% 0.04% 0.01% 0.01% 0.02%
  0.02% 0.04% 0.02% 0.07% 0.02% 0.04% 0.01% 0.02% 0.09% 0.14% 2.36% 0.03%
  0.04% 0.08%]
 [0.03% 0.03% 0.21% 0.03% 0.03% 0.19% 0.27% 0.30% 0.06% 0.01% 0.02% 0.03%
  0.02% 0.12% 0.04% 0.02% 0.03% 0.30% 0.01% 0.17% 0.09% 0.16% 0.15% 1.17%
  0.15% 0.05%]
 [0.01% 0.09% 0.01% 0.18% 0.02% 0.07% 0.07% 0.01% 0.01% 0.01% 0.25% 0.02%
  0.03% 0.02% 0.10% 0.02% 0.04% 0.41% 0.04% 0.02% 0.02% 0.03% 0.00% 0.01%
  2.24% 0.04%]
 [0.03% 0.09% 0.05% 0.02% 0.08% 0.17% 0.02% 0.26% 0.04% 0.02% 0.04% 0.04%
  0.09% 0.03% 0.14% 0.15% 0.12% 0.04% 0.01% 0.03% 0.07% 0.02% 0.66% 0.02%
  0.18% 1.48%]
Overfitting:                         No
Note: Labels have been remapped to 'T'=0, 'E'=1, 'M'=2, 'P'=3, 'S'=4, 'B'=5, 'D'=6, 'G'=7, 'W'=8, 'V'=9, 'J'=10, 'Y'=11, 'C'=12, 'N'=13, 'Z'=14, 'A'=15, 'L'=16, 'X'=17, 'F'=18, 'K'=19, 'U'=20, 'R'=21, 'O'=22, 'H'=23, 'I'=24, 'Q'=25.
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "BNG-letter-nominal-1000000.csv"


#Number of output logits
num_output_logits = 26

#Number of attributes
num_attr = 16
n_classes = 26

mappings = [{1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}, {1686921404.0: 0, 3271256840.0: 1, 4078137749.0: 2}]
list_of_cols_to_normalize = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]

transform_true = False

def column_norm(column,mappings):
    listy = []
    for i,val in enumerate(column.reshape(-1)):
        if not (val in mappings):
            mappings[val] = int(max(mappings.values())) + 1
        listy.append(mappings[val])
    return np.array(listy)

def Normalize(data_arr):
    if list_of_cols_to_normalize:
        for i,mapping in zip(list_of_cols_to_normalize, mappings):
            if i >= data_arr.shape[1]:
                break
            col = data_arr[:, i]
            normcol = column_norm(col,mapping)
            data_arr[:, i] = normcol
        return data_arr
    else:
        return data_arr

def transform(X):
    mean = None
    components = None
    whiten = None
    explained_variance = None
    if (transform_true):
        mean = np.array([])
        components = np.array([])
        whiten = None
        explained_variance = np.array([])
        X = X - mean

    X_transformed = np.dot(X, components.T)
    if whiten:
        X_transformed /= np.sqrt(explained_variance)
    return X_transformed

# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target="class"


def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target="class"
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return
    if (testfile):
        target = ''
        hc = -1
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'T': 0, 'E': 1, 'M': 2, 'P': 3, 'S': 4, 'B': 5, 'D': 6, 'G': 7, 'W': 8, 'V': 9, 'J': 10, 'Y': 11, 'C': 12, 'N': 13, 'Z': 14, 'A': 15, 'L': 16, 'X': 17, 'F': 18, 'K': 19, 'U': 20, 'R': 21, 'O': 22, 'H': 23, 'I': 24, 'Q': 25}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Helper (save an import)
def argmax(l):
    f = lambda i: l[i]
    return max(range(len(l)), key=f)
# Classifier
def single_classify(row):
    #inits
    x = row
    o = [0] * num_output_logits


    #Nueron Equations
    h_0 = max((((-0.35666233 * float(x[0]))+ (0.065921076 * float(x[1]))+ (-0.1552089 * float(x[2]))+ (-0.020774221 * float(x[3]))+ (0.6246712 * float(x[4]))+ (0.19450863 * float(x[5]))+ (-0.7862469 * float(x[6]))+ (0.28970078 * float(x[7]))+ (0.6727241 * float(x[8]))+ (1.24619 * float(x[9]))+ (-1.0407592 * float(x[10]))+ (-1.0682582 * float(x[11]))+ (-0.2109042 * float(x[12]))+ (-0.719772 * float(x[13]))+ (0.7372021 * float(x[14]))+ (-0.38620198 * float(x[15]))) + 1.2528265), 0)
    h_1 = max((((0.2678317 * float(x[0]))+ (0.113930486 * float(x[1]))+ (0.41534257 * float(x[2]))+ (-0.65172714 * float(x[3]))+ (0.22855474 * float(x[4]))+ (-0.66131145 * float(x[5]))+ (-0.044672295 * float(x[6]))+ (-0.027861714 * float(x[7]))+ (-0.19102585 * float(x[8]))+ (1.1873893 * float(x[9]))+ (0.13382494 * float(x[10]))+ (-0.122294426 * float(x[11]))+ (1.8349793 * float(x[12]))+ (-1.2317736 * float(x[13]))+ (0.3388738 * float(x[14]))+ (-0.1966279 * float(x[15]))) + -0.56375045), 0)
    h_2 = max((((0.011129795 * float(x[0]))+ (0.091377996 * float(x[1]))+ (0.06524179 * float(x[2]))+ (-0.18363889 * float(x[3]))+ (-0.7858741 * float(x[4]))+ (0.12438822 * float(x[5]))+ (0.6483497 * float(x[6]))+ (-0.40217054 * float(x[7]))+ (-0.10119861 * float(x[8]))+ (-0.1888186 * float(x[9]))+ (0.773873 * float(x[10]))+ (0.36587405 * float(x[11]))+ (0.4826782 * float(x[12]))+ (-1.1557106 * float(x[13]))+ (-0.00087770243 * float(x[14]))+ (-0.25285524 * float(x[15]))) + 1.3285013), 0)
    h_3 = max((((0.36436298 * float(x[0]))+ (-0.16670978 * float(x[1]))+ (0.17244051 * float(x[2]))+ (0.12832648 * float(x[3]))+ (0.27295187 * float(x[4]))+ (0.6826825 * float(x[5]))+ (-0.814491 * float(x[6]))+ (-0.6989141 * float(x[7]))+ (0.2844815 * float(x[8]))+ (0.18341193 * float(x[9]))+ (0.80519605 * float(x[10]))+ (-0.8831163 * float(x[11]))+ (0.28653988 * float(x[12]))+ (0.4699966 * float(x[13]))+ (-0.5320288 * float(x[14]))+ (-0.92036855 * float(x[15]))) + 0.35374206), 0)
    h_4 = max((((0.3142653 * float(x[0]))+ (0.114192925 * float(x[1]))+ (0.23883843 * float(x[2]))+ (0.07028863 * float(x[3]))+ (0.35651654 * float(x[4]))+ (-0.88064486 * float(x[5]))+ (-0.51345193 * float(x[6]))+ (-0.69106525 * float(x[7]))+ (-0.4422426 * float(x[8]))+ (0.2405111 * float(x[9]))+ (0.21431026 * float(x[10]))+ (-0.17657906 * float(x[11]))+ (0.06478358 * float(x[12]))+ (-0.2842648 * float(x[13]))+ (-0.044244505 * float(x[14]))+ (-0.16605593 * float(x[15]))) + 2.289659), 0)
    h_5 = max((((0.38408452 * float(x[0]))+ (0.060307015 * float(x[1]))+ (-0.33864835 * float(x[2]))+ (0.023953866 * float(x[3]))+ (0.18968141 * float(x[4]))+ (-0.7918997 * float(x[5]))+ (-0.5601938 * float(x[6]))+ (1.3427467 * float(x[7]))+ (0.47235206 * float(x[8]))+ (-0.18729235 * float(x[9]))+ (0.010738744 * float(x[10]))+ (-1.4001127 * float(x[11]))+ (0.12902641 * float(x[12]))+ (-0.19667363 * float(x[13]))+ (0.34640834 * float(x[14]))+ (-0.2326099 * float(x[15]))) + 1.1375482), 0)
    h_6 = max((((0.25661466 * float(x[0]))+ (0.02612884 * float(x[1]))+ (0.06423805 * float(x[2]))+ (-0.039557505 * float(x[3]))+ (0.3651399 * float(x[4]))+ (-1.2721454 * float(x[5]))+ (-0.019140465 * float(x[6]))+ (-0.45846227 * float(x[7]))+ (-1.153323 * float(x[8]))+ (1.0598043 * float(x[9]))+ (1.0716635 * float(x[10]))+ (0.8244916 * float(x[11]))+ (0.005087944 * float(x[12]))+ (-0.11514255 * float(x[13]))+ (-0.21864136 * float(x[14]))+ (-0.012572562 * float(x[15]))) + 0.68531597), 0)
    h_7 = max((((-0.30280942 * float(x[0]))+ (-0.098960906 * float(x[1]))+ (0.031105038 * float(x[2]))+ (0.09148038 * float(x[3]))+ (-0.16199666 * float(x[4]))+ (-0.38399258 * float(x[5]))+ (-0.7306594 * float(x[6]))+ (0.42510673 * float(x[7]))+ (-0.39467376 * float(x[8]))+ (-0.9036823 * float(x[9]))+ (-1.043992 * float(x[10]))+ (1.0823998 * float(x[11]))+ (0.8369231 * float(x[12]))+ (0.19654377 * float(x[13]))+ (0.7442175 * float(x[14]))+ (-0.03330088 * float(x[15]))) + 0.86774206), 0)
    h_8 = max((((-0.05481466 * float(x[0]))+ (0.009364477 * float(x[1]))+ (0.26209724 * float(x[2]))+ (-0.21748935 * float(x[3]))+ (0.56969076 * float(x[4]))+ (0.07773323 * float(x[5]))+ (-1.711211 * float(x[6]))+ (-0.27973175 * float(x[7]))+ (0.38096437 * float(x[8]))+ (0.09380946 * float(x[9]))+ (0.10825948 * float(x[10]))+ (-0.020961711 * float(x[11]))+ (0.06455035 * float(x[12]))+ (-0.92500305 * float(x[13]))+ (1.5896295 * float(x[14]))+ (-0.3310872 * float(x[15]))) + -0.5046733), 0)
    h_9 = max((((0.029014716 * float(x[0]))+ (-0.063125245 * float(x[1]))+ (0.47947636 * float(x[2]))+ (-0.22539325 * float(x[3]))+ (0.112767845 * float(x[4]))+ (0.96872133 * float(x[5]))+ (0.023009833 * float(x[6]))+ (1.0212247 * float(x[7]))+ (-0.25054333 * float(x[8]))+ (0.2557145 * float(x[9]))+ (-0.52520686 * float(x[10]))+ (-0.7823522 * float(x[11]))+ (0.21132758 * float(x[12]))+ (-1.7369864 * float(x[13]))+ (0.8330308 * float(x[14]))+ (-0.6420337 * float(x[15]))) + -0.27067012), 0)
    h_10 = max((((0.46321175 * float(x[0]))+ (-0.12854938 * float(x[1]))+ (0.28879404 * float(x[2]))+ (-0.04107083 * float(x[3]))+ (0.18687369 * float(x[4]))+ (0.628809 * float(x[5]))+ (-1.9302711 * float(x[6]))+ (0.6096459 * float(x[7]))+ (-0.087634176 * float(x[8]))+ (-0.72753584 * float(x[9]))+ (0.7094251 * float(x[10]))+ (0.42071903 * float(x[11]))+ (-0.06338503 * float(x[12]))+ (0.2918904 * float(x[13]))+ (-0.08336594 * float(x[14]))+ (-0.5038314 * float(x[15]))) + -0.12491349), 0)
    h_11 = max((((0.354449 * float(x[0]))+ (-0.008622673 * float(x[1]))+ (0.32797903 * float(x[2]))+ (-0.1966277 * float(x[3]))+ (0.210281 * float(x[4]))+ (-0.484272 * float(x[5]))+ (0.30007574 * float(x[6]))+ (1.0180954 * float(x[7]))+ (-0.3498892 * float(x[8]))+ (-2.4273736 * float(x[9]))+ (0.30126646 * float(x[10]))+ (-0.54989153 * float(x[11]))+ (0.31482983 * float(x[12]))+ (0.70749027 * float(x[13]))+ (-0.11472974 * float(x[14]))+ (-0.26131198 * float(x[15]))) + -0.40540132), 0)
    h_12 = max((((-0.061626118 * float(x[0]))+ (-0.027489187 * float(x[1]))+ (0.032395855 * float(x[2]))+ (-0.009994093 * float(x[3]))+ (0.014369942 * float(x[4]))+ (-0.07819907 * float(x[5]))+ (-0.91777647 * float(x[6]))+ (0.6865417 * float(x[7]))+ (0.8622688 * float(x[8]))+ (-2.470667 * float(x[9]))+ (-0.05456795 * float(x[10]))+ (0.5299125 * float(x[11]))+ (-0.21186545 * float(x[12]))+ (-0.64330035 * float(x[13]))+ (1.2664781 * float(x[14]))+ (-0.013962028 * float(x[15]))) + -0.4289804), 0)
    h_13 = max((((-0.19055761 * float(x[0]))+ (-0.43900296 * float(x[1]))+ (0.6756453 * float(x[2]))+ (0.115459464 * float(x[3]))+ (0.05906175 * float(x[4]))+ (-1.2524092 * float(x[5]))+ (-0.35610244 * float(x[6]))+ (0.5784698 * float(x[7]))+ (-1.0546826 * float(x[8]))+ (-0.37854367 * float(x[9]))+ (0.72350705 * float(x[10]))+ (0.2923894 * float(x[11]))+ (0.7256978 * float(x[12]))+ (0.09211621 * float(x[13]))+ (0.19595513 * float(x[14]))+ (-0.46438685 * float(x[15]))) + 1.7076079), 0)
    h_14 = max((((-0.12580448 * float(x[0]))+ (0.056430023 * float(x[1]))+ (-0.21029125 * float(x[2]))+ (0.0041955216 * float(x[3]))+ (0.13774522 * float(x[4]))+ (-0.6352182 * float(x[5]))+ (-0.3588848 * float(x[6]))+ (-0.26583222 * float(x[7]))+ (1.4008406 * float(x[8]))+ (-0.5845398 * float(x[9]))+ (0.6593913 * float(x[10]))+ (0.17273681 * float(x[11]))+ (-0.39605036 * float(x[12]))+ (-0.21766938 * float(x[13]))+ (1.5681431 * float(x[14]))+ (-0.3337389 * float(x[15]))) + -1.6190252), 0)
    h_15 = max((((-0.30201137 * float(x[0]))+ (-0.10608221 * float(x[1]))+ (0.044302452 * float(x[2]))+ (0.23161572 * float(x[3]))+ (0.5428296 * float(x[4]))+ (0.31891283 * float(x[5]))+ (0.21881832 * float(x[6]))+ (-1.0468684 * float(x[7]))+ (-0.7036895 * float(x[8]))+ (-0.81321067 * float(x[9]))+ (-0.024307258 * float(x[10]))+ (-0.33544046 * float(x[11]))+ (-0.07545158 * float(x[12]))+ (-0.37923574 * float(x[13]))+ (1.8421221 * float(x[14]))+ (0.24410225 * float(x[15]))) + -0.059400067), 0)
    h_16 = max((((-0.18109614 * float(x[0]))+ (0.14984936 * float(x[1]))+ (0.12905896 * float(x[2]))+ (-0.13321428 * float(x[3]))+ (-0.22075471 * float(x[4]))+ (0.35475612 * float(x[5]))+ (-0.16269687 * float(x[6]))+ (-0.35879394 * float(x[7]))+ (0.46186483 * float(x[8]))+ (-0.278528 * float(x[9]))+ (0.6707046 * float(x[10]))+ (-0.6422728 * float(x[11]))+ (-0.021650594 * float(x[12]))+ (-0.10330787 * float(x[13]))+ (1.3061345 * float(x[14]))+ (-1.2501107 * float(x[15]))) + 1.1639417), 0)
    h_17 = max((((0.12720835 * float(x[0]))+ (-0.12937339 * float(x[1]))+ (0.40124872 * float(x[2]))+ (-0.09793887 * float(x[3]))+ (0.3223225 * float(x[4]))+ (-0.96172684 * float(x[5]))+ (0.52305293 * float(x[6]))+ (-0.018649515 * float(x[7]))+ (1.6204382 * float(x[8]))+ (-0.31887957 * float(x[9]))+ (-0.37301514 * float(x[10]))+ (-0.47682872 * float(x[11]))+ (-0.35947534 * float(x[12]))+ (-0.098819554 * float(x[13]))+ (0.3588129 * float(x[14]))+ (-0.3936744 * float(x[15]))) + 0.046560947), 0)
    h_18 = max((((0.45627698 * float(x[0]))+ (-0.46084413 * float(x[1]))+ (-0.1767136 * float(x[2]))+ (0.701268 * float(x[3]))+ (0.09007772 * float(x[4]))+ (0.40803406 * float(x[5]))+ (-0.3589287 * float(x[6]))+ (-0.27992085 * float(x[7]))+ (0.26995385 * float(x[8]))+ (0.54791445 * float(x[9]))+ (0.21877293 * float(x[10]))+ (0.511682 * float(x[11]))+ (-0.9266435 * float(x[12]))+ (-0.023492316 * float(x[13]))+ (0.6491073 * float(x[14]))+ (0.23331244 * float(x[15]))) + -0.008405561), 0)
    h_19 = max((((-0.16441195 * float(x[0]))+ (-0.031850528 * float(x[1]))+ (-0.5396562 * float(x[2]))+ (0.27708364 * float(x[3]))+ (-0.046041176 * float(x[4]))+ (-0.4713501 * float(x[5]))+ (-0.71129876 * float(x[6]))+ (1.9492272 * float(x[7]))+ (-0.59405243 * float(x[8]))+ (-0.2769369 * float(x[9]))+ (-0.22555912 * float(x[10]))+ (0.17619114 * float(x[11]))+ (0.07154987 * float(x[12]))+ (-0.06560883 * float(x[13]))+ (0.45284313 * float(x[14]))+ (0.21762274 * float(x[15]))) + 0.35498413), 0)
    o[0] = (0.6560227 * h_0)+ (0.32172352 * h_1)+ (0.9456586 * h_2)+ (0.42729396 * h_3)+ (1.2640343 * h_4)+ (-0.25703177 * h_5)+ (-1.5654057 * h_6)+ (0.24916539 * h_7)+ (0.9269847 * h_8)+ (-0.6740059 * h_9)+ (-0.03173192 * h_10)+ (-1.0043447 * h_11)+ (0.99072224 * h_12)+ (0.59859514 * h_13)+ (-1.2351401 * h_14)+ (-0.4622874 * h_15)+ (-1.2250915 * h_16)+ (-1.1890968 * h_17)+ (-0.038015585 * h_18)+ (0.5458397 * h_19) + -0.3367643
    o[1] = (-0.7207247 * h_0)+ (0.46236107 * h_1)+ (-0.27808583 * h_2)+ (0.5242147 * h_3)+ (0.7512128 * h_4)+ (0.46438655 * h_5)+ (-1.229988 * h_6)+ (-0.2623764 * h_7)+ (-1.9035879 * h_8)+ (0.7733062 * h_9)+ (-0.009986368 * h_10)+ (-0.40815586 * h_11)+ (0.95123005 * h_12)+ (0.45583594 * h_13)+ (-0.64209086 * h_14)+ (0.12213944 * h_15)+ (-0.14755626 * h_16)+ (-0.9909148 * h_17)+ (-0.05725403 * h_18)+ (-0.52273977 * h_19) + 1.4230934
    o[2] = (0.34107476 * h_0)+ (-1.8017712 * h_1)+ (-0.35308042 * h_2)+ (0.050402027 * h_3)+ (1.1975156 * h_4)+ (-0.19235367 * h_5)+ (0.8070404 * h_6)+ (-0.2552797 * h_7)+ (-0.06275982 * h_8)+ (-0.16462833 * h_9)+ (-0.73168856 * h_10)+ (0.5792779 * h_11)+ (-1.0778809 * h_12)+ (-1.1427557 * h_13)+ (1.4049118 * h_14)+ (-0.26315683 * h_15)+ (0.32204068 * h_16)+ (0.214465 * h_17)+ (0.39613673 * h_18)+ (0.6588489 * h_19) + 1.7019973
    o[3] = (0.8508281 * h_0)+ (0.50497615 * h_1)+ (0.8337546 * h_2)+ (-0.058611214 * h_3)+ (-0.5907006 * h_4)+ (-1.0093043 * h_5)+ (-0.74503094 * h_6)+ (0.38289982 * h_7)+ (0.054690998 * h_8)+ (-0.7998797 * h_9)+ (0.79163194 * h_10)+ (-0.011910174 * h_11)+ (0.26889908 * h_12)+ (-0.101931825 * h_13)+ (0.95990264 * h_14)+ (-0.49959704 * h_15)+ (-0.5926165 * h_16)+ (-0.5582323 * h_17)+ (0.1881642 * h_18)+ (-0.5028659 * h_19) + 0.8624198
    o[4] = (-0.3151793 * h_0)+ (0.78744906 * h_1)+ (-0.50892955 * h_2)+ (0.40870872 * h_3)+ (-0.16011518 * h_4)+ (-0.15573129 * h_5)+ (-0.05444238 * h_6)+ (-0.5538976 * h_7)+ (-0.69801354 * h_8)+ (-0.2672524 * h_9)+ (-0.6016158 * h_10)+ (0.21004444 * h_11)+ (0.79713404 * h_12)+ (0.33897924 * h_13)+ (-0.7601376 * h_14)+ (0.004017882 * h_15)+ (-0.19426101 * h_16)+ (-0.083023876 * h_17)+ (0.18013626 * h_18)+ (0.014733924 * h_19) + 0.7527184
    o[5] = (-0.23257436 * h_0)+ (-0.30553788 * h_1)+ (0.1364282 * h_2)+ (-0.86020607 * h_3)+ (1.5091969 * h_4)+ (0.28899392 * h_5)+ (-0.72338736 * h_6)+ (-0.27825445 * h_7)+ (-0.9519803 * h_8)+ (-0.6616824 * h_9)+ (0.28650087 * h_10)+ (-0.785826 * h_11)+ (0.08537822 * h_12)+ (0.874897 * h_13)+ (-0.4317228 * h_14)+ (-1.4493492 * h_15)+ (0.40439022 * h_16)+ (-0.30407068 * h_17)+ (0.24459523 * h_18)+ (-0.62922204 * h_19) + 2.1562476
    o[6] = (-0.56591344 * h_0)+ (0.07753965 * h_1)+ (0.2746857 * h_2)+ (-0.37892666 * h_3)+ (-0.56344706 * h_4)+ (-0.16519439 * h_5)+ (0.22711149 * h_6)+ (0.43875423 * h_7)+ (0.51831967 * h_8)+ (-0.5458986 * h_9)+ (0.033650804 * h_10)+ (0.5257936 * h_11)+ (-1.2529367 * h_12)+ (0.12085444 * h_13)+ (0.08872254 * h_14)+ (0.14422588 * h_15)+ (0.15131338 * h_16)+ (-0.6246333 * h_17)+ (0.60721594 * h_18)+ (-0.3784693 * h_19) + 0.86594194
    o[7] = (0.27337393 * h_0)+ (-0.41010156 * h_1)+ (0.117761 * h_2)+ (-0.0072687506 * h_3)+ (-0.03403183 * h_4)+ (0.7694751 * h_5)+ (0.09432528 * h_6)+ (0.23537076 * h_7)+ (-0.26621974 * h_8)+ (-0.060982034 * h_9)+ (-0.63299584 * h_10)+ (-0.6736822 * h_11)+ (0.09270117 * h_12)+ (0.6594292 * h_13)+ (-0.5722225 * h_14)+ (0.083092354 * h_15)+ (0.511479 * h_16)+ (-0.18550937 * h_17)+ (-0.043507583 * h_18)+ (-1.6184787 * h_19) + 1.7591687
    o[8] = (0.884756 * h_0)+ (-1.2573638 * h_1)+ (0.5771257 * h_2)+ (-0.44269884 * h_3)+ (1.4943848 * h_4)+ (-0.39979953 * h_5)+ (0.68314075 * h_6)+ (-0.8658914 * h_7)+ (0.29144025 * h_8)+ (0.47414422 * h_9)+ (0.17886743 * h_10)+ (-0.8592879 * h_11)+ (0.44131395 * h_12)+ (-1.2936728 * h_13)+ (0.6908805 * h_14)+ (-0.5272497 * h_15)+ (-0.45477694 * h_16)+ (0.1664047 * h_17)+ (0.1454771 * h_18)+ (0.76889724 * h_19) + 1.0395528
    o[9] = (0.9835467 * h_0)+ (-0.6564734 * h_1)+ (0.8355488 * h_2)+ (0.519541 * h_3)+ (-1.1945668 * h_4)+ (-0.15485604 * h_5)+ (1.3908974 * h_6)+ (0.98241246 * h_7)+ (1.2626376 * h_8)+ (0.084697396 * h_9)+ (0.3549943 * h_10)+ (-0.9584469 * h_11)+ (-0.09346281 * h_12)+ (-0.8133562 * h_13)+ (-0.28670162 * h_14)+ (-0.65689707 * h_15)+ (-1.2200228 * h_16)+ (1.1164448 * h_17)+ (-0.46786687 * h_18)+ (-0.15185623 * h_19) + 0.19993193
    o[10] = (0.06406047 * h_0)+ (0.79593825 * h_1)+ (-0.37037584 * h_2)+ (-0.60035837 * h_3)+ (-0.305995 * h_4)+ (0.041812364 * h_5)+ (0.41554493 * h_6)+ (-0.93202454 * h_7)+ (-0.51361996 * h_8)+ (-0.4628281 * h_9)+ (0.5852541 * h_10)+ (1.0039405 * h_11)+ (0.5856422 * h_12)+ (0.00032315988 * h_13)+ (-0.047334258 * h_14)+ (0.95234954 * h_15)+ (-0.78484994 * h_16)+ (0.6048945 * h_17)+ (-0.53000665 * h_18)+ (-0.6212712 * h_19) + -0.28924656
    o[11] = (0.20622216 * h_0)+ (0.41657022 * h_1)+ (1.2562785 * h_2)+ (0.16765177 * h_3)+ (0.34536675 * h_4)+ (-0.5372202 * h_5)+ (-0.15929307 * h_6)+ (0.5595455 * h_7)+ (0.8649892 * h_8)+ (0.44791722 * h_9)+ (-0.1568172 * h_10)+ (-0.5029445 * h_11)+ (0.3731833 * h_12)+ (-1.1123695 * h_13)+ (-0.90968704 * h_14)+ (0.2392621 * h_15)+ (-1.8776274 * h_16)+ (0.6002925 * h_17)+ (-0.4617182 * h_18)+ (0.69371057 * h_19) + 0.4679061
    o[12] = (0.30500183 * h_0)+ (-0.45890585 * h_1)+ (-0.31872115 * h_2)+ (0.10104927 * h_3)+ (-0.46699575 * h_4)+ (1.206425 * h_5)+ (0.041058067 * h_6)+ (-0.42627418 * h_7)+ (-0.0738974 * h_8)+ (0.36493942 * h_9)+ (-0.28120992 * h_10)+ (-0.3113722 * h_11)+ (0.8246256 * h_12)+ (0.48226196 * h_13)+ (-2.0543535 * h_14)+ (1.1722578 * h_15)+ (0.109658085 * h_16)+ (-0.5045476 * h_17)+ (-0.31606415 * h_18)+ (-1.9919852 * h_19) + 2.0439634
    o[13] = (0.5578707 * h_0)+ (-0.31144744 * h_1)+ (-0.5183139 * h_2)+ (0.20886381 * h_3)+ (-0.28537133 * h_4)+ (-0.7046532 * h_5)+ (1.1733038 * h_6)+ (-0.5908456 * h_7)+ (0.6121031 * h_8)+ (-0.68978167 * h_9)+ (-0.43319234 * h_10)+ (0.25098884 * h_11)+ (0.23811351 * h_12)+ (-0.90962267 * h_13)+ (0.45151386 * h_14)+ (0.14963646 * h_15)+ (0.08132073 * h_16)+ (0.37530518 * h_17)+ (0.37236106 * h_18)+ (0.9537036 * h_19) + 0.5671963
    o[14] = (-1.3777169 * h_0)+ (0.72902435 * h_1)+ (-0.6394602 * h_2)+ (-0.32521972 * h_3)+ (0.80219567 * h_4)+ (-0.019790823 * h_5)+ (-0.16055559 * h_6)+ (-0.07738895 * h_7)+ (-1.0173163 * h_8)+ (0.3626924 * h_9)+ (0.11194177 * h_10)+ (0.8282688 * h_11)+ (-0.17317168 * h_12)+ (0.13458243 * h_13)+ (0.093359314 * h_14)+ (-0.6773249 * h_15)+ (-1.0666223 * h_16)+ (-0.018550053 * h_17)+ (-0.58703494 * h_18)+ (0.23482132 * h_19) + 0.94750553
    o[15] = (-1.6250125 * h_0)+ (0.5556512 * h_1)+ (-0.92904115 * h_2)+ (-0.042828087 * h_3)+ (-1.0398568 * h_4)+ (1.255126 * h_5)+ (0.37518921 * h_6)+ (0.34375212 * h_7)+ (-0.62620336 * h_8)+ (0.578723 * h_9)+ (-2.1893988 * h_10)+ (0.7709133 * h_11)+ (-0.5590895 * h_12)+ (0.07610526 * h_13)+ (1.4066434 * h_14)+ (-1.929021 * h_15)+ (-0.3137132 * h_16)+ (-0.35931838 * h_17)+ (0.959124 * h_18)+ (-1.8408182 * h_19) + 0.32028592
    o[16] = (-1.3233113 * h_0)+ (0.43191156 * h_1)+ (-0.2551999 * h_2)+ (0.15096667 * h_3)+ (-1.3091841 * h_4)+ (0.2674973 * h_5)+ (0.61975205 * h_6)+ (-0.8601633 * h_7)+ (-0.23140295 * h_8)+ (1.3899623 * h_9)+ (-1.9748656 * h_10)+ (1.2819824 * h_11)+ (-0.07237192 * h_12)+ (-0.17962502 * h_13)+ (-1.9122989 * h_14)+ (-0.29154804 * h_15)+ (0.22500357 * h_16)+ (0.08908789 * h_17)+ (1.1315513 * h_18)+ (-0.62062085 * h_19) + -0.25527427
    o[17] = (-0.11077452 * h_0)+ (0.3041365 * h_1)+ (-0.53958404 * h_2)+ (-0.1276176 * h_3)+ (-0.28203514 * h_4)+ (0.4839738 * h_5)+ (0.14749108 * h_6)+ (0.03555496 * h_7)+ (-0.44266653 * h_8)+ (0.23314598 * h_9)+ (0.21201405 * h_10)+ (-0.18133542 * h_11)+ (-0.36448333 * h_12)+ (0.11272412 * h_13)+ (0.35812074 * h_14)+ (0.3578946 * h_15)+ (-0.71813184 * h_16)+ (-0.33148772 * h_17)+ (-0.43716538 * h_18)+ (0.24156724 * h_19) + 0.9360494
    o[18] = (0.09916519 * h_0)+ (0.24361697 * h_1)+ (0.11084832 * h_2)+ (0.4502726 * h_3)+ (-1.082594 * h_4)+ (0.116574064 * h_5)+ (0.46371678 * h_6)+ (0.879949 * h_7)+ (-0.09570612 * h_8)+ (-0.24800514 * h_9)+ (0.59241676 * h_10)+ (-0.08658766 * h_11)+ (0.80486614 * h_12)+ (-0.4331049 * h_13)+ (-0.5745687 * h_14)+ (-0.69819677 * h_15)+ (-0.28373235 * h_16)+ (-0.30391073 * h_17)+ (0.054875616 * h_18)+ (-0.5067861 * h_19) + 0.24666187
    o[19] = (-0.15948504 * h_0)+ (-0.6572915 * h_1)+ (0.23360634 * h_2)+ (1.0100362 * h_3)+ (-1.0496563 * h_4)+ (0.23598789 * h_5)+ (0.8787857 * h_6)+ (0.19555895 * h_7)+ (0.0327927 * h_8)+ (0.43275985 * h_9)+ (-0.089776866 * h_10)+ (-0.059644457 * h_11)+ (-1.876137 * h_12)+ (-0.6585819 * h_13)+ (0.0016818243 * h_14)+ (1.091311 * h_15)+ (0.5015244 * h_16)+ (0.055821396 * h_17)+ (-0.4565489 * h_18)+ (1.203684 * h_19) + 0.37096518
    o[20] = (0.44092223 * h_0)+ (0.24006793 * h_1)+ (-0.7016382 * h_2)+ (-0.7996548 * h_3)+ (0.10514789 * h_4)+ (-1.0497825 * h_5)+ (-0.08880364 * h_6)+ (-0.04065772 * h_7)+ (-0.7393672 * h_8)+ (0.051382314 * h_9)+ (-0.03464084 * h_10)+ (0.3454095 * h_11)+ (-0.4481003 * h_12)+ (-0.72315466 * h_13)+ (-0.5033287 * h_14)+ (0.84857774 * h_15)+ (0.6826801 * h_16)+ (0.03993624 * h_17)+ (0.26856765 * h_18)+ (0.37398288 * h_19) + 1.9071773
    o[21] = (-0.60849524 * h_0)+ (-0.4642626 * h_1)+ (0.1058524 * h_2)+ (0.56533325 * h_3)+ (0.023685202 * h_4)+ (-0.048344374 * h_5)+ (0.024661537 * h_6)+ (-0.5072516 * h_7)+ (0.66171265 * h_8)+ (-0.4733993 * h_9)+ (0.19294181 * h_10)+ (-0.16131812 * h_11)+ (-1.0408142 * h_12)+ (0.16592173 * h_13)+ (0.28170797 * h_14)+ (-0.9888761 * h_15)+ (0.60628396 * h_16)+ (-0.1779621 * h_17)+ (0.11569504 * h_18)+ (0.20848384 * h_19) + 1.2070212
    o[22] = (-0.07124864 * h_0)+ (-0.22080958 * h_1)+ (-0.056881953 * h_2)+ (-0.57006055 * h_3)+ (-0.9866341 * h_4)+ (0.99117404 * h_5)+ (0.1673871 * h_6)+ (0.30868948 * h_7)+ (0.7930757 * h_8)+ (-0.76176596 * h_9)+ (-0.6212165 * h_10)+ (-0.6573422 * h_11)+ (-1.0591192 * h_12)+ (0.65563715 * h_13)+ (0.7440373 * h_14)+ (0.3792403 * h_15)+ (0.0842115 * h_16)+ (-0.4076006 * h_17)+ (0.05649531 * h_18)+ (-2.321669 * h_19) + 2.0313313
    o[23] = (-0.24824667 * h_0)+ (-0.5165646 * h_1)+ (0.6263447 * h_2)+ (0.16974366 * h_3)+ (1.5359408 * h_4)+ (-0.4429164 * h_5)+ (-0.38167256 * h_6)+ (-0.020658094 * h_7)+ (0.24788076 * h_8)+ (-0.47194886 * h_9)+ (-0.53935313 * h_10)+ (-0.5354857 * h_11)+ (-1.3577951 * h_12)+ (-0.34139326 * h_13)+ (0.28153268 * h_14)+ (0.44786954 * h_15)+ (0.46257073 * h_16)+ (-0.027699718 * h_17)+ (0.038618915 * h_18)+ (1.6079192 * h_19) + 0.91161424
    o[24] = (-0.7139831 * h_0)+ (0.5338366 * h_1)+ (-0.9206624 * h_2)+ (-1.6432394 * h_3)+ (0.0622292 * h_4)+ (-0.41772786 * h_5)+ (-0.43232274 * h_6)+ (0.042000234 * h_7)+ (0.07788857 * h_8)+ (0.4488674 * h_9)+ (0.38414764 * h_10)+ (0.3913235 * h_11)+ (-0.20715146 * h_12)+ (0.71583545 * h_13)+ (-0.47799823 * h_14)+ (-0.32089436 * h_15)+ (-0.08048831 * h_16)+ (0.7032007 * h_17)+ (-0.25360402 * h_18)+ (-0.4644933 * h_19) + 0.22848022
    o[25] = (0.57666063 * h_0)+ (0.4909611 * h_1)+ (0.31846306 * h_2)+ (-0.82907 * h_3)+ (-0.29703766 * h_4)+ (0.28416255 * h_5)+ (0.060752366 * h_6)+ (-0.2507563 * h_7)+ (0.5047741 * h_8)+ (-0.99858135 * h_9)+ (-0.5463461 * h_10)+ (0.89736366 * h_11)+ (-0.5645785 * h_12)+ (0.042180825 * h_13)+ (-0.96926165 * h_14)+ (-0.8354841 * h_15)+ (0.8274857 * h_16)+ (0.027080486 * h_17)+ (-0.17137048 * h_18)+ (-1.6102809 * h_19) + 1.5107526

    

    #Output Decision Rule
    if num_output_logits==1:
        return o[0]>=0
    else:
        return argmax(o)


def classify(arr, transform=False):
    #apply transformation if necessary
    if transform:
        arr[:,:-1] = transform(arr[:,:-1])
    #init
    w_h = np.array([[-0.3566623330116272, 0.06592107564210892, -0.15520890057086945, -0.020774221047759056, 0.624671220779419, 0.1945086270570755, -0.7862468957901001, 0.28970077633857727, 0.6727241277694702, 1.2461899518966675, -1.0407592058181763, -1.0682581663131714, -0.21090419590473175, -0.7197719812393188, 0.7372021079063416, -0.38620197772979736], [0.2678317129611969, 0.11393048614263535, 0.4153425693511963, -0.6517271399497986, 0.22855474054813385, -0.6613114476203918, -0.044672295451164246, -0.027861714363098145, -0.19102585315704346, 1.1873892545700073, 0.13382494449615479, -0.12229442596435547, 1.8349792957305908, -1.2317736148834229, 0.33887380361557007, -0.1966279000043869], [0.011129794642329216, 0.09137799590826035, 0.06524179130792618, -0.18363888561725616, -0.7858741283416748, 0.12438821792602539, 0.6483497023582458, -0.4021705389022827, -0.10119861364364624, -0.18881860375404358, 0.773872971534729, 0.3658740520477295, 0.48267820477485657, -1.1557105779647827, -0.0008777024340815842, -0.25285524129867554], [0.3643629848957062, -0.1667097806930542, 0.1724405139684677, 0.12832647562026978, 0.2729518711566925, 0.6826825141906738, -0.8144909739494324, -0.698914110660553, 0.2844814956188202, 0.18341192603111267, 0.8051960468292236, -0.8831163048744202, 0.28653988242149353, 0.4699966013431549, -0.5320287942886353, -0.9203685522079468], [0.31426531076431274, 0.11419292539358139, 0.23883843421936035, 0.07028862833976746, 0.3565165400505066, -0.8806448578834534, -0.5134519338607788, -0.69106525182724, -0.4422425925731659, 0.24051110446453094, 0.21431025862693787, -0.1765790581703186, 0.06478358060121536, -0.28426480293273926, -0.04424450546503067, -0.16605593264102936], [0.3840845227241516, 0.06030701473355293, -0.33864834904670715, 0.023953866213560104, 0.18968141078948975, -0.7918996810913086, -0.5601937770843506, 1.3427467346191406, 0.4723520576953888, -0.18729235231876373, 0.010738744400441647, -1.4001127481460571, 0.1290264129638672, -0.19667363166809082, 0.34640833735466003, -0.23260989785194397], [0.25661465525627136, 0.02612883970141411, 0.0642380490899086, -0.039557505398988724, 0.365139901638031, -1.272145390510559, -0.019140465185046196, -0.45846226811408997, -1.1533230543136597, 1.0598043203353882, 1.071663498878479, 0.8244916200637817, 0.005087944213300943, -0.11514254659414291, -0.21864135563373566, -0.01257256232202053], [-0.3028094172477722, -0.09896090626716614, 0.03110503777861595, 0.09148038178682327, -0.16199666261672974, -0.38399258255958557, -0.7306594252586365, 0.4251067340373993, -0.39467376470565796, -0.903682291507721, -1.043992042541504, 1.082399845123291, 0.8369231224060059, 0.19654376804828644, 0.7442175149917603, -0.03330088034272194], [-0.0548146590590477, 0.00936447735875845, 0.26209723949432373, -0.2174893468618393, 0.5696907639503479, 0.07773323357105255, -1.7112109661102295, -0.27973175048828125, 0.38096436858177185, 0.09380946308374405, 0.10825947672128677, -0.020961711183190346, 0.06455034762620926, -0.9250030517578125, 1.5896295309066772, -0.331087201833725], [0.029014715924859047, -0.06312524527311325, 0.47947636246681213, -0.22539325058460236, 0.11276784539222717, 0.968721330165863, 0.023009832948446274, 1.0212247371673584, -0.2505433261394501, 0.2557145059108734, -0.5252068638801575, -0.7823522090911865, 0.21132758259773254, -1.7369863986968994, 0.8330308198928833, -0.6420336961746216], [0.4632117450237274, -0.12854938209056854, 0.28879404067993164, -0.04107083007693291, 0.1868736892938614, 0.6288089752197266, -1.9302711486816406, 0.6096459031105042, -0.08763417601585388, -0.7275358438491821, 0.7094250917434692, 0.4207190275192261, -0.06338503211736679, 0.291890412569046, -0.08336593955755234, -0.5038313865661621], [0.35444900393486023, -0.008622673340141773, 0.32797902822494507, -0.19662770628929138, 0.21028099954128265, -0.4842720031738281, 0.3000757396221161, 1.0180953741073608, -0.34988918900489807, -2.4273736476898193, 0.30126646161079407, -0.5498915314674377, 0.31482982635498047, 0.7074902653694153, -0.11472973972558975, -0.26131197810173035], [-0.061626117676496506, -0.027489187195897102, 0.0323958545923233, -0.00999409332871437, 0.014369942247867584, -0.07819907367229462, -0.9177764654159546, 0.6865416765213013, 0.8622688055038452, -2.4706668853759766, -0.05456795170903206, 0.5299124717712402, -0.21186545491218567, -0.6433003544807434, 1.2664780616760254, -0.013962027616798878], [-0.19055761396884918, -0.43900296092033386, 0.6756452918052673, 0.11545946449041367, 0.05906175076961517, -1.2524092197418213, -0.35610243678092957, 0.5784698128700256, -1.0546826124191284, -0.3785436749458313, 0.7235070466995239, 0.2923893928527832, 0.7256978154182434, 0.09211620688438416, 0.19595512747764587, -0.46438685059547424], [-0.12580448389053345, 0.05643002316355705, -0.21029125154018402, 0.0041955215856432915, 0.1377452164888382, -0.6352182030677795, -0.3588848114013672, -0.26583221554756165, 1.4008406400680542, -0.5845397710800171, 0.6593912839889526, 0.17273680865764618, -0.396050363779068, -0.21766938269138336, 1.5681431293487549, -0.33373889327049255], [-0.3020113706588745, -0.10608220845460892, 0.044302452355623245, 0.23161572217941284, 0.5428295731544495, 0.3189128339290619, 0.2188183218240738, -1.0468684434890747, -0.7036895155906677, -0.813210666179657, -0.024307258427143097, -0.335440456867218, -0.07545158267021179, -0.37923574447631836, 1.8421220779418945, 0.24410225450992584], [-0.1810961365699768, 0.14984935522079468, 0.12905895709991455, -0.13321428000926971, -0.2207547128200531, 0.35475611686706543, -0.16269686818122864, -0.3587939441204071, 0.4618648290634155, -0.27852800488471985, 0.6707046031951904, -0.6422727704048157, -0.021650593727827072, -0.10330787301063538, 1.3061344623565674, -1.2501107454299927], [0.12720835208892822, -0.12937338650226593, 0.4012487232685089, -0.09793887287378311, 0.3223224878311157, -0.9617268443107605, 0.5230529308319092, -0.018649514764547348, 1.6204382181167603, -0.3188795745372772, -0.3730151355266571, -0.4768287241458893, -0.35947534441947937, -0.0988195538520813, 0.3588128983974457, -0.39367440342903137], [0.4562769830226898, -0.46084412932395935, -0.17671360075473785, 0.7012680172920227, 0.0900777205824852, 0.4080340564250946, -0.35892871022224426, -0.2799208462238312, 0.2699538469314575, 0.547914445400238, 0.21877293288707733, 0.5116819739341736, -0.9266434907913208, -0.023492315784096718, 0.6491072773933411, 0.2333124428987503], [-0.16441194713115692, -0.031850527971982956, -0.5396562218666077, 0.2770836353302002, -0.04604117572307587, -0.4713501036167145, -0.7112987637519836, 1.949227213859558, -0.5940524339675903, -0.2769368886947632, -0.22555911540985107, 0.17619113624095917, 0.07154987007379532, -0.06560882925987244, 0.4528431296348572, 0.21762274205684662]])
    b_h = np.array([1.252826452255249, -0.5637504458427429, 1.3285013437271118, 0.3537420630455017, 2.289659023284912, 1.1375482082366943, 0.6853159666061401, 0.8677420616149902, -0.5046733021736145, -0.2706701159477234, -0.12491349130868912, -0.4054013192653656, -0.42898041009902954, 1.7076078653335571, -1.6190252304077148, -0.05940006673336029, 1.163941740989685, 0.046560946851968765, -0.008405560627579689, 0.3549841344356537])
    w_o = np.array([[0.6560227274894714, 0.3217235207557678, 0.9456586241722107, 0.42729395627975464, 1.2640342712402344, -0.25703176856040955, -1.5654057264328003, 0.2491653859615326, 0.9269847273826599, -0.674005925655365, -0.03173191845417023, -1.0043447017669678, 0.9907222390174866, 0.598595142364502, -1.235140085220337, -0.46228739619255066, -1.2250914573669434, -1.1890968084335327, -0.03801558539271355, 0.5458397269248962], [-0.7207247018814087, 0.46236106753349304, -0.2780858278274536, 0.5242146849632263, 0.7512127757072449, 0.46438655257225037, -1.2299879789352417, -0.2623763978481293, -1.9035879373550415, 0.7733061909675598, -0.009986368007957935, -0.4081558585166931, 0.9512300491333008, 0.4558359384536743, -0.6420908570289612, 0.12213943898677826, -0.14755626022815704, -0.9909148216247559, -0.05725403130054474, -0.5227397680282593], [0.34107476472854614, -1.8017711639404297, -0.3530804216861725, 0.05040202662348747, 1.197515606880188, -0.19235366582870483, 0.8070403933525085, -0.25527969002723694, -0.06275981664657593, -0.16462832689285278, -0.7316885590553284, 0.5792778730392456, -1.077880859375, -1.1427557468414307, 1.404911756515503, -0.26315683126449585, 0.3220406770706177, 0.21446500718593597, 0.39613673090934753, 0.6588488817214966], [0.8508281111717224, 0.5049761533737183, 0.8337545990943909, -0.05861121416091919, -0.590700626373291, -1.0093042850494385, -0.74503093957901, 0.38289982080459595, 0.054690998047590256, -0.7998797297477722, 0.7916319370269775, -0.011910174041986465, 0.2688990831375122, -0.10193182528018951, 0.9599026441574097, -0.49959704279899597, -0.5926164984703064, -0.558232307434082, 0.18816420435905457, -0.5028659105300903], [-0.3151792883872986, 0.787449061870575, -0.5089295506477356, 0.40870872139930725, -0.16011518239974976, -0.15573129057884216, -0.054442379623651505, -0.5538976192474365, -0.6980135440826416, -0.2672523856163025, -0.6016157865524292, 0.21004444360733032, 0.7971340417861938, 0.33897924423217773, -0.7601376175880432, 0.00401788204908371, -0.19426101446151733, -0.08302387595176697, 0.18013626337051392, 0.014733923599123955], [-0.23257435858249664, -0.3055378794670105, 0.13642820715904236, -0.8602060675621033, 1.5091968774795532, 0.2889939248561859, -0.7233873605728149, -0.2782544493675232, -0.9519802927970886, -0.6616824269294739, 0.28650087118148804, -0.7858260273933411, 0.08537822216749191, 0.8748970031738281, -0.4317227900028229, -1.4493491649627686, 0.40439021587371826, -0.30407068133354187, 0.2445952296257019, -0.62922203540802], [-0.5659134387969971, 0.07753965258598328, 0.27468571066856384, -0.37892666459083557, -0.5634470582008362, -0.16519439220428467, 0.22711148858070374, 0.43875423073768616, 0.5183196663856506, -0.5458986163139343, 0.033650804311037064, 0.5257936120033264, -1.2529367208480835, 0.1208544373512268, 0.08872254192829132, 0.14422588050365448, 0.15131337940692902, -0.6246333122253418, 0.607215940952301, -0.3784692883491516], [0.27337393164634705, -0.4101015627384186, 0.11776100099086761, -0.007268750574439764, -0.03403183072805405, 0.7694751024246216, 0.09432528167963028, 0.23537075519561768, -0.26621973514556885, -0.06098203361034393, -0.6329958438873291, -0.6736822128295898, 0.09270116686820984, 0.6594291925430298, -0.5722224712371826, 0.0830923542380333, 0.5114790201187134, -0.18550936877727509, -0.04350758343935013, -1.6184786558151245], [0.8847560286521912, -1.2573637962341309, 0.5771257281303406, -0.4426988363265991, 1.494384765625, -0.39979952573776245, 0.683140754699707, -0.8658913969993591, 0.2914402484893799, 0.47414422035217285, 0.1788674294948578, -0.8592879176139832, 0.4413139522075653, -1.293672800064087, 0.6908804774284363, -0.5272496938705444, -0.45477694272994995, 0.16640469431877136, 0.14547710120677948, 0.7688972353935242], [0.9835466742515564, -0.6564733982086182, 0.8355488181114197, 0.5195410251617432, -1.1945668458938599, -0.15485604107379913, 1.3908973932266235, 0.9824124574661255, 1.2626376152038574, 0.08469739556312561, 0.3549942970275879, -0.9584469199180603, -0.09346280992031097, -0.8133562207221985, -0.28670161962509155, -0.6568970680236816, -1.2200227975845337, 1.1164448261260986, -0.4678668677806854, -0.1518562287092209], [0.06406047195196152, 0.79593825340271, -0.3703758418560028, -0.6003583669662476, -0.30599498748779297, 0.041812364012002945, 0.41554492712020874, -0.9320245385169983, -0.5136199593544006, -0.4628280997276306, 0.5852540731430054, 1.003940463066101, 0.5856422185897827, 0.000323159882100299, -0.04733425751328468, 0.9523495435714722, -0.7848499417304993, 0.6048945188522339, -0.5300066471099854, -0.6212711930274963], [0.20622216165065765, 0.4165702164173126, 1.2562785148620605, 0.16765177249908447, 0.3453667461872101, -0.5372201800346375, -0.159293070435524, 0.5595455169677734, 0.8649892210960388, 0.44791722297668457, -0.15681719779968262, -0.5029445290565491, 0.37318331003189087, -1.1123695373535156, -0.9096870422363281, 0.23926210403442383, -1.8776273727416992, 0.6002925038337708, -0.46171820163726807, 0.6937105655670166], [0.305001825094223, -0.4589058458805084, -0.31872114539146423, 0.1010492667555809, -0.4669957458972931, 1.2064249515533447, 0.04105806723237038, -0.4262741804122925, -0.07389739900827408, 0.364939421415329, -0.28120991587638855, -0.31137219071388245, 0.8246256113052368, 0.4822619557380676, -2.0543534755706787, 1.1722577810287476, 0.10965808480978012, -0.5045475959777832, -0.31606414914131165, -1.9919852018356323], [0.5578706860542297, -0.3114474415779114, -0.5183138847351074, 0.20886380970478058, -0.285371333360672, -0.7046532034873962, 1.1733038425445557, -0.5908455848693848, 0.6121031045913696, -0.689781665802002, -0.43319234251976013, 0.25098884105682373, 0.23811350762844086, -0.9096226692199707, 0.4515138566493988, 0.14963646233081818, 0.08132073283195496, 0.37530517578125, 0.37236106395721436, 0.9537035822868347], [-1.3777168989181519, 0.729024350643158, -0.6394602060317993, -0.3252197206020355, 0.80219566822052, -0.01979082264006138, -0.16055558621883392, -0.0773889496922493, -1.0173163414001465, 0.36269238591194153, 0.11194176971912384, 0.828268826007843, -0.17317168414592743, 0.13458243012428284, 0.0933593139052391, -0.6773248910903931, -1.066622257232666, -0.018550053238868713, -0.5870349407196045, 0.23482131958007812], [-1.6250125169754028, 0.5556511878967285, -0.9290411472320557, -0.04282808676362038, -1.0398567914962769, 1.2551259994506836, 0.3751892149448395, 0.3437521159648895, -0.6262033581733704, 0.5787230134010315, -2.189398765563965, 0.770913302898407, -0.5590894818305969, 0.0761052593588829, 1.4066433906555176, -1.9290210008621216, -0.3137131929397583, -0.3593183755874634, 0.9591240286827087, -1.840818166732788], [-1.3233113288879395, 0.43191155791282654, -0.2551999092102051, 0.15096667408943176, -1.3091840744018555, 0.26749730110168457, 0.619752049446106, -0.8601632714271545, -0.2314029484987259, 1.3899623155593872, -1.9748655557632446, 1.281982421875, -0.07237192243337631, -0.1796250194311142, -1.9122989177703857, -0.2915480434894562, 0.22500357031822205, 0.08908788859844208, 1.1315512657165527, -0.620620846748352], [-0.11077451705932617, 0.3041365146636963, -0.5395840406417847, -0.12761759757995605, -0.28203514218330383, 0.48397380113601685, 0.14749108254909515, 0.03555496037006378, -0.44266653060913086, 0.23314598202705383, 0.21201404929161072, -0.1813354194164276, -0.3644833266735077, 0.11272411793470383, 0.35812073945999146, 0.3578945994377136, -0.7181318402290344, -0.3314877152442932, -0.43716537952423096, 0.2415672391653061], [0.0991651862859726, 0.2436169683933258, 0.1108483225107193, 0.4502725899219513, -1.0825940370559692, 0.11657406389713287, 0.4637167751789093, 0.8799489736557007, -0.09570612013339996, -0.24800513684749603, 0.5924167633056641, -0.08658766001462936, 0.8048661351203918, -0.43310490250587463, -0.5745686888694763, -0.6981967687606812, -0.2837323546409607, -0.3039107322692871, 0.05487561598420143, -0.5067861080169678], [-0.15948504209518433, -0.6572914719581604, 0.23360633850097656, 1.0100362300872803, -1.0496562719345093, 0.23598788678646088, 0.8787857294082642, 0.19555895030498505, 0.032792698591947556, 0.4327598512172699, -0.08977686613798141, -0.05964445695281029, -1.8761370182037354, -0.6585819125175476, 0.0016818243311718106, 1.091310977935791, 0.5015243887901306, 0.05582139641046524, -0.45654889941215515, 1.2036839723587036], [0.44092223048210144, 0.24006792902946472, -0.7016382217407227, -0.7996547818183899, 0.10514789074659348, -1.0497825145721436, -0.08880364149808884, -0.04065772145986557, -0.7393671870231628, 0.051382314413785934, -0.0346408411860466, 0.3454095125198364, -0.4481002986431122, -0.7231546640396118, -0.5033286809921265, 0.8485777378082275, 0.682680070400238, 0.03993624076247215, 0.26856765151023865, 0.37398287653923035], [-0.6084952354431152, -0.46426260471343994, 0.1058524027466774, 0.5653332471847534, 0.02368520200252533, -0.04834437370300293, 0.024661537259817123, -0.5072516202926636, 0.661712646484375, -0.4733993113040924, 0.192941814661026, -0.1613181233406067, -1.0408141613006592, 0.16592173278331757, 0.2817079722881317, -0.9888761043548584, 0.606283962726593, -0.17796209454536438, 0.1156950369477272, 0.20848384499549866], [-0.07124864310026169, -0.22080957889556885, -0.05688195303082466, -0.5700605511665344, -0.9866340756416321, 0.991174042224884, 0.16738709807395935, 0.3086894750595093, 0.793075680732727, -0.7617659568786621, -0.6212164759635925, -0.6573421955108643, -1.0591192245483398, 0.6556371450424194, 0.7440372705459595, 0.3792403042316437, 0.08421149849891663, -0.40760061144828796, 0.0564953088760376, -2.321669101715088], [-0.2482466697692871, -0.5165646076202393, 0.6263446807861328, 0.16974365711212158, 1.5359407663345337, -0.4429163932800293, -0.38167256116867065, -0.020658094435930252, 0.247880756855011, -0.47194886207580566, -0.5393531322479248, -0.5354856848716736, -1.3577951192855835, -0.34139326214790344, 0.2815326750278473, 0.44786953926086426, 0.4625707268714905, -0.02769971825182438, 0.038618914783000946, 1.6079192161560059], [-0.7139831185340881, 0.5338366031646729, -0.9206624031066895, -1.6432393789291382, 0.06222920119762421, -0.41772785782814026, -0.43232274055480957, 0.04200023412704468, 0.07788857072591782, 0.44886741042137146, 0.38414764404296875, 0.3913235068321228, -0.20715145766735077, 0.715835452079773, -0.4779982268810272, -0.32089436054229736, -0.08048830926418304, 0.7032006978988647, -0.2536040246486664, -0.4644933044910431], [0.5766606330871582, 0.49096110463142395, 0.3184630572795868, -0.829069972038269, -0.29703766107559204, 0.2841625511646271, 0.06075236573815346, -0.25075629353523254, 0.5047740936279297, -0.9985813498497009, -0.546346127986908, 0.8973636627197266, -0.5645784735679626, 0.042180825024843216, -0.969261646270752, -0.8354840874671936, 0.8274856805801392, 0.027080485597252846, -0.1713704764842987, -1.6102808713912964]])
    b_o = np.array([-0.33676430583000183, 1.4230934381484985, 1.7019972801208496, 0.8624197840690613, 0.7527183890342712, 2.156247615814209, 0.8659419417381287, 1.7591687440872192, 1.0395528078079224, 0.19993193447589874, -0.2892465591430664, 0.46790608763694763, 2.0439634323120117, 0.5671963095664978, 0.947505533695221, 0.3202859163284302, -0.2552742660045624, 0.9360494017601013, 0.24666187167167664, 0.3709651827812195, 1.9071773290634155, 1.2070212364196777, 2.0313313007354736, 0.9116142392158508, 0.22848021984100342, 1.510752558708191])

    #Hidden Layer
    h = np.dot(arr, w_h.T) + b_h
    
    relu = np.maximum(h, np.zeros_like(h))


    #Output
    out = np.dot(relu, w_o.T) + b_o
    if num_output_logits == 1:
        return (out >= 0).astype('int').reshape(-1)
    else:
        return (np.argmax(out, axis=1)).reshape(-1)



def Predict(arr,headerless,csvfile, get_key, classmapping):
    with open(csvfile, 'r') as csvinput:
        #readers and writers
        reader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(reader, None) + ["Prediction"]))
        
        
        for i, row in enumerate(reader):
            #use the transformed array as input to predictor
            pred = str(get_key(int(single_classify(arr[i])), classmapping))
            #use original untransformed line to write out
            row.append(pred)
            print(','.join(row))


def Validate(cleanarr):
    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs
    


# Main method
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile',action='store_true',help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    args = parser.parse_args()
    faulthandler.enable()


    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}


    #load file
    cleanarr = np.loadtxt(cleanfile, delimiter=',', dtype='float64')


    #Normalize
    cleanarr = Normalize(cleanarr)


    #Transform
    if transform_true:
        if args.validate:
            trans = transform(cleanarr[:, :-1])
            cleanarr = np.concatenate((trans, cleanarr[:, -1].reshape(-1, 1)), axis = 1)
        else:
            cleanarr = transform(cleanarr)


    #Predict
    if not args.validate:
        Predict(cleanarr, args.headerless, preprocessedfile, get_key, classmapping)


    #Validate
    else:
        classifier_type = 'NN'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds = Validate(cleanarr)
        else:
            count, correct_count, numeachclass, preds = Validate(cleanarr)
        #Correct Labels
        true_labels = cleanarr[:, -1]


        #Report Metrics
        model_cap = 680
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count
        
            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            if args.json:
                #                json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'n_classes':2, 'Number of False Negative Instances': num_FN, 'Number of False Positive Instances': num_FP, 'Number of True Positive Instances': num_TP, 'Number of True Negative Instances': num_TN,   'False Negatives': FN, 'False Positives': FP, 'True Negatives': TN, 'True Positives': TP, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0}
                json_dict = {'instance_count':                        count ,
                            'classifier_type':                        classifier_type ,
                            'n_classes':                            2 ,
                            'number_of_false_negative_instances':    num_FN ,
                            'number_of_false_positive_instances':    num_FP ,
                            'number_of_true_positive_instances':    num_TP ,
                            'number_of_true_negative_instances':    num_TN,
                            'false_negatives':                        FN ,
                            'false_positives':                        FP ,
                            'true_negatives':                        TN ,
                            'true_positives':                        TP ,
                            'number_correct':                        num_correct ,
                            'best_guess':                            randguess ,
                            'model_accuracy':                        modelacc ,
                            'model_capacity':                        model_cap ,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                             }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            if args.json:
        #        json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0, 'n_classes': n_classes}
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'n_classes':                            n_classes,
                            'number_correct':                        num_correct,
                            'best_guess':                            randguess,
                            'model_accuracy':                        modelacc,
                            'model_capacity':                        model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                            }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                sys.exit()
            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")


            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            n_labels = labels.size
            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]
            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm
        mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])

    #Clean Up
    if not args.cleanfile:
        os.remove(cleanfile)
        os.remove(preprocessedfile)


