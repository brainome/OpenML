#!/usr/bin/env python3
#
# This code has been produced by an evaluation version of Daimensions(tm).
# Portions of this code copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.
# Distribution of this code in binary form or commercial use of any kind is forbidden.
# For a detailed license agreement see: http://brainome.ai/license
# Use of predictions results at your own risk.
#
# Output of Brainome Daimensions(tm) 0.98 Table Compiler v0.98.
# Invocation: btc -f QC -target class -cm {'0':0,'1':1} christine.csv -o christine.py -nsamples 0 --yes -nsamples 0 -e 100
# Total compiler execution time: 0:50:12.68. Finished on: Sep-03-2020 10:33:42.
# This source code requires Python 3.
#
"""
Classifier Type:                     Decision Tree
System Type:                         Binary classifier
Best-guess accuracy:                 50.00%
Overall Model accuracy:              97.96% (5308/5418 correct)
Overall Improvement over best guess: 47.96% (of possible 50.0%)
Model capacity (MEC):                2411 bits
Generalization ratio:                2.20 bits/bit
Model efficiency:                    0.01%/parameter
System behavior
True Negatives:                      49.98% (2708/5418)
True Positives:                      47.99% (2600/5418)
False Negatives:                     2.01% (109/5418)
False Positives:                     0.02% (1/5418)
True Pos. Rate/Sensitivity/Recall:   0.96
True Neg. Rate/Specificity:          1.00
Precision:                           1.00
F-1 Measure:                         0.98
False Negative Rate/Miss Rate:       0.04
Critical Success Index:              0.96
Confusion Matrix:
 [49.98% 0.02%]
 [2.01% 47.99%]
Overfitting:                         No
Note: Unable to split dataset. The predictor was trained and evaluated on the same data.
Note: Labels have been remapped to '0'=0, '1'=1.
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "christine.csv"


#Number of attributes
num_attr = 1636
n_classes = 2


# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target="class"


def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target="class"
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return
    if (testfile):
        target = ''
        hc = -1
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    clean.mapping={'0':0,'1':1}

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([402886.5, 405781.0, 406939.5, 408644.0, 408915.5, 409565.5, 410750.0, 411009.0, 411276.5, 412252.0, 413175.5, 413307.0, 413386.5, 413462.5, 414173.0, 414610.5, 414743.0, 414900.0, 415010.0, 415062.0, 415111.0, 415208.5, 416047.5, 416497.5, 416692.0, 416718.5, 416732.5, 416932.0, 416976.0, 417081.5, 417872.0, 417956.5, 418126.0, 418189.5, 418242.5, 418439.0, 418668.0, 418785.0, 418962.0, 419331.0, 419401.5, 419446.0, 419735.5, 419766.5, 419870.0, 419966.0, 419984.0, 420077.5, 420218.5, 420259.5, 420393.0, 420472.5, 420507.0, 420554.0, 420773.0, 420843.5, 421032.0, 421118.0, 421434.5, 421476.0, 421698.5, 421717.0, 421750.5, 421783.5, 421831.0, 421986.0, 422262.0, 422276.0, 422313.0, 422330.5, 422647.5, 422738.5, 422847.0, 422905.0, 422941.5, 422957.5, 423143.0, 423197.0, 423243.0, 423375.0, 423598.0, 423705.0, 423865.5, 423942.0, 424024.0, 424059.0, 424097.0, 424124.0, 424156.5, 424235.5, 424257.0, 424327.5, 424357.5, 424362.5, 424386.0, 424395.5, 424435.0, 424485.0, 424524.5, 424572.0, 424603.0, 424649.0, 424712.5, 424745.5, 424847.5, 424871.0, 425172.0, 425193.5, 425226.0, 425233.5, 425253.0, 425272.5, 425592.0, 425641.5, 425927.0, 425941.0, 426020.5, 426045.0, 426082.0, 426106.0, 426388.5, 426409.5, 426548.0, 426574.5, 427094.0, 427140.5, 427186.0, 427197.5, 427281.0, 427324.5, 427346.5, 427379.0, 427529.5, 427580.5, 427714.5, 427716.5, 427752.5, 427814.0, 427847.0, 427888.0, 427946.0, 428116.0, 428166.5, 428219.0, 428301.0, 428320.0, 428338.5, 428351.0, 428407.0, 428446.5, 428733.5, 428760.0, 428789.0, 428807.0, 428820.5, 428866.5, 428921.0, 428979.0, 429112.0, 429161.0, 429309.5, 429401.0, 429655.5, 429770.0, 429813.0, 429861.0, 430024.0, 430046.5, 430112.5, 430151.0, 430198.5, 430230.0, 430285.0, 430310.5, 430354.0, 430445.5, 430453.0, 430488.0, 430584.5, 430592.0, 430648.0, 430653.5, 430662.5, 430672.0, 430685.5, 430702.0, 430840.5, 430876.5, 430929.5, 430941.0, 431048.0, 431067.5, 431112.0, 431130.0, 431146.0, 431179.5, 431209.5, 431242.5, 431352.0, 431477.0, 431552.5, 431558.0, 431663.5, 431744.0, 431750.0, 431767.0, 431829.5, 431845.0, 431857.5, 431862.5, 431870.0, 431895.0, 431919.5, 431930.5, 431971.0, 432017.5, 432049.5, 432086.5, 432136.5, 432167.5, 432197.5, 432206.0, 432216.5, 432225.0, 432246.5, 432308.0, 432390.0, 432406.5, 432479.5, 432526.0, 432545.5, 432554.5, 432571.5, 432630.0, 432640.5, 432671.5, 432755.0, 432788.5, 432864.5, 432901.0, 432992.0, 433002.5, 433047.5, 433080.0, 433089.5, 433127.5, 433179.5, 433193.5, 433230.5, 433275.5, 433355.5, 433387.0, 433575.0, 433624.0, 433660.0, 433669.5, 433709.5, 433773.0, 433863.0, 433887.0, 433975.0, 433991.5, 434028.5, 434109.5, 434160.0, 434161.5, 434194.0, 434229.5, 434251.0, 434263.5, 434276.5, 434296.5, 434323.0, 434347.5, 434424.0, 434435.5, 434551.5, 434577.0, 434746.5, 434751.5, 434767.5, 434784.5, 434802.5, 434832.0, 434890.5, 434891.5, 434961.5, 434970.5, 434987.5, 435043.0, 435086.0, 435093.0, 435159.0, 435221.5, 435261.0, 435285.5, 435289.5, 435298.0, 435451.5, 435457.0, 435484.5, 435490.0, 435531.0, 435576.5, 435635.5, 435660.0, 435690.0, 435692.5, 435814.5, 435821.0, 435909.5, 435927.0, 435948.0, 435974.5, 435998.5, 436021.0, 436028.0, 436045.0, 436062.0, 436069.0, 436075.0, 436080.5, 436091.0, 436101.5, 436110.0, 436127.5, 436164.5, 436209.5, 436212.0, 436265.0, 436273.0, 436299.5, 436325.5, 436338.5, 436352.5, 436361.5, 436372.0, 436401.5, 436424.5, 436429.5, 436436.0, 436448.5, 436464.0, 436473.5, 436493.5, 436519.5, 436527.0, 436551.5, 436577.5, 436588.5, 436772.5, 436782.5, 436826.0, 436836.5, 436848.0, 436867.5, 436934.0, 436951.5, 436965.5, 436980.0, 437010.5, 437023.5, 437047.0, 437064.5, 437126.0, 437140.5, 437148.5, 437156.5, 437166.0, 437172.0, 437195.5, 437225.0, 437258.0, 437284.5, 437299.5, 437327.5, 437362.0, 437382.0, 437413.5, 437455.0, 437463.5, 437485.0, 437504.5, 437529.0, 437552.5, 437589.0, 437617.0, 437660.5, 437753.5, 437765.0, 437795.5, 437836.5, 437856.5, 437867.5, 437883.0, 437901.0, 437986.5, 437998.0, 437999.5, 438013.5, 438160.5, 438197.0, 438217.5, 438284.0, 438298.5, 438303.5, 438342.5, 438358.5, 438426.0, 438444.5, 438522.5, 438527.5, 438588.5, 438606.5, 438665.0, 438735.5, 438755.5, 438776.0, 438860.0, 438879.0, 438898.0, 438927.0, 438969.5, 439007.0, 439147.5, 439157.0, 439259.5, 439278.5, 439303.5, 439330.5, 439373.0, 439403.0, 439412.0, 439416.0, 439420.0, 439427.0, 439437.0, 439445.0, 439460.0, 439475.5, 439490.0, 439519.5, 439605.5, 439624.0, 439640.5, 439649.0, 439686.5, 439691.5, 439749.5, 439759.0, 439765.0, 439778.5, 439840.0, 439852.5, 439986.0, 439990.5, 440052.5, 440115.0, 440140.0, 440179.0, 440233.5, 440247.5, 440300.0, 440362.5, 440378.5, 440387.5, 440457.5, 440478.5, 440491.5, 440505.5, 440525.0, 440543.5, 440561.0, 440574.5, 440618.0, 440636.0, 440695.0, 440755.0, 440768.0, 440785.5, 440806.5, 440823.0, 440848.5, 440850.0, 440861.0, 440881.0, 440887.5, 440896.5, 440944.0, 440969.5, 441002.0, 441031.0, 441057.0, 441080.0, 441094.0, 441149.0, 441159.0, 441165.0, 441173.5, 441181.5, 441237.5, 441298.5, 441321.0, 441351.5, 441371.0, 441389.5, 441495.5, 441567.0, 441605.5, 441618.5, 441622.5, 441632.0, 441675.5, 441685.0, 441704.5, 441736.5, 441749.5, 441775.5, 441790.0, 441806.0, 441817.5, 441819.0, 441827.5, 441898.5, 441935.5, 441958.0, 441997.5, 442005.5, 442011.0, 442062.5, 442065.0, 442097.5, 442132.5, 442159.0, 442213.0, 442228.0, 442276.5, 442288.0, 442358.0, 442370.5, 442431.5, 442448.5, 442456.5, 442467.5, 442474.0, 442477.0, 442484.5, 442491.0, 442503.0, 442518.5, 442528.5, 442538.0, 442552.5, 442571.5, 442574.0, 442602.0, 442666.5, 442675.0, 442760.0, 442778.5, 442789.0, 442839.5, 442867.0, 442879.5, 442890.0, 442899.5, 443005.0, 443033.5, 443063.5, 443076.0, 443097.5, 443102.5, 443111.5, 443119.5, 443181.5, 443200.0, 443210.0, 443213.0, 443216.0, 443243.0, 443280.0, 443301.0, 443316.0, 443322.0, 443384.5, 443407.0, 443441.0, 443518.5, 443552.0, 443596.5, 443633.5, 443669.5, 443688.5, 443701.0, 443721.5, 443744.0, 443759.5, 443767.0, 443816.0, 443835.0, 443879.0, 443890.0, 443943.0, 443982.0, 444023.5, 444039.5, 444096.5, 444112.0, 444114.0, 444117.0, 444124.0, 444131.0, 444151.5, 444165.5, 444196.0, 444215.5, 444229.0, 444248.5, 444266.0, 444299.5, 444333.5, 444342.5, 444348.0, 444389.5, 444392.5, 444461.0, 444497.0, 444543.5, 444598.0, 444614.0, 444628.5, 444644.5, 444652.5, 444656.0, 444687.0, 444708.5, 444723.0, 444765.0, 444770.5, 444777.0, 444802.0, 444809.5, 444837.5, 444840.0, 444845.5, 444855.0, 444862.5, 444867.0, 444906.0, 444915.5, 444952.5, 444993.0, 445012.0, 445025.0, 445038.0, 445042.0, 445045.5, 445056.0, 445087.5, 445099.5, 445106.5, 445111.0, 445115.5, 445128.5, 445178.5, 445201.5, 445214.5, 445223.5, 445243.0, 445258.0, 445301.0, 445347.0, 445355.5, 445382.0, 445389.5, 445433.5, 445468.5, 445474.0, 445483.5, 445495.5, 445519.0, 445543.5, 445557.0, 445569.5, 445582.0, 445612.0, 445636.5, 445654.0, 445661.5, 445665.5, 445687.0, 445695.5, 445698.5, 445702.0, 445705.5, 445722.5, 445787.0, 445793.0, 445798.5, 445809.5, 445827.0, 445829.5, 445834.0, 445839.0, 445844.0, 445850.0, 445856.5, 445861.5, 445865.0, 445869.0, 445876.0, 445906.0, 445938.5, 445961.0, 446007.5, 446033.0, 446041.5, 446051.0, 446070.5, 446080.5, 446094.5, 446103.5, 446142.5, 446153.5, 446171.0, 446178.5, 446187.5, 446196.5, 446223.0, 446226.5, 446236.0, 446254.5, 446259.0, 446283.5, 446320.5, 446341.5, 446365.5, 446389.5, 446402.0, 446420.0, 446440.5, 446458.5, 446471.5, 446475.0, 446491.0, 446506.0, 446521.5, 446532.0, 446558.0, 446625.0, 446677.0, 446688.0, 446701.0, 446730.5, 446764.0, 446794.5, 446841.0, 446894.0, 446898.0, 446904.5, 446910.5, 446918.0, 447042.0, 447078.5, 447108.5, 447164.5, 447172.5, 447207.0, 447287.5, 447301.5, 447315.5, 447375.0, 447392.5, 447409.5, 447416.5, 447424.5, 447435.5, 447442.5, 447446.5, 447447.5, 447449.5, 447462.5, 447521.0, 447524.5, 447529.0, 447532.0, 447588.0, 447601.0, 447612.5, 447619.0, 447632.5, 447651.5, 447659.5, 447685.5, 447739.5, 447792.5, 447808.5, 447836.5, 447848.0, 447854.0, 447877.0, 447906.0, 447930.5, 447941.5, 447947.0, 447950.0, 447954.0, 447958.0, 447983.5, 447998.5, 448000.5, 448008.5, 448024.0, 448034.0, 448068.0, 448075.5, 448112.5, 448139.0, 448155.5, 448157.5, 448205.0, 448250.5, 448294.0, 448301.5, 448306.0, 448340.5, 448367.5, 448377.0, 448378.5, 448380.5, 448390.5, 448392.0, 448396.5, 448406.5, 448416.5, 448435.5, 448487.5, 448497.0, 448536.5, 448559.0, 448583.0, 448602.0, 448616.0, 448628.5, 448687.5, 448693.5, 448706.0, 448713.0, 448732.5, 448735.0, 448783.5, 448796.5, 448811.0, 448831.5, 448837.0, 448847.0, 448897.0, 448906.5, 448923.0, 448960.5, 448963.5, 448983.5, 449019.5, 449023.5, 449027.5, 449040.0, 449050.5, 449058.0, 449073.0, 449080.0, 449095.0, 449107.0, 449152.5, 449161.5, 449195.5, 449215.0, 449231.5, 449260.0, 449280.5, 449290.0, 449314.5, 449356.0, 449362.0, 449374.0, 449424.0, 449439.0, 449453.0, 449461.5, 449494.5, 449498.5, 449503.5, 449508.5, 449521.5, 449536.5, 449541.0, 449544.5, 449550.0, 449558.0, 449586.0, 449614.0, 449632.5, 449640.5, 449666.5, 449679.5, 449697.0, 449703.5, 449717.0, 449737.0, 449754.5, 449767.0, 449777.0, 449783.5, 449794.5, 449816.0, 449833.0, 449843.0, 449858.0, 449864.5, 449867.5, 449871.5, 449919.5, 449937.5, 449940.5, 449963.5, 449979.5, 449990.5, 449997.0, 450000.0, 450043.5, 450073.0, 450083.5, 450102.0, 450108.5, 450122.0, 450139.5, 450157.0, 450216.5, 450219.5, 450224.5, 450233.0, 450247.5, 450256.5, 450286.5, 450296.5, 450300.5, 450326.5, 450343.5, 450354.5, 450400.0, 450415.0, 450420.5, 450442.0, 450468.5, 450476.0, 450525.0, 450538.0, 450545.5, 450569.5, 450576.0, 450616.5, 450639.0, 450643.5, 450660.5, 450705.0, 450717.5, 450735.0, 450738.0, 450748.5, 450751.0, 450755.5, 450779.5, 450791.0, 450797.5, 450813.5, 450837.5, 450845.0, 450874.5, 450877.5, 450883.0, 450911.5, 450923.0, 450939.5, 450944.0, 450956.5, 450992.5, 450995.5, 451058.5, 451074.0, 451080.5, 451162.0, 451167.5, 451182.0, 451190.5, 451200.5, 451261.0, 451271.0, 451290.0, 451295.0, 451298.0, 451431.5, 451442.5, 451454.5, 451473.0, 451485.5, 451494.5, 451497.5, 451520.5, 451558.5, 451642.0, 451673.5, 451679.5, 451707.5, 451740.5, 451757.5, 451853.5, 451866.5, 451880.5, 451893.5, 451906.5, 451920.5, 451945.5, 451957.0, 451984.0, 451997.0, 452012.0, 452024.5, 452039.5, 452055.5, 452097.0, 452110.0, 452194.0, 452195.5, 452197.5, 452226.5, 452232.5, 452264.5, 452294.0, 452315.0, 452344.5, 452353.5, 452381.5, 452398.5, 452410.5, 452423.0, 452430.0, 452467.0, 452487.5, 452499.5, 452507.0, 452516.5, 452527.0, 452532.5, 452566.5, 452573.0, 452576.5, 452582.0, 452611.5, 452619.5, 452624.0, 452626.0, 452634.0, 452655.0, 452658.5, 452662.0, 452665.0, 452681.5, 452684.0, 452691.0, 452720.0, 452730.5, 452733.0, 452744.5, 452745.5, 452747.0, 452748.5, 452781.0, 452787.5, 452794.0, 452797.5, 452821.5, 452833.5, 452847.5, 452866.5, 452898.0, 452909.0, 452922.0, 452942.0, 452952.5, 452961.0, 452972.5, 453005.5, 453021.5, 453031.5, 453052.5, 453064.0, 453082.0, 453096.5, 453110.0, 453154.5, 453189.0, 453210.5, 453218.0, 453220.5, 453247.0, 453282.0, 453296.5, 453307.0, 453344.5, 453356.5, 453416.5, 453439.0, 453470.0, 453500.5, 453507.5, 453516.0, 453527.5, 453538.5, 453567.0, 453586.5, 453594.5, 453620.0, 453632.0, 453647.5, 453666.5, 453739.5, 453744.0, 453765.5, 453778.0, 453798.0, 453830.5, 453846.5, 453860.0, 453873.5, 453884.5, 453894.0, 453909.5, 453928.0, 453933.5, 453951.0, 453962.5, 453969.5, 453999.5, 454055.5, 454077.0, 454086.0, 454111.5, 454123.0, 454129.5, 454141.0, 454150.5, 454169.5, 454191.0, 454211.0, 454218.5, 454227.0, 454236.5, 454258.0, 454270.5, 454280.0, 454284.5, 454298.5, 454313.0, 454330.0, 454335.5, 454419.5, 454431.5, 454443.0, 454468.0, 454483.0, 454492.0, 454499.5, 454510.5, 454565.0, 454598.5, 454608.0, 454613.0, 454616.5, 454621.5, 454628.5, 454658.5, 454677.0, 454694.0, 454705.5, 454713.5, 454723.5, 454738.5, 454762.5, 454787.5, 454826.5, 454871.5, 454878.0, 454892.0, 454900.5, 454917.5, 454962.5, 454972.5, 454985.0, 454999.5, 455017.5, 455049.0, 455056.5, 455066.5, 455073.0, 455076.5, 455176.5, 455263.0, 455280.0, 455307.5, 455325.5, 455330.5, 455351.5, 455376.5, 455381.0, 455387.0, 455394.0, 455399.0, 455403.5, 455412.5, 455425.0, 455435.5, 455447.5, 455458.5, 455467.0, 455480.5, 455501.5, 455534.5, 455538.5, 455553.5, 455573.5, 455613.0, 455630.5, 455640.5, 455652.0, 455675.0, 455691.0, 455698.5, 455705.0, 455718.0, 455723.0, 455726.5, 455730.0, 455731.5, 455732.5, 455739.5, 455769.0, 455773.5, 455778.0, 455783.5, 455784.5, 455791.0, 455797.5, 455799.0, 455821.0, 455823.5, 455842.0, 455844.5, 455854.0, 455868.0, 455876.5, 455903.0, 455909.5, 455937.5, 455963.0, 456013.0, 456033.5, 456036.5, 456042.0, 456062.0, 456079.5, 456128.0, 456138.0, 456194.0, 456200.5, 456206.0, 456263.5, 456281.0, 456292.5, 456300.0, 456336.5, 456354.0, 456392.0, 456401.5, 456412.0, 456442.5, 456457.0, 456562.5, 456570.5, 456572.5, 456586.0, 456599.5, 456609.5, 456672.0, 456683.5, 456686.0, 456695.0, 456728.0, 456729.5, 456751.0, 456764.0, 456773.0, 456784.5, 456789.5, 456799.0, 456831.0, 456853.0, 456871.5, 456877.0, 456886.0, 456891.0, 456915.5, 456927.0, 456941.5, 456954.5, 456999.5, 457018.5, 457022.5, 457026.5, 457029.5, 457063.5, 457066.5, 457075.0, 457089.5, 457117.0, 457131.0, 457151.0, 457165.5, 457177.0, 457179.0, 457189.0, 457204.0, 457214.0, 457219.0, 457288.0, 457347.0, 457358.5, 457410.5, 457415.5, 457419.0, 457427.5, 457448.5, 457454.0, 457455.5, 457485.0, 457489.5, 457493.0, 457517.5, 457553.5, 457561.5, 457568.5, 457573.0, 457589.0, 457621.5, 457625.5, 457643.5, 457664.0, 457665.5, 457667.5, 457678.5, 457710.0, 457716.0, 457719.5, 457723.0, 457736.5, 457792.5, 457835.0, 457864.0, 457875.5, 457881.0, 457924.5, 457957.5, 457973.0, 457986.5, 458006.5, 458017.0, 458023.5, 458041.0, 458044.5, 458046.0, 458051.5, 458057.5, 458060.0, 458061.5, 458096.0, 458115.5, 458128.0, 458144.0, 458146.5, 458147.5, 458160.5, 458173.5, 458190.0, 458217.0, 458226.5, 458258.0, 458271.5, 458274.5, 458278.5, 458287.0, 458309.0, 458339.5, 458358.5, 458361.5, 458363.5, 458374.5, 458396.0, 458441.5, 458443.5, 458465.0, 458486.5, 458495.0, 458501.5, 458527.5, 458537.0, 458551.0, 458561.5, 458574.0, 458594.0, 458602.0, 458619.5, 458650.5, 458671.5, 458675.5, 458678.5, 458684.5, 458691.5, 458746.0, 458752.5, 458796.5, 458820.5, 458822.5, 458831.5, 458849.5, 458894.0, 458909.5, 458927.5, 458930.0, 458937.5, 458961.5, 459021.0, 459043.5, 459072.0, 459121.5, 459123.5, 459125.0, 459126.5, 459160.5, 459188.0, 459196.5, 459228.5, 459250.5, 459260.5, 459263.5, 459266.0, 459297.5, 459299.5, 459315.5, 459323.5, 459334.5, 459369.5, 459379.5, 459387.0, 459403.0, 459418.5, 459430.0, 459441.0, 459486.5, 459496.5, 459503.5, 459554.0, 459561.5, 459582.5, 459587.0, 459597.0, 459615.0, 459671.0, 459691.5, 459715.0, 459727.5, 459733.5, 459754.0, 459766.5, 459773.0, 459779.5, 459791.5, 459795.5, 459842.5, 459848.5, 459858.5, 459870.5, 459876.0, 459933.5, 459956.5, 459976.5, 459992.5, 459996.5, 459998.5, 460017.5, 460037.5, 460041.5, 460084.5, 460118.5, 460123.5, 460145.0, 460158.5, 460219.0, 460228.5, 460244.5, 460260.5, 460310.5, 460325.5, 460345.5, 460373.0, 460384.5, 460385.5, 460412.5, 460415.5, 460444.5, 460460.0, 460494.5, 460534.5, 460540.0, 460556.5, 460593.0, 460647.5, 460650.0, 460672.0, 460689.5, 460705.5, 460711.5, 460716.0, 460724.0, 460732.5, 460765.5, 460774.0, 460789.0, 460797.5, 460811.5, 460813.5, 460816.0, 460818.0, 460821.0, 460882.0, 460892.0, 460919.0, 460927.5, 460937.0, 460944.0, 460952.0, 460967.5, 460980.0, 460987.0, 460996.0, 461005.5, 461021.0, 461032.0, 461048.0, 461073.5, 461087.0, 461091.0, 461126.5, 461132.5, 461171.5, 461206.0, 461299.5, 461312.0, 461326.5, 461335.5, 461362.5, 461401.0, 461428.0, 461467.5, 461479.5, 461484.5, 461485.5, 461487.5, 461491.0, 461507.0, 461534.0, 461544.5, 461583.5, 461596.5, 461606.0, 461612.5, 461632.0, 461649.0, 461666.5, 461687.0, 461736.5, 461742.0, 461749.0, 461800.5, 461814.0, 461823.5, 461848.5, 461920.5, 461931.0, 461953.5, 461975.0, 461983.5, 461999.0, 462029.5, 462059.0, 462072.5, 462104.5, 462120.0, 462153.5, 462174.5, 462202.5, 462224.5, 462258.5, 462287.0, 462322.0, 462334.5, 462341.0, 462391.0, 462395.0, 462407.5, 462431.5, 462462.0, 462485.5, 462505.5, 462524.0, 462538.5, 462608.5, 462610.5, 462622.5, 462643.0, 462662.5, 462667.5, 462669.5, 462703.0, 462718.0, 462726.5, 462739.0, 462749.0, 462757.5, 462775.5, 462816.0, 462819.0, 462824.0, 462831.0, 462838.0, 462843.5, 462855.0, 462882.0, 462934.5, 462944.0, 462964.5, 462978.5, 463008.0, 463031.0, 463040.0, 463064.0, 463089.0, 463099.0, 463103.0, 463109.5, 463114.0, 463115.5, 463121.5, 463136.0, 463174.5, 463231.0, 463239.5, 463270.0, 463274.5, 463277.0, 463283.5, 463289.0, 463321.0, 463327.5, 463341.0, 463351.5, 463422.5, 463426.0, 463441.0, 463470.0, 463534.5, 463549.0, 463625.0, 463631.0, 463664.0, 463677.0, 463696.5, 463745.0, 463766.5, 463771.5, 463797.5, 463816.0, 463833.5, 463845.5, 463898.0, 463917.5, 463941.0, 463968.0, 463985.5, 464000.0, 464014.0, 464037.5, 464049.5, 464080.0, 464118.0, 464127.5, 464184.5, 464209.0, 464243.0, 464251.5, 464260.0, 464296.0, 464354.5, 464378.0, 464396.5, 464409.0, 464443.5, 464504.0, 464521.5, 464527.0, 464545.0, 464555.5, 464570.5, 464590.5, 464626.0, 464635.5, 464682.5, 464695.5, 464710.5, 464726.0, 464739.0, 464741.5, 464747.5, 464755.0, 464786.5, 464790.5, 464809.0, 464832.0, 464868.0, 464880.5, 464925.0, 464940.5, 464942.0, 465007.0, 465068.5, 465097.5, 465109.5, 465120.5, 465138.0, 465153.0, 465190.5, 465199.5, 465215.0, 465224.5, 465266.5, 465286.5, 465314.5, 465317.5, 465322.0, 465330.5, 465375.5, 465388.0, 465422.0, 465474.5, 465498.5, 465515.5, 465522.5, 465526.0, 465667.5, 465681.5, 465728.0, 465740.0, 465752.0, 465754.5, 465762.0, 465785.5, 465824.5, 465840.0, 465855.0, 465857.5, 465882.5, 465941.5, 465967.5, 465980.0, 466001.0, 466008.5, 466022.0, 466030.5, 466034.5, 466072.0, 466108.0, 466141.0, 466151.0, 466168.5, 466195.0, 466199.0, 466202.5, 466208.5, 466210.5, 466234.0, 466305.5, 466307.5, 466313.5, 466326.0, 466362.5, 466368.5, 466378.5, 466421.0, 466530.5, 466551.0, 466573.5, 466596.5, 466616.5, 466660.5, 466665.0, 466670.0, 466701.0, 466758.5, 466795.5, 466801.0, 466809.0, 466826.0, 466852.0, 466887.5, 466912.0, 466918.5, 466972.0, 466996.5, 467022.0, 467037.5, 467058.5, 467070.0, 467074.5, 467089.5, 467141.0, 467159.0, 467224.5, 467254.5, 467324.5, 467330.0, 467336.5, 467344.0, 467353.0, 467359.0, 467367.0, 467395.0, 467422.0, 467447.5, 467462.5, 467464.5, 467467.5, 467476.5, 467540.0, 467550.5, 467595.0, 467618.0, 467623.5, 467631.0, 467664.0, 467669.5, 467688.0, 467724.5, 467752.5, 467754.0, 467776.0, 467785.5, 467798.5, 467846.5, 467849.0, 467853.0, 467927.5, 467957.5, 468043.5, 468052.0, 468058.0, 468061.5, 468065.0, 468090.0, 468098.5, 468123.0, 468214.0, 468227.0, 468231.0, 468233.5, 468235.5, 468239.0, 468259.0, 468279.0, 468287.0, 468299.5, 468313.5, 468336.0, 468372.0, 468429.5, 468474.5, 468484.0, 468491.0, 468523.5, 468556.5, 468565.0, 468571.5, 468594.0, 468626.0, 468638.5, 468676.0, 468684.0, 468711.0, 468724.0, 468773.5, 468777.5, 468781.5, 468797.0, 468811.5, 468833.0, 468855.0, 468863.5, 469076.5, 469087.5, 469099.5, 469106.0, 469120.0, 469139.5, 469146.5, 469153.5, 469277.5, 469287.0, 469331.0, 469343.0, 469346.0, 469393.5, 469416.0, 469445.0, 469483.0, 469513.0, 469519.0, 469522.0, 469531.0, 469541.5, 469556.5, 469585.5, 469598.5, 469613.0, 469627.5, 469683.5, 469695.5, 469704.5, 469722.5, 469745.0, 469760.0, 469770.5, 469832.5, 469836.5, 469888.5, 469911.5, 469919.0, 469925.0, 470035.0, 470048.5, 470060.5, 470073.5, 470077.5, 470095.5, 470137.5, 470170.0, 470208.5, 470234.0, 470254.5, 470286.0, 470332.5, 470348.0, 470448.0, 470459.0, 470474.5, 470482.0, 470525.5, 470540.0, 470564.0, 470567.5, 470701.0, 470709.0, 470717.5, 470749.0, 470760.5, 470770.5, 470821.0, 470870.5, 470913.5, 470934.0, 471012.0, 471034.5, 471057.0, 471071.5, 471082.5, 471084.0, 471111.0, 471114.0, 471126.5, 471134.0, 471140.0, 471147.5, 471155.5, 471165.0, 471326.0, 471335.0, 471391.0, 471403.5, 471422.0, 471442.5, 471472.0, 471477.5, 471582.5, 471589.0, 471593.5, 471609.0, 471625.5, 471652.0, 471788.0, 471805.0, 471903.5, 471919.0, 471948.0, 471963.0, 471978.5, 471984.0, 471998.5, 472016.0, 472025.0, 472056.0, 472127.0, 472134.5, 472162.0, 472198.5, 472204.5, 472210.5, 472214.0, 472226.0, 472258.5, 472265.5, 472312.5, 472346.5, 472365.0, 472380.0, 472471.5, 472495.0, 472555.0, 472565.0, 472613.0, 472640.5, 472653.5, 472656.0, 472671.5, 472724.0, 472732.5, 472749.0, 472831.0, 472855.5, 472952.5, 472961.0, 473004.0, 473007.0, 473013.0, 473029.5, 473104.5, 473109.0, 473116.5, 473124.0, 473131.5, 473142.0, 473146.5, 473176.0, 473179.0, 473180.5, 473281.5, 473291.0, 473303.5, 473319.5, 473327.5, 473332.0, 473339.5, 473379.0, 473385.5, 473392.0, 473431.5, 473461.5, 473489.0, 473491.5, 473566.0, 473573.0, 473578.0, 473597.0, 473621.0, 473639.0, 473696.5, 473703.5, 473735.5, 473764.0, 473773.5, 473833.5, 473880.0, 473907.0, 473992.0, 474015.0, 474033.0, 474043.5, 474082.0, 474102.5, 474207.0, 474227.0, 474241.0, 474261.5, 474271.0, 474273.0, 474279.0, 474367.0, 474441.5, 474460.5, 474599.5, 474609.5, 474616.0, 474638.0, 474776.0, 474792.0, 474863.5, 474870.0, 474873.5, 474886.5, 474972.5, 474983.0, 475001.5, 475034.5, 475112.0, 475114.5, 475123.0, 475162.5, 475185.5, 475191.0, 475230.0, 475296.0, 475325.5, 475331.5, 475355.5, 475371.5, 475388.0, 475394.5, 475406.5, 475413.5, 475419.0, 475440.5, 475463.5, 475493.5, 475548.5, 475553.0, 475575.0, 475617.0, 475671.5, 475692.0, 475706.0, 475732.0, 475815.5, 475830.0, 475842.5, 475857.0, 475918.0, 475952.5, 476010.0, 476049.5, 476061.0, 476093.0, 476104.5, 476118.0, 476129.5, 476138.0, 476146.5, 476171.5, 476238.0, 476267.5, 476294.0, 476299.0, 476363.5, 476378.5, 476448.5, 476458.5, 476486.0, 476489.5, 476499.0, 476508.5, 476538.0, 476603.5, 476625.5, 476670.5, 476677.0, 476722.5, 476863.5, 476883.5, 476913.5, 477004.5, 477032.0, 477047.0, 477214.5, 477245.5, 477349.0, 477417.0, 477453.5, 477538.0, 477639.0, 477658.5, 477692.5, 477720.5, 477752.5, 477760.0, 477775.5, 477791.0, 477795.0, 477802.0, 477922.0, 477937.0, 477943.0, 477951.5, 478145.5, 478177.0, 478200.5, 478220.0, 478355.5, 478393.5, 478457.5, 478521.0, 478568.5, 478581.5, 478588.5, 478606.0, 478623.0, 478640.5, 478657.0, 478675.5, 478959.0, 478962.0, 479014.0, 479041.0, 479073.0, 479099.5, 479126.5, 479135.0, 479175.5, 479234.0, 479236.5, 479245.0, 479398.0, 479489.5, 479503.0, 479556.5, 479616.0, 479654.0, 479695.5, 479734.0, 479820.0, 479824.0, 480008.5, 480073.0, 480106.0, 480144.0, 480172.0, 480199.5, 480349.5, 480367.5, 480377.0, 480383.0, 480414.0, 480423.0, 480442.5, 480487.5, 480638.0, 480656.0, 480676.0, 480705.0, 480848.0, 480852.5, 480865.0, 480873.5, 480902.5, 480961.5, 481129.5, 481168.5, 481267.5, 481275.5, 481302.5, 481372.5, 481500.5, 481506.0, 481606.5, 481638.5, 481675.0, 481728.5, 481763.5, 481772.5, 481795.0, 481824.0, 481892.5, 481906.5, 482060.0, 482195.0, 482210.5, 482214.0, 482281.0, 482291.0, 482393.0, 482424.5, 482463.5, 482492.5, 482527.0, 482551.0, 482609.5, 482643.0, 482664.0, 482674.5, 482688.0, 482699.5, 482782.5, 482807.5, 482978.5, 482983.5, 483194.0, 483218.0, 483332.5, 483369.0, 483422.0, 483461.0, 483510.0, 483585.5, 483818.5, 483894.5, 483904.0, 483910.5, 484080.5, 484167.5, 484457.5, 484576.0, 484718.0, 484828.5, 484880.0, 484899.5, 485030.0, 485046.5, 485272.0, 485349.5, 485621.5, 485731.0, 485800.5, 485813.5, 485825.5, 485917.5, 485993.0, 486018.5, 486128.5, 486161.5, 486207.0, 486245.5, 486368.0, 486377.5, 486765.5, 486849.5, 487086.5, 487111.0, 487412.0, 487496.0, 487797.0, 487894.0, 487947.5, 488018.0, 488069.0, 488105.0, 488293.0, 488349.5, 488395.5, 488463.5, 488575.5, 488642.5, 488685.5, 488736.0, 489062.0, 489094.0, 489128.0, 489139.5, 489371.5, 489384.5, 489940.5, 490064.0, 490260.5, 490353.5, 490441.5, 490554.5, 490994.5, 491388.0, 491414.0, 491417.5, 491524.0, 491934.5, 492234.5, 492286.5, 492847.0, 492942.5, 493330.5, 493376.5, 493940.0, 494124.5, 494683.0, 494791.5, 495564.5, 495652.5, 496512.5, 496788.0, 497799.0, 498028.0, 512522.5, 538063.0])
def eqenergy(rows):
    return np.sum(rows, axis=1)
def classify(rows):
    energys = eqenergy(rows)
    start_label = 0
    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = (numers[indys] + start_label) % 2
        outputs[defaultindys]=1
        return outputs
    return thresh_search(energys)

numthresholds = 2411



# Main method
model_cap = numthresholds


def Validate(file):
    #Load Array
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs, cleanarr[:, -1]


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:, -1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    cleanarr = cleanarr.reshape(cleanarr.shape[0], -1)
    with open(preprocessedfile, 'r') as csvinput:
        dirtyreader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(dirtyreader, None) + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            print(str(','.join(str(j) for j in (['"' + i + '"' if ',' in i else i for i in row]))) + ',' + str(get_key(int(outputs[k]), classmapping)))



#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    args = parser.parse_args()
    faulthandler.enable()

    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile, classmapping)


    else:
        classifier_type = 'DT'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds, true_labels = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count
        
            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            if args.json:
                #                json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'n_classes':2, 'Number of False Negative Instances': num_FN, 'Number of False Positive Instances': num_FP, 'Number of True Positive Instances': num_TP, 'Number of True Negative Instances': num_TN,   'False Negatives': FN, 'False Positives': FP, 'True Negatives': TN, 'True Positives': TP, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0}
                json_dict = {'instance_count':                        count ,
                            'classifier_type':                        classifier_type ,
                            'n_classes':                            2 ,
                            'number_of_false_negative_instances':    num_FN ,
                            'number_of_false_positive_instances':    num_FP ,
                            'number_of_true_positive_instances':    num_TP ,
                            'number_of_true_negative_instances':    num_TN,
                            'false_negatives':                        FN ,
                            'false_positives':                        FP ,
                            'true_negatives':                        TN ,
                            'true_positives':                        TP ,
                            'number_correct':                        num_correct ,
                            'best_guess':                            randguess ,
                            'model_accuracy':                        modelacc ,
                            'model_capacity':                        model_cap ,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                             }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            if args.json:
        #        json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0, 'n_classes': n_classes}
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'n_classes':                            n_classes,
                            'number_correct':                        num_correct,
                            'best_guess':                            randguess,
                            'model_accuracy':                        modelacc,
                            'model_capacity':                        model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                            }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                sys.exit()
            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")


            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            n_labels = labels.size
            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]
            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm
        mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        os.remove(preprocessedfile)


