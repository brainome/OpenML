#!/usr/bin/env python3
#
# This code has been produced by an evaluation version of Daimensions(tm).
# Portions of this code copyright (c) 2019, 2020 by Brainome, Inc. All Rights Reserved.
# Distribution of this code in binary form or commercial use of any kind is forbidden.
# For a detailed license agreement see: http://brainome.ai/license
# Use of predictions results at your own risk.
#
# Output of Brainome Daimensions(tm) 0.98 Table Compiler v0.98.
# Invocation: btc -f QC -target match SpeedDating.csv -o SpeedDating.py -nsamples 0 --yes -nsamples 0 -e 100
# Total compiler execution time: 0:00:21.33. Finished on: Sep-03-2020 18:05:43.
# This source code requires Python 3.
#
"""
Classifier Type:                     Decision Tree
System Type:                         Binary classifier
Best-guess accuracy:                 83.52%
Overall Model accuracy:              100.00% (8378/8378 correct)
Overall Improvement over best guess: 16.48% (of possible 16.48%)
Model capacity (MEC):                2295 bits
Generalization ratio:                3.65 bits/bit
Model efficiency:                    0.00%/parameter
System behavior
True Negatives:                      83.53% (6998/8378)
True Positives:                      16.47% (1380/8378)
False Negatives:                     0.00% (0/8378)
False Positives:                     0.00% (0/8378)
True Pos. Rate/Sensitivity/Recall:   1.00
True Neg. Rate/Specificity:          1.00
Precision:                           1.00
F-1 Measure:                         1.00
False Negative Rate/Miss Rate:       0.00
Critical Success Index:              1.00
Confusion Matrix:
 [83.53% 0.00%]
 [0.00% 16.47%]
Overfitting:                         No
Note: Unable to split dataset. The predictor was trained and evaluated on the same data.
"""

# Imports -- Python3 standard library
import sys
import math
import os
import argparse
import tempfile
import csv
import binascii
import faulthandler

# Imports -- external
try:
    import numpy as np # For numpy see: http://numpy.org
    from numpy import array
except:
    print("This predictor requires the Numpy library. For installation instructions please refer to: http://numpy.org")

# Magic constants follow
# I/O buffer for clean. Reduce this constant for low memory devices. 
IOBUF = 100000000

# Ugly workaround for large classifiers
sys.setrecursionlimit(1000000)

# Training file given to compiler
TRAINFILE = "SpeedDating.csv"


#Number of attributes
num_attr = 122
n_classes = 2


# Preprocessor for CSV files

ignorelabels=[]
ignorecolumns=[]
target="match"


def preprocess(inputcsvfile, outputcsvfile, headerless=False, testfile=False, target='', ignorecolumns=[], ignorelabels=[]):
    #This function streams in a csv and outputs a csv with the correct columns and target column on the right hand side. 
    #Precursor to clean

    il=[]

    ignorelabels=[]
    ignorecolumns=[]
    target="match"
    if ignorelabels == [] and ignorecolumns == [] and target == "":
        return
    if (testfile):
        target = ''
        hc = -1
    
    with open(outputcsvfile, "w+") as outputfile:
        with open(inputcsvfile) as csvfile:
            reader = csv.reader(csvfile)
            if (headerless == False):
                header=next(reader, None)
                try:
                    if not testfile:
                        if (target != ''): 
                            hc = header.index(target)
                        else:
                            hc = len(header) - 1
                            target=header[hc]
                except:
                    raise NameError("Target '" + target + "' not found! Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = header.index(ignorecolumns[i])
                        if not testfile:
                            if (col == hc):
                                raise ValueError("Attribute '" + ignorecolumns[i] + "' is the target. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise NameError("Attribute '" + ignorecolumns[i] + "' not found in header. Header must be same as in file passed to btc.")
                first = True
                for i in range(0, len(header)):

                    if (i == hc):
                        continue
                    if (i in il):
                        continue
                    if first:
                        first = False
                    else:
                        print(",", end='', file=outputfile)
                    print(header[i], end='', file=outputfile)
                if not testfile:
                    print("," + header[hc], file=outputfile)
                else:
                    print("", file=outputfile)

                for row in csv.DictReader(open(inputcsvfile)):
                    if target and (row[target] in ignorelabels):
                        continue
                    first = True
                    for name in header:
                        if (name in ignorecolumns):
                            continue
                        if (name == target):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[name]):
                            print('"' + row[name].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[name].replace('"', ''), end='', file=outputfile)
                    if not testfile:
                        print("," + row[target], file=outputfile)
                    else:
                        print("", file=outputfile)

            else:
                try:
                    if (target != ""): 
                        hc = int(target)
                    else:
                        hc = -1
                except:
                    raise NameError("No header found but attribute name given as target. Header must be same as in file passed to btc.")
                for i in range(0, len(ignorecolumns)):
                    try:
                        col = int(ignorecolumns[i])
                        if (col == hc):
                            raise ValueError("Attribute " + str(col) + " is the target. Cannot ignore. Header must be same as in file passed to btc.")
                        il = il + [col]
                    except ValueError:
                        raise
                    except:
                        raise ValueError("No header found but attribute name given in ignore column list. Header must be same as in file passed to btc.")
                for row in reader:
                    first = True
                    if (hc == -1) and (not testfile):
                        hc = len(row) - 1
                    if (row[hc] in ignorelabels):
                        continue
                    for i in range(0, len(row)):
                        if (i in il):
                            continue
                        if (i == hc):
                            continue
                        if first:
                            first = False
                        else:
                            print(",", end='', file=outputfile)
                        if (',' in row[i]):
                            print('"' + row[i].replace('"', '') + '"', end='', file=outputfile)
                        else:
                            print(row[i].replace('"', ''), end = '', file=outputfile)
                    if not testfile:
                        print("," + row[hc], file=outputfile)
                    else:
                        print("", file=outputfile)


def clean(filename, outfile, rounding=-1, headerless=False, testfile=False):
    #This function takes a preprocessed csv and cleans it to real numbers for prediction or validation


    clean.classlist = []
    clean.testfile = testfile
    clean.mapping = {}
    

    def convert(cell):
        value = str(cell)
        try:
            result = int(value)
            return result
        except:
            try:
                result = float(value)
                if (rounding != -1):
                    result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
                return result
            except:
                result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
                return result

    #Function to return key for any value 
    def get_key(val, clean_classmapping):
        if clean_classmapping == {}:
            return val
        for key, value in clean_classmapping.items(): 
            if val == value:
                return key
        if val not in list(clean_classmapping.values):
            raise ValueError("Label key does not exist")


    #Function to convert the class label
    def convertclassid(cell):
        if (clean.testfile):
            return convert(cell)
        value = str(cell)
        if (value == ''):
            raise ValueError("All cells in the target column must contain a class label.")

        if (not clean.mapping == {}):
            result = -1
            try:
                result = clean.mapping[cell]
            except:
                raise ValueError("Class label '" + value + "' encountered in input not defined in user-provided mapping.")
            if (not result == int(result)):
                raise ValueError("Class labels must be mapped to integer.")
            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
            return result
        try:
            result = float(cell)
            if (rounding != -1):
                result = int(result * math.pow(10, rounding)) / math.pow(10, rounding)
            else:
                result = int(int(result * 100) / 100)  # round classes to two digits

            if (not str(result) in clean.classlist):
                clean.classlist = clean.classlist + [str(result)]
        except:
            result = (binascii.crc32(value.encode('utf8')) % (1 << 32))
            if (result in clean.classlist):
                result = clean.classlist.index(result)
            else:
                clean.classlist = clean.classlist + [result]
                result = clean.classlist.index(result)
            if (not result == int(result)):
                raise ValueError("Class labels must be mappable to integer.")
        finally:
            if (result < 0):
                raise ValueError("Integer class labels must be positive and contiguous.")

        return result


    #Main Cleaning Code
    rowcount = 0
    with open(filename) as csv_file:
        reader = csv.reader(csv_file)
        f = open(outfile, "w+")
        if (headerless == False):
            next(reader, None)
        outbuf = []
        for row in reader:
            if (row == []):  # Skip empty rows
                continue
            rowcount = rowcount + 1
            rowlen = num_attr
            if (not testfile):
                rowlen = rowlen + 1    
            if (not len(row) == rowlen):
                raise ValueError("Column count must match trained predictor. Row " + str(rowcount) + " differs.")
            i = 0
            for elem in row:
                if(i + 1 < len(row)):
                    outbuf.append(str(convert(elem)))
                    outbuf.append(',')
                else:
                    classid = str(convertclassid(elem))
                    outbuf.append(classid)
                i = i + 1
            if (len(outbuf) < IOBUF):
                outbuf.append(os.linesep)
            else:
                print(''.join(outbuf), file=f)
                outbuf = []
        print(''.join(outbuf), end="", file=f)
        f.close()

        if (testfile == False and not len(clean.classlist) >= 2):
            raise ValueError("Number of classes must be at least 2.")

        return get_key, clean.mapping


# Calculate energy

# Imports -- external
import numpy as np # For numpy see: http://numpy.org
from numpy import array
energy_thresholds = array([121696810480.67499, 123135666454.9, 123558353486.39499, 124489434133.035, 125401803164.94, 128799227921.23999, 128859291958.685, 128899501572.81, 129030881799.07, 129307378236.34, 129787125360.37, 129913401606.155, 130001762638.64499, 130447079015.77, 130487750932.90001, 130981056357.035, 131256676941.29501, 131790061620.59999, 131899226533.72, 132001832730.36, 132049098821.85, 132271124575.06, 132281816434.53, 132535696935.34, 132570893593.485, 132603755766.73999, 132617406429.025, 132647495716.235, 132675487254.20999, 133057832103.38, 133063526428.695, 133076035320.54001, 133090966936.625, 133145184359.23001, 133194919550.695, 133494793601.765, 133599520844.275, 133717122182.765, 133761563314.04, 133839615209.3, 134084515756.615, 134420771657.99, 134462944938.145, 134547773567.515, 134649800653.14499, 134860299608.685, 134868206737.205, 134910760586.855, 134979376048.0, 135492639325.545, 135540383723.31, 135660086148.725, 135679083469.265, 135727212110.655, 135827576807.965, 136071659490.23, 136122847361.545, 136170704154.85, 136222957405.85999, 136274419887.215, 136321002230.58, 136382995601.015, 136385392086.60501, 136475596523.86002, 136487415563.27501, 136513339205.67, 136550750508.5, 136606804259.695, 136689752129.39499, 136765019476.48, 136796986807.795, 136848669192.66, 136874059940.1, 136901059718.66, 136967275801.63501, 137060514953.4, 137119906458.305, 137143414744.42, 137153864785.57, 137481605527.62, 137514386422.93, 137993664276.92, 138022319073.425, 138106040226.075, 138125339381.41498, 138133950680.53497, 138148031846.19998, 138204388877.46503, 138252369210.345, 138313244434.425, 138348701441.06, 138351479653.635, 138354192658.555, 138372160078.90002, 138401768269.57, 138446692384.985, 138584127921.32, 138631149963.96002, 138666217346.97998, 138740323315.77002, 138751395394.975, 138767454409.62, 138780494997.55002, 138785523775.69, 138786209774.455, 138787838743.735, 138791371144.155, 138839014646.29498, 138865923947.78998, 138897995525.22998, 138919772587.9, 138957845560.10498, 139000306766.455, 139173123922.99, 139187884298.075, 139334090803.635, 139351122250.91998, 139361483309.905, 139371916631.46997, 139401660154.83002, 139407800288.88, 139442913347.835, 139451353914.09998, 139462955330.855, 139478426262.41, 139555639330.715, 139560678261.07, 139581368275.53, 139592076003.02502, 139614584905.675, 139624042293.13, 139724955513.88, 139767430201.885, 139780202886.19, 139800737974.335, 139821982107.26498, 139829204918.525, 139995193634.665, 140036639824.83002, 140121092790.39502, 140137480952.62, 140191263233.295, 140199652594.66, 140204002841.76, 140212980483.475, 140224417809.20502, 140228279577.57, 140268845767.37, 140297509975.605, 140349764765.39502, 140353307957.60498, 140419061047.56, 140483407023.255, 140497592957.595, 140508903145.11, 140520204344.0, 140522903430.35498, 140542704639.375, 140558514132.85498, 140572879246.195, 140582933802.245, 140588474481.49, 140613076430.72498, 140717101706.635, 140719643758.3, 140724820522.0, 140767946360.43, 140837026349.225, 140852583427.075, 140907751088.68, 140931387491.55, 140972198210.79, 140976776521.195, 140993747628.27, 140994693765.41498, 141022048503.685, 141055745000.09003, 141091502007.45502, 141107139675.26, 141121836534.115, 141129857853.625, 141137741687.87, 141146224500.82, 141174687231.13, 141199278189.08502, 141292069174.56, 141310900281.72998, 141333014074.24, 141369931976.125, 141460038263.275, 141464753738.01, 141479336211.835, 141495444136.435, 141538327256.12, 141542218869.685, 141590589158.35, 141602460727.56, 141659776832.44, 141671074238.87, 141678317395.1, 141681505051.28998, 141690328960.90997, 141738191061.515, 141745335591.86, 141754201923.37, 141777475758.675, 141784540995.09003, 141857861822.855, 141886807116.09003, 141900277036.47, 141908471664.26498, 141924358778.825, 141956365511.13, 141977366539.805, 141981572378.5, 141999912911.855, 142064671313.685, 142170077765.185, 142175295345.47498, 142180370994.445, 142227882720.685, 142257948789.76, 142271536978.065, 142307128164.37, 142337622503.795, 142384553270.825, 142392503693.885, 142402036436.095, 142417607234.665, 142440575878.35498, 142442615382.53003, 142545062693.36, 142546340787.95, 142557689859.815, 142569735799.375, 142637411926.68, 142641274672.825, 142798244442.665, 142810408034.54, 142815730326.13, 142846796625.93, 142865074570.905, 142910987678.09, 142928660447.60498, 143011561777.55, 143017517377.25, 143026326860.58502, 143067927973.34, 143077035924.055, 143104629387.935, 143135683564.96503, 143172088446.35498, 143177193512.615, 143197726471.88, 143208486840.71, 143252857187.725, 143262318202.24, 143413741065.66498, 143429020254.16998, 143531838918.51, 143537875511.115, 143543484721.40503, 143554638064.815, 143575122297.59, 143596733727.915, 143776865598.025, 143804533922.215, 143806781085.75, 143816852976.97, 143857607584.245, 143889791953.94, 143899048805.045, 143905024719.735, 144004649205.29, 144010035666.61002, 144014189834.255, 144016559308.405, 144062419666.265, 144072247698.66498, 144103145889.88, 144108368389.555, 144143942023.985, 144149057562.69, 144202241975.155, 144213039011.785, 144295073886.045, 144307956082.66, 144315046207.34, 144334518037.38, 144376464931.37, 144383818066.155, 144462374819.885, 144489221468.285, 144519255084.60498, 144536947856.22, 144538339874.515, 144539374341.64, 144554407215.91498, 144562390059.375, 144567535624.555, 144577207735.45, 144590217436.435, 144600580053.60498, 144607607978.495, 144611929997.055, 144636169088.53, 144649347290.905, 144712704574.925, 144722750448.58002, 144758239336.72998, 144766201735.985, 144959631319.56, 144965551791.235, 144979385456.445, 145000302160.075, 145011027202.15503, 145013122798.19, 145042682340.26498, 145051064075.15, 145057111504.34, 145061537067.63, 145100803294.26, 145110711528.215, 145117257662.075, 145119116683.36, 145121540699.83002, 145124794449.86, 145127884617.175, 145128815965.785, 145167172621.36, 145167927185.34, 145205821871.59, 145214960589.72, 145217498729.48502, 145219543161.54, 145229495267.10498, 145246458091.685, 145328875726.96, 145333050156.20502, 145336486505.825, 145339154647.365, 145422508075.81, 145425796647.135, 145436131724.035, 145441828725.05, 145481214998.13, 145489607175.69, 145494983267.815, 145501496397.53998, 145538148194.315, 145544537062.81, 145580486275.505, 145584214644.575, 145596005343.87, 145616230791.135, 145641529026.86, 145657465640.71002, 145704063943.56, 145729561848.26, 145777533921.26, 145793207193.975, 145804488408.345, 145813453129.45, 145845805791.66998, 145856481086.925, 145875172127.315, 145899205253.38, 145908736509.74, 145923021563.02502, 145952280121.695, 145976706348.325, 145990762375.095, 146001019920.21503, 146007002439.95502, 146016057109.11002, 146030043733.405, 146041176274.965, 146041898380.295, 146046696272.62, 146061585951.015, 146066286134.9, 146090411366.41, 146099239374.6, 146137776476.3, 146151208460.49, 146196784864.685, 146203870177.485, 146221564873.255, 146224344272.83002, 146332549510.555, 146342175175.125, 146364164214.08502, 146370052113.47, 146494890483.43, 146507127260.94998, 146590848379.83502, 146604426484.84003, 146613709980.325, 146617600144.21, 146624152484.29498, 146630402963.3, 146724385074.29, 146727726998.5, 146773684934.065, 146801946660.35498, 146821347824.32, 146828734455.62, 146835861413.83002, 146836610508.54498, 146854245704.25, 146863971198.93, 146904995355.535, 146914572261.765, 146930145999.07, 146937698902.9, 146945603636.055, 146954033588.2, 146973992894.705, 146978849711.85, 146981703783.775, 146983572332.28, 146987158917.395, 146989260801.995, 147045188683.14, 147051105402.595, 147066442966.52502, 147073279644.02002, 147081696220.215, 147087436183.535, 147092870232.16, 147097237024.27002, 147098925384.535, 147099468408.215, 147108652219.66998, 147109960449.43, 147170485503.37, 147174628606.53998, 147180134827.44, 147188070516.72, 147282778522.14, 147290352427.77002, 147311183497.21002, 147322988050.46002, 147393506553.83502, 147407759464.065, 147421093505.02002, 147431986423.6, 147447592224.66998, 147457889216.2, 147478878466.22998, 147488330523.37, 147493452010.76, 147497488287.72998, 147562750994.49, 147566108734.24, 147571795864.615, 147582412284.3, 147600732188.97998, 147606288339.88, 147611663427.81, 147614808522.93, 147668611055.08502, 147670689508.46002, 147674733109.095, 147687132985.62, 147711057105.655, 147727419756.025, 147746461717.1, 147754034468.285, 147878173172.10498, 147885153881.91498, 147956680015.65997, 147969006237.715, 147995667596.71, 148000840625.615, 148007672221.18, 148018815058.685, 148056893298.47, 148064616118.33502, 148068880133.58002, 148070732577.125, 148084696513.325, 148099323456.84003, 148113066558.47998, 148116946267.335, 148140229964.12, 148145848154.765, 148152139109.03998, 148157862956.72998, 148162788577.745, 148172026895.46002, 148181114232.14502, 148183001915.32, 148186819287.41998, 148192642637.28998, 148228222436.93, 148236902803.135, 148272048227.125, 148275455009.0, 148288019287.88, 148296162074.565, 148304096659.45502, 148314324993.035, 148390871515.76, 148416981500.14502, 148433701477.625, 148435190977.02002, 148544363251.20502, 148567341353.81, 148596276424.685, 148622363346.46503, 148622900207.90002, 148623570682.45502, 148626941692.04498, 148627835988.775, 148641136974.255, 148667270715.88, 148690378296.305, 148694009948.91998, 148695304232.52, 148701362846.47998, 148708957118.755, 148711459681.455, 148713081046.47998, 148714276369.05, 148727271172.12, 148739583104.185, 148758234595.40002, 148774980290.08002, 148853483316.47998, 148859134299.635, 148861225565.575, 148864712633.19, 148942647094.035, 148959208763.5, 148971419238.97, 148972241393.73502, 148973113027.075, 148976496538.12, 148993945963.715, 149002719720.5, 149012816704.46, 149018606215.77, 149031698368.525, 149038747858.89, 149050416153.75, 149056082626.985, 149096960265.24, 149100633918.065, 149131883773.52002, 149141138237.60498, 149154804008.49, 149167138705.82, 149175229214.125, 149179225256.37, 149205035318.89502, 149224829121.665, 149244533003.94, 149246276854.33002, 149260047023.83002, 149266714039.485, 149276462256.37, 149278444760.105, 149291360164.185, 149294685319.975, 149316192772.22998, 149319293739.84998, 149370867553.975, 149375835814.675, 149394160066.04498, 149423038155.82, 149435160152.55, 149442158193.805, 149455945349.47498, 149465419258.315, 149517134751.62, 149521476578.43, 149544175887.62, 149550128383.615, 149577725449.47498, 149578953938.71, 149631107224.89, 149634733588.725, 149713402480.66998, 149716276867.345, 149736303098.14502, 149758084536.055, 149777571293.72998, 149785402996.46002, 149846355749.755, 149856131306.54498, 149922468887.815, 149926513419.7, 149932613352.19, 149935299556.71503, 149962384664.615, 149965551153.23502, 149993756113.89502, 150000176127.79, 150014001884.56, 150020051788.495, 150037892712.35498, 150043448295.01, 150081988996.28998, 150082766872.7, 150083390780.83002, 150084486347.885, 150096852212.35498, 150103503938.445, 150143043571.315, 150150366826.615, 150172528098.195, 150175006404.485, 150192212630.525, 150200335913.41498, 150233256015.08002, 150234401407.45502, 150251660960.765, 150255575732.035, 150305522994.125, 150307191068.02502, 150313355943.09, 150314667703.365, 150390898878.465, 150396511066.09, 150429256396.16, 150441250325.01, 150445669075.02002, 150449175531.39, 150455286029.10498, 150458506264.695, 150468060374.915, 150468529825.465, 150478973461.22498, 150482185566.65997, 150484678120.81, 150485915328.18, 150500919903.89502, 150502552249.84998, 150516845662.175, 150519269146.565, 150547811877.78, 150551390392.985, 150570322749.1, 150585619656.97, 150593350116.865, 150600284489.9, 150607862856.885, 150609951522.22, 150623949052.24, 150635161420.09003, 150678591115.60498, 150683725136.685, 150686584317.17, 150689418029.45, 150698940242.3, 150704215033.8, 150709733580.34497, 150713671500.505, 150744262907.005, 150750182394.475, 150806761122.89, 150811695991.5, 150837004208.4, 150843658584.31, 150854950309.99, 150875842250.435, 150907653302.59, 150914989105.345, 150929689054.765, 150931363415.45502, 150950170225.22, 150950728285.155, 150989570813.945, 150993396664.275, 151069490843.04, 151082165801.985, 151117766093.38998, 151121303681.175, 151139981623.625, 151146772838.78998, 151150406017.485, 151151993697.78998, 151152638194.33002, 151154682467.925, 151156243590.72498, 151157945117.15002, 151167004033.365, 151176300424.07, 151187246388.95502, 151188622048.475, 151224479915.015, 151229838070.765, 151234253146.02002, 151242112733.745, 151249352665.72998, 151251011029.935, 151329659594.715, 151332649141.065, 151370540180.77002, 151371092828.665, 151385981781.255, 151402737761.945, 151414812510.3, 151416615346.355, 151434646596.375, 151439620727.155, 151455545081.925, 151467050867.845, 151471942388.405, 151476164643.265, 151484427224.085, 151487734808.59, 151502429657.935, 151504667102.41998, 151520423650.62, 151523458406.675, 151528280878.52002, 151533003944.33502, 151554860760.29498, 151572403362.91498, 151585874370.845, 151590864255.635, 151632180196.905, 151634347216.96002, 151636177646.12, 151651227341.09998, 151673544799.18, 151677675830.75, 151693492840.30002, 151699502823.11002, 151727404572.45502, 151730946132.565, 151735101445.63, 151737755221.64502, 151778515153.875, 151781142612.645, 151785920854.305, 151798159360.275, 151828208413.485, 151837382587.075, 151841529291.87, 151844986044.155, 151876248581.515, 151886757779.31, 151899225973.48, 151915124634.84, 151965871856.095, 151974981321.58, 152015563399.33002, 152021062933.60498, 152032209380.51, 152035533882.22, 152077034432.55, 152079760199.32, 152103375313.615, 152105863924.09003, 152117159634.645, 152120243345.925, 152135089910.175, 152135794532.22, 152136547301.265, 152137580902.83502, 152139807404.85, 152143446966.615, 152154689789.835, 152163738638.29498, 152181132835.89502, 152189734845.835, 152205237000.36, 152208957405.94, 152224753647.995, 152230585524.90997, 152245682588.65, 152259849584.635, 152268946439.63998, 152271613442.94, 152278496172.11002, 152281903628.01, 152327029268.065, 152332337677.185, 152336867170.5, 152338910941.9, 152352042720.6, 152354987774.995, 152360874464.66998, 152362715454.365, 152404606098.065, 152410799647.13, 152416095259.23, 152418418417.40002, 152482117560.575, 152491486745.64, 152515120565.59, 152523299764.65, 152564678637.8, 152572999518.075, 152584725411.41, 152591885435.35498, 152595618002.035, 152599525338.115, 152602872011.65, 152608728346.27002, 152638384135.765, 152646163402.265, 152673375032.745, 152689434354.46002, 152706255152.805, 152709015008.19, 152775359690.025, 152788368437.62, 152828482485.225, 152831672529.84003, 152839891704.95, 152844007385.63, 152848319093.87, 152856265799.14, 152865742167.535, 152870214424.21, 152880378975.15, 152884909970.09998, 152956007389.51, 152959485263.74, 152978368906.33002, 152979655232.655, 152990486500.24, 152997856288.08502, 153014791910.725, 153015456911.765, 153019129660.825, 153023787767.17, 153047949260.73502, 153067096741.93, 153121910394.56, 153127235123.815, 153138765178.695, 153142592234.26, 153145459289.555, 153152732418.765, 153158941494.225, 153161432028.565, 153204940879.425, 153206273237.785, 153231696788.86, 153235808483.84998, 153239104901.235, 153242011077.78, 153249283271.02502, 153255036707.545, 153262972245.13, 153270465856.375, 153273894000.975, 153277015556.84003, 153294157366.21997, 153296162872.09497, 153306963979.9, 153322171717.51, 153326375583.185, 153329902567.29, 153337649066.32, 153344156097.245, 153348025327.25, 153352450223.26, 153354351673.01, 153361997552.63, 153369833948.555, 153371476781.41, 153392045540.35498, 153397315510.185, 153428853350.52502, 153440614147.475, 153458374078.73, 153464333782.49, 153517626224.57, 153523542199.695, 153537349101.115, 153544239329.45502, 153579917902.61, 153581635519.22498, 153597148324.6, 153599388607.8, 153605618699.05, 153610332837.54498, 153614092057.72998, 153615527834.87, 153642122037.09003, 153649680734.045, 153652231141.875, 153654708344.015, 153674880354.77997, 153679448914.365, 153684593852.945, 153687665371.61, 153698532360.235, 153705368152.305, 153728073075.525, 153733444058.755, 153753572917.64502, 153760585019.22998, 153786788642.115, 153787627184.745, 153890278348.055, 153891743338.96002, 153946544109.40997, 153950352309.71, 153951012527.25, 153954408101.63, 153960992017.65, 153964137486.40002, 153972978593.39502, 153976327031.95502, 154020678812.615, 154023863377.725, 154043364241.525, 154046861399.325, 154099448945.805, 154101998703.495, 154138586102.185, 154150941361.945, 154197015119.92502, 154202005136.78998, 154262686295.885, 154267305300.15, 154289786562.47498, 154292704826.76, 154294155232.96, 154294695452.005, 154301248927.09998, 154307083421.905, 154346151471.4, 154353045530.45, 154372025329.685, 154380402792.95, 154409475672.02, 154421781732.16498, 154423165083.27502, 154431592477.165, 154440179349.125, 154441542849.535, 154450210552.515, 154458717632.39, 154475528370.57, 154484136480.275, 154502689787.75, 154504593870.60498, 154511216036.295, 154520094063.94, 154538383287.37, 154540890603.495, 154542200623.715, 154545056140.87, 154559987532.485, 154566380531.735, 154572469768.255, 154580456894.195, 154590898177.62, 154600121542.95502, 154614952664.45, 154618388106.2, 154668695091.96002, 154669002644.16, 154671690920.685, 154675445460.555, 154680616972.81, 154683601891.39, 154696671482.34, 154699986858.34998, 154706992831.3, 154714988386.595, 154719154063.215, 154721075497.53998, 154755677148.75, 154770070939.49, 154794725307.905, 154801568894.155, 154821012071.185, 154823694148.28497, 154856708781.375, 154857615669.85498, 154886433787.735, 154888173643.26, 154892321946.235, 154893531358.445, 154924072971.27002, 154924634702.77502, 155007229797.22, 155008533945.66998, 155020812047.09, 155023701025.615, 155091132631.35498, 155096650697.09998, 155124294741.405, 155130354320.47, 155135093939.08502, 155137339895.44, 155144330203.27, 155153671652.385, 155202727114.33502, 155207938873.27002, 155209018752.58002, 155210692207.595, 155222825060.34998, 155226450150.11002, 155236025271.505, 155240248688.665, 155265448457.175, 155274445582.97998, 155287498318.36, 155290708456.175, 155332327461.91998, 155335544260.895, 155342529285.85498, 155346018725.24, 155354796774.05, 155361921848.41498, 155403732568.405, 155411951764.7, 155458567110.065, 155470714749.66, 155474932825.265, 155476967084.02002, 155489320619.375, 155494267219.18, 155526644343.165, 155526932456.355, 155527955150.40002, 155530519012.72, 155534313086.505, 155538430211.47, 155548050586.14502, 155548380310.505, 155559945021.83002, 155561120271.64502, 155561525412.86, 155564559394.02002, 155647858250.535, 155649172433.995, 155653168669.63, 155657817032.28497, 155693025708.24, 155697135459.405, 155701356340.575, 155704431657.66998, 155712962612.155, 155716030827.24, 155719193694.425, 155724355969.93, 155729391394.07, 155731599295.99, 155739858514.48, 155743076468.925, 155748906637.935, 155750833405.935, 155752341747.84, 155754561698.83002, 155776024668.66, 155779170524.425, 155864104896.93, 155870407335.56, 155872787013.115, 155877767409.055, 155927431731.435, 155937601045.31, 155940460532.865, 155946805764.08502, 155978389293.87, 155984586933.1, 155998158632.78, 155998785584.44, 156044236969.71503, 156048373019.08502, 156058494804.58502, 156060542187.325, 156098632603.54, 156105179539.59, 156125280879.0, 156126975410.78998, 156131672401.375, 156134562855.135, 156144725621.84, 156147616931.81, 156152374341.63, 156159537274.52002, 156185101887.275, 156193403229.05, 156207024574.91998, 156208511341.165, 156246717149.6, 156247687733.95, 156261405139.945, 156264538896.585, 156268592705.60498, 156271301579.435, 156288405021.56, 156292310933.94, 156302887126.035, 156316398824.66, 156330841779.375, 156336931692.305, 156337915236.41, 156341272586.92, 156348815132.095, 156356355943.6, 156383168810.16498, 156396169432.69, 156408319386.38, 156412822362.83, 156435174495.635, 156438667353.21002, 156453067020.13, 156453589369.945, 156472936960.585, 156476945513.79498, 156481874550.96, 156486740855.22998, 156507037064.01498, 156511924568.78, 156529851955.675, 156530382262.09, 156535944009.28998, 156541946466.365, 156592580883.46002, 156594362271.79498, 156596880769.905, 156599159589.045, 156608011702.87, 156609876838.62, 156679917433.135, 156689809421.28, 156721224357.40002, 156723502927.26, 156758945029.5, 156762103696.91998, 156772366510.27997, 156774559370.675, 156784073569.78, 156785872431.90002, 156805818172.485, 156813968908.86, 156822266486.7, 156824243786.185, 156828251390.45502, 156834815103.37, 156907960932.77, 156911028336.97498, 156914310376.44998, 156915283098.60498, 156932541052.085, 156947274177.91998, 156949755858.41498, 156959942266.135, 156986941314.62, 156988301530.04498, 156992362752.8, 156994707915.84, 157007296703.45502, 157008979317.555, 157020798488.885, 157024853763.345, 157048574278.305, 157058257119.59, 157079488052.885, 157087404234.72498, 157115293524.95, 157118793731.72, 157125880297.66, 157131186670.985, 157176824055.36, 157177132984.77002, 157181609227.12, 157183726962.43, 157194956231.705, 157205354064.76, 157218903581.47, 157220532851.345, 157248872946.37, 157253688939.29498, 157354414752.34003, 157356244014.16998, 157359127584.10498, 157363815987.37, 157374452669.73502, 157376146166.66, 157425536636.72, 157429923863.58, 157434111467.195, 157438805900.015, 157452818069.63, 157454130467.29498, 157489692968.03, 157493264975.125, 157497460984.375, 157506678069.84998, 157516064647.29498, 157518238952.97, 157543691081.86002, 157554317917.135, 157557180383.255, 157561994766.575, 157589603439.43, 157593181027.005, 157604278445.33, 157606298756.11, 157615729975.925, 157619583430.78, 157622185606.16498, 157626211319.39, 157641249505.36, 157643051643.375, 157655792164.58002, 157660962562.57, 157710435503.495, 157723394192.755, 157730423686.075, 157735349384.485, 157750715012.6, 157754653731.48, 157773135052.93, 157777361858.14502, 157783483608.82, 157784201145.88, 157865690711.89, 157866910155.22, 157887688721.805, 157889736933.185, 158029661160.155, 158032044622.685, 158044202772.52502, 158047480663.015, 158068460670.90002, 158069765731.22498, 158071843372.15, 158073716533.985, 158092577168.98, 158096912052.385, 158105687291.47, 158106511605.515, 158120756679.835, 158124589789.71, 158156428850.84003, 158158192626.04, 158164466439.225, 158171509631.54, 158187649551.1, 158188940648.64, 158223509869.875, 158226847928.05, 158240904604.16, 158242239007.245, 158248038094.025, 158249528565.235, 158259857778.06, 158262500277.185, 158271025122.53, 158280386547.72998, 158294929519.285, 158301403724.78003, 158333782859.115, 158335660473.305, 158336400129.515, 158342551444.25, 158360251890.435, 158362535989.68, 158383508453.345, 158386456423.805, 158405518692.11, 158408374579.975, 158412024110.63, 158415308442.88, 158426669879.85498, 158429216260.595, 158434363753.66, 158436173975.33502, 158440506860.795, 158442825307.19, 158448454943.25, 158450122741.125, 158460624713.84003, 158462163562.57, 158473397971.86, 158474734825.71, 158506338834.51, 158508396219.88, 158619766619.075, 158621744303.25, 158623627555.035, 158624250444.005, 158628908567.405, 158635919437.935, 158651773080.765, 158656183165.365, 158677916663.735, 158680161165.64, 158754262120.26, 158756571848.9, 158772617586.93, 158773884646.37, 158781259530.43, 158784457317.87, 158791900178.03, 158804338333.39502, 158809379544.36, 158814483932.255, 158850359882.05, 158850793684.185, 158868235229.78497, 158875102508.19, 158905037468.45, 158905839192.905, 158908794651.51, 158909783036.57, 158936022761.72498, 158937915945.81, 158966488967.91, 158967801364.96002, 158992634275.175, 158992930784.255, 158996142592.77502, 158997579638.45502, 159038805085.51, 159047920545.05, 159080027259.3, 159080099520.58, 159104459445.82, 159106861181.38, 159110413800.44, 159112399421.72998, 159117058859.88998, 159121329125.75, 159130143988.45, 159133377875.87, 159152339578.27002, 159153935585.45, 159157318173.135, 159160203734.59003, 159189489509.375, 159192920411.84998, 159200643877.655, 159207124962.64, 159243547929.04, 159253384999.935, 159286370610.15002, 159292256432.71503, 159299218061.685, 159300949210.90002, 159306706961.515, 159317993497.635, 159369324230.035, 159384690819.38, 159467930356.84998, 159471157091.90997, 159474321093.27002, 159475101621.20502, 159486519692.42502, 159492558326.455, 159527824909.685, 159536092390.285, 159544287256.875, 159546462347.455, 159591507688.655, 159596772551.845, 159598467432.385, 159602247177.52002, 159607504111.155, 159609928072.46002, 159611201696.41998, 159612463946.96, 159672142493.345, 159672747583.53497, 159679823342.255, 159684315843.72, 159704973550.775, 159726545416.615, 159749710490.33002, 159755848797.73, 159808875978.785, 159816600524.25, 159823626188.25, 159826577511.88498, 159827142588.745, 159829410653.33002, 159862710387.505, 159867371777.38, 159879759377.57, 159885584285.23, 159912803754.65997, 159917258064.99, 159918138654.51, 159919172914.89502, 159927919075.2, 159928860743.79498, 159941084324.90997, 159945601031.96, 159971375326.565, 159975464210.64502, 159987858610.64502, 159988349399.515, 160004830087.385, 160006102055.37, 160026706909.425, 160033692429.725, 160041367576.495, 160049219493.45502, 160051413455.865, 160053401114.03998, 160054923156.72, 160056317369.95502, 160072331813.71002, 160073947460.85498, 160117304518.01498, 160119113212.555, 160122850415.54498, 160126194590.93, 160129980084.845, 160133974168.29, 160194683929.5, 160196161657.935, 160245592807.03, 160256115280.565, 160269126884.41998, 160275678922.8, 160284491672.37, 160286541937.41998, 160288730220.34, 160290070592.89, 160323829214.365, 160325255879.365, 160396014114.65, 160399118670.815, 160404576453.91998, 160408391386.82, 160416130308.285, 160416842653.03, 160419800282.88, 160431632043.15997, 160443973758.86, 160445764861.63, 160451743333.235, 160452056939.18, 160476328847.235, 160488720262.695, 160492415246.43, 160504498245.31, 160518299742.81, 160521645979.85, 160524740229.76, 160527965948.595, 160570576452.46, 160572683437.025, 160594577391.185, 160602169341.185, 160615958953.27502, 160616707104.55002, 160666399838.305, 160668312738.03, 160680692171.08, 160683821309.775, 160704923787.91498, 160709727216.40997, 160785356471.40002, 160787902027.425, 160805854321.195, 160806789382.46, 160814816114.23, 160822422702.735, 160830709000.88, 160834122951.78003, 160884963232.075, 160893559493.925, 160901470347.995, 160903246850.4, 160931588184.685, 160933583671.33002, 160974061720.465, 160977855639.74, 161040735747.075, 161046129663.805, 161058998629.005, 161061401280.76, 161065125133.695, 161072238293.08002, 161092264967.16998, 161096169706.385, 161110520164.81, 161115778672.985, 161124507326.53, 161128076420.925, 161132416008.68, 161134929022.39, 161162799356.47498, 161163684005.07, 161166446164.275, 161169234030.6, 161182231610.14, 161183140720.415, 161203522182.325, 161206188465.94, 161232648377.47498, 161236428623.63, 161240744370.595, 161243294850.21, 161247612410.395, 161251258002.08002, 161271798119.53, 161277476640.215, 161291519555.065, 161304144584.845, 161314267050.22998, 161320530561.635, 161351298968.495, 161357444315.05, 161367366717.755, 161372391697.385, 161388577376.77502, 161391728781.135, 161396795716.66, 161399772729.31, 161401828958.495, 161404982255.705, 161419415970.08002, 161420258218.995, 161436635134.14, 161441820628.81, 161451120233.9, 161455493162.29, 161506699837.435, 161512899201.53998, 161579884709.755, 161588234900.87, 161615947085.97498, 161617247710.21002, 161659658578.86, 161661178305.10498, 161719306724.185, 161720324128.34, 161872779565.975, 161877254435.91, 161890211847.955, 161892346030.8, 161913828091.6, 161923853275.305, 161933922824.47, 161936282469.88, 161941247656.01, 161944625843.04498, 161947939291.785, 161962663966.085, 161987135379.64502, 161995751683.125, 162016153521.62, 162020715050.845, 162030889729.375, 162033697015.75, 162038376079.625, 162044266890.165, 162100658746.47998, 162107243361.22, 162117194884.85, 162124423590.58002, 162164771727.57, 162170570601.005, 162175033948.01, 162177996499.34497, 162182855573.01, 162187113203.47998, 162199091161.16498, 162202839423.44, 162224236272.59, 162233467229.515, 162243810873.65997, 162245328537.76498, 162265698920.16, 162266763486.02, 162287755014.325, 162290125118.82, 162309330498.03, 162312627786.15002, 162321659957.83502, 162332200056.02002, 162354965307.75, 162357242538.425, 162391992439.935, 162395223501.74, 162437034051.89, 162439991500.385, 162448692490.36, 162454238338.975, 162459505818.365, 162459858820.86, 162492970209.38, 162496553485.43, 162510399894.25, 162511131004.78, 162532075666.175, 162534174784.45, 162540073332.95502, 162546014393.895, 162550245795.12, 162557451603.56, 162593205462.675, 162597717201.31, 162653566848.425, 162659132997.81, 162661180873.095, 162663295122.52, 162670522333.35, 162677073570.09, 162691281222.93, 162693833674.22998, 162713191475.815, 162721946235.71503, 162751832476.13, 162753035563.81, 162760814593.22, 162770260701.99, 162773926327.815, 162778617539.85498, 162818764103.62, 162823078510.62, 162848674601.16998, 162849445162.55, 162882093110.12, 162885669135.39502, 162888729951.58002, 162893934474.45502, 162923269423.215, 162925640902.21, 162975572719.16, 162977439214.76, 162982529527.21002, 162989397693.615, 163017505124.725, 163019057563.97998, 163023929251.72998, 163030166629.65002, 163032963257.11002, 163036330967.75, 163045494826.35498, 163057902000.70502, 163068760787.93, 163079609952.70502, 163085291766.0, 163088321533.15002, 163106330740.72998, 163109073871.22, 163113370294.2, 163115616830.08002, 163118080066.63, 163121805142.955, 163130524554.51, 163134477228.02502, 163138727177.95, 163142895861.74, 163215440470.87, 163218757631.61002, 163301751371.12, 163303139722.27502, 163303919376.845, 163311225558.645, 163323858096.665, 163331777307.1, 163339832905.53, 163348866731.85498, 163376570079.285, 163381264978.97, 163393807224.475, 163397093122.71002, 163408478923.755, 163417000920.775, 163458391908.345, 163459282001.98, 163469333265.35498, 163475945727.49, 163537600659.03998, 163546844544.59497, 163549438887.91998, 163553002177.22, 163588615932.405, 163591885732.91998, 163623686214.8, 163625762632.31, 163676538167.20502, 163680214677.965, 163701057902.1, 163706335377.185, 163717340979.66998, 163720062415.58002, 163740157682.41498, 163749264404.085, 163790729956.655, 163805575783.595, 163823382424.03497, 163831043972.82, 163834320447.005, 163835348682.07, 163846804812.19, 163862076658.31, 163881582794.125, 163888522222.195, 163893779860.44998, 163895694756.96, 163934146114.885, 163937606570.03998, 163946359020.065, 163949327840.8, 164035677092.08502, 164038588680.3, 164048707469.84998, 164049274815.60498, 164064987251.685, 164070120348.0, 164097640928.495, 164098476575.04498, 164101745471.905, 164110076059.625, 164144451437.845, 164146310466.925, 164148771783.82, 164150951703.88, 164167679720.01, 164169920347.90002, 164177692806.85498, 164179950409.64, 164208151320.91, 164211329758.715, 164245895634.16, 164251773867.835, 164356240101.345, 164358835414.195, 164383218009.875, 164385866979.41498, 164444694565.175, 164448199568.365, 164483166257.445, 164496308343.31, 164539318432.16, 164543144241.48, 164563027336.375, 164568062762.24, 164577913072.11, 164584563752.085, 164624134618.945, 164634776142.59, 164638553187.53497, 164639275894.68, 164644071429.285, 164646900134.27, 164669709285.495, 164674727132.70502, 164678917394.79498, 164685382055.21, 164691284106.09998, 164696823483.195, 164716069670.12, 164720672398.22498, 164744005456.885, 164746953633.08002, 164752122924.24, 164757671983.22, 164779295985.69, 164780328099.14, 164808842751.075, 164817215607.265, 164827253286.735, 164830386477.84998, 164835198759.85, 164845653616.40503, 164859970231.825, 164865496148.215, 164885971082.08002, 164889080880.635, 164942461212.22, 164945816228.72, 164947734160.04, 164950656749.61, 164952537514.22498, 164952652049.115, 164971679335.56, 164973372909.285, 164997860480.59998, 164998155078.19998, 165010405815.53, 165022935015.22, 165128292114.53, 165135821520.535, 165159044039.58002, 165161306648.08002, 165198698073.775, 165201450868.37, 165210914726.22, 165213586190.82, 165223537668.57, 165225173087.525, 165231708291.33502, 165239343654.315, 165243097649.02502, 165247359483.805, 165282110032.435, 165292406845.41998, 165360093340.84998, 165364190399.89502, 165411568062.84, 165414108195.8, 165455481630.09998, 165458400013.565, 165511825826.8, 165518625932.675, 165568454406.615, 165577132912.565, 165665298266.97498, 165675896064.13, 165684108068.31, 165698876172.355, 165711668967.735, 165715728614.64502, 165751016832.175, 165756449315.475, 165784362615.445, 165796068390.935, 165827378422.32, 165834997029.755, 165873703808.18, 165879822312.59003, 166078238170.60498, 166086765136.26, 166137568434.72, 166141898507.24, 166143354509.22998, 166153699187.58002, 166215267032.78497, 166221912969.91, 166228595191.485, 166231319927.565, 166276263324.72, 166281122375.21002, 166284363909.95, 166287643167.97998, 166296696035.8, 166303603671.825, 166316443270.76, 166319708975.3, 166352109465.01, 166373818572.04498, 166403811016.18, 166406497788.46503, 166411431050.065, 166414988883.875, 166422987424.96, 166429376381.06998, 166433846506.49, 166442331390.94, 166448861518.82, 166455183365.255, 166511219132.2, 166513962338.475, 166518371739.485, 166521162709.86, 166550848257.33502, 166567012079.58002, 166586692752.91498, 166598036232.435, 166602656930.89502, 166606468787.93, 166645663199.83502, 166653372696.04498, 166677564472.685, 166686525446.45, 166693592580.86002, 166698249742.865, 166829652895.25, 166835560587.47, 166848900136.69, 166855116968.49, 166937315680.38998, 166939344453.995, 166940927743.59, 166942820759.85498, 166947311421.52002, 166951096515.48, 167037040250.95502, 167044559126.60498, 167048462601.84998, 167049316220.78497, 167058834395.97998, 167062571757.305, 167178680983.94, 167179992173.37, 167200531060.79, 167207041149.685, 167252623466.47, 167268404738.285, 167272676524.11, 167279098080.89502, 167300035772.625, 167304594719.445, 167451359497.59003, 167452633362.885, 167464493858.76, 167467710549.125, 167525002983.95, 167547944713.32, 167573263991.41498, 167577289452.79498, 167604521778.97998, 167606740319.06, 167665161868.78998, 167679911464.16, 167686502598.995, 167690038978.865, 167773742285.53003, 167780878443.975, 167787040585.71503, 167794433214.86002, 167806069866.595, 167806525974.99, 167825671075.44, 167828752361.595, 167901423055.875, 167905103959.055, 167913134522.135, 167913376939.745, 167938887643.025, 167941020045.555, 168016956039.53, 168018939416.555, 168029319629.705, 168034734883.005, 168092720493.91998, 168094549127.49, 168166536640.865, 168168873373.38, 168180989782.86, 168185740456.63, 168191883212.61, 168195331404.385, 168207316347.4, 168210718173.53998, 168214723958.715, 168223209967.90497, 168301772564.585, 168305169273.875, 168309366869.605, 168320313236.08002, 168390631554.40002, 168394434628.58502, 168399021573.03998, 168400939042.22998, 168404570240.53, 168408522971.62, 168496118995.95, 168503764234.41, 168568119208.41998, 168575072324.93, 168585419463.49, 168585716730.935, 168640504782.11, 168649126633.29498, 168664834709.91998, 168670387384.985, 168712997081.315, 168717438305.47498, 168720639761.405, 168724288077.73, 168749881205.04, 168754280251.46503, 168796038430.36, 168808258753.13998, 168843783292.705, 168847793211.685, 168921993692.86, 168927762567.27, 168993388975.675, 168996960333.76, 169018993005.865, 169030275378.995, 169069156331.12, 169073583025.71, 169268265972.16998, 169269843263.395, 169297735078.35, 169302603612.52, 169369760738.005, 169374540776.67, 169380197539.65497, 169391589120.095, 169408263296.375, 169415405269.085, 169434607629.21002, 169438919295.095, 169591088264.87, 169601496519.545, 169637099599.89502, 169642859796.22, 169687082479.73, 169690983262.175, 169713513395.095, 169722310322.815, 169764014428.02002, 169765383634.17, 169789530776.72998, 169800207595.76, 169836335262.21002, 169841127343.96503, 169898511095.425, 169899066290.185, 169949186094.185, 169952690012.685, 169990647066.845, 169997063170.285, 170111949028.235, 170115630138.125, 170175540938.66, 170183255528.07, 170235723388.49, 170253270141.83002, 170276924622.66998, 170280699190.805, 170300320189.57, 170302505189.07, 170305596143.525, 170311341813.19, 170341070934.285, 170345331691.20502, 170467661609.735, 170489817860.19, 170525312767.75, 170530255164.22498, 170619376516.41998, 170625725420.5, 170713239973.085, 170728713921.20502, 170757269446.245, 170763499082.765, 170806244023.70502, 170810565996.0, 170816201412.705, 170822495540.28, 171020898938.08002, 171022861884.735, 171053134599.38, 171057923099.09, 171063856965.66, 171066446007.375, 171189347837.08502, 171195572093.28, 171222130389.45, 171224610646.055, 171234670281.33, 171254377122.025, 171316286757.785, 171318310381.84003, 171432462288.175, 171441818578.31, 171498285005.35498, 171500837083.84, 171654732412.445, 171664359720.58002, 171721772284.88, 171727453343.61, 171857527705.89, 171862075167.52502, 171874012171.15002, 171876990980.415, 171880150188.96503, 171883647011.44, 171895307446.21, 171898082824.5, 171908032340.545, 171913410284.155, 171952374910.62, 171959758458.715, 171976691724.20502, 171980421099.265, 172022761018.49, 172046584785.06, 172125278348.365, 172135599719.19, 172150331267.055, 172152128451.60498, 172176821966.34998, 172184999170.03497, 172383543021.815, 172390237889.94, 172494376021.53, 172496315236.21, 172536184141.66498, 172550524469.065, 172569991263.26498, 172579582866.03998, 172647454312.82, 172659155062.315, 172722034376.46503, 172723790998.79, 172733623099.75, 172740631173.26, 172743772315.11002, 172754327086.81, 172757492202.04498, 172762266057.97998, 172788581524.97998, 172793865609.10498, 172937997820.0, 172941686862.93, 172986357479.38, 172992003014.265, 172997472675.03497, 173005460102.72498, 173019135069.59, 173025819972.09, 173069580980.815, 173074493741.87, 173156366650.78003, 173171304500.1, 173218516592.42, 173219856520.105, 173279303142.035, 173283883125.135, 173458388151.695, 173462918049.2, 173519723157.26, 173522553917.875, 173545243344.65, 173556333098.78497, 173674265724.29, 173678549470.84003, 173697649425.21, 173702574460.72998, 173796741457.21503, 173811991832.06, 173838366779.77002, 173857968408.055, 173968316354.29498, 173968908300.525, 174156280188.14, 174159701534.585, 174227162809.01, 174235679390.99, 174308248737.18, 174319827492.715, 174337494510.74, 174340681815.62, 174551968865.2, 174553727699.92502, 174617208014.12, 174621844579.755, 174719485577.305, 174735814161.01, 174739972672.88, 174741372452.58, 174762380975.685, 174775650251.43, 174798671710.75, 174809996027.255, 174915966989.285, 174929505067.68, 174951298728.075, 174962709964.02, 175021104107.935, 175024685976.565, 175067804883.51, 175076378807.26, 175195673751.345, 175200972597.97, 175304107866.905, 175309290709.82, 175542676517.485, 175553914957.965, 175725794291.65, 175741219575.385, 175838767887.90002, 175841989587.59003, 176112633916.375, 176123065806.65002, 176139054593.875, 176143449154.445, 176233709454.61, 176239393850.45, 176375367684.385, 176385498422.0, 176472494459.715, 176489236172.77002, 176556769861.3, 176588796557.815, 176619959675.99, 176640273540.79, 176654762311.075, 176659772907.925, 176923416834.76, 176924409996.10498, 177303763590.565, 177323348249.75, 177482699480.505, 177490471787.685, 177718857972.335, 177725390381.61, 178124359221.335, 178128949712.585, 178246297722.345, 178256455736.45502, 178604449692.305, 178614611565.59003, 178766787919.61, 178775066673.89, 179208864875.52, 179218589688.83002, 179581155082.415, 179581463729.88, 179760425485.79498, 179776374721.805, 179869982470.13998, 179917088498.03998, 179988966798.34998, 179994171395.185, 180314086967.495, 180314891968.87, 180621927606.685, 180630488738.34003, 181344753649.24, 181361010832.18, 182345212600.865, 182376246564.41, 183202083615.41, 183213650439.245, 183510897102.56, 183528359808.46002, 183708291361.45502, 183716269483.685, 184179649872.135, 184199741503.595, 184260818479.875, 184280646681.235, 184841257290.57, 184876137534.39, 185023607682.21503, 185051767804.065, 185220904883.16998, 185253117545.375, 186016057137.715, 186052605945.125, 186159570170.61, 186179958827.33, 186309433752.855, 186322694224.45, 187061397250.25, 187073323612.615, 187915117079.88, 187920846792.44, 187927503412.66498, 187954767958.29498, 188009019190.61502, 188014475244.83002, 188251525210.315, 188272197794.76, 188935726479.515, 188987580530.81, 190919468864.95, 191066973166.72, 191401279283.095, 191497758262.415, 192030615480.325, 192069630292.57, 193135849055.28, 193193168104.405, 193802629220.59, 193819066184.63, 194039721896.93, 194098103119.96002, 231173279338.95, 234085731128.5, 240373997771.995, 240773635003.5, 241029629178.0, 241529661884.0, 241933368248.5, 242787132563.5, 242890986414.0, 243023522841.0, 243277391271.5, 243625486502.0, 244232127142.0, 244389011476.5, 244870089391.5, 245232278843.5, 245624245675.5, 245712162461.005, 245807987174.0, 245856939648.5, 246121513594.0, 246310636152.25, 247913378696.0, 248369421110.0, 253697793772.0, 254207323991.5, 257711943631.25, 258148989010.5])
def eqenergy(rows):
    return np.sum(rows, axis=1)
def classify(rows):
    energys = eqenergy(rows)
    start_label = 0
    def thresh_search(input_energys):
        numers = np.searchsorted(energy_thresholds, input_energys, side='left')-1
        indys = np.argwhere(np.logical_and(numers<len(energy_thresholds), numers>=0)).reshape(-1)
        defaultindys = np.argwhere(np.logical_not(np.logical_and(numers<len(energy_thresholds), numers>=0))).reshape(-1)
        outputs = np.zeros(input_energys.shape[0])
        outputs[indys] = (numers[indys] + start_label) % 2
        outputs[defaultindys]=1
        return outputs
    return thresh_search(energys)

numthresholds = 2295



# Main method
model_cap = numthresholds


def Validate(file):
    #Load Array
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')


    if n_classes == 2:
        #note that classification is a single line of code
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0 = 0, 0, 0, 0, 0, 0, 0, 0
        correct_count = int(np.sum(outputs.reshape(-1) == cleanarr[:, -1].reshape(-1)))
        count = outputs.shape[0]
        num_TP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 1)))
        num_TN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 0)))
        num_FN = int(np.sum(np.logical_and(outputs.reshape(-1) == 0, cleanarr[:, -1].reshape(-1) == 1)))
        num_FP = int(np.sum(np.logical_and(outputs.reshape(-1) == 1, cleanarr[:, -1].reshape(-1) == 0)))
        num_class_0 = int(np.sum(cleanarr[:, -1].reshape(-1) == 0))
        num_class_1 = int(np.sum(cleanarr[:, -1].reshape(-1) == 1))
        return count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, outputs, cleanarr[:, -1]


    else:
        #validation
        outputs = classify(cleanarr[:, :-1])


        #metrics
        count, correct_count = 0, 0
        numeachclass = {}
        for k, o in enumerate(outputs):
            if int(o) == int(float(cleanarr[k, -1])):
                correct_count += 1
            if int(float(cleanarr[k, -1])) in numeachclass.keys():
                numeachclass[int(float(cleanarr[k, -1]))] += 1
            else:
                numeachclass[int(float(cleanarr[k, -1]))] = 1
            count += 1
        return count, correct_count, numeachclass, outputs, cleanarr[:, -1]


#Predict on unlabeled data
def Predict(file, get_key, headerless, preprocessedfile, classmapping):
    cleanarr = np.loadtxt(file, delimiter=',', dtype='float64')
    cleanarr = cleanarr.reshape(cleanarr.shape[0], -1)
    with open(preprocessedfile, 'r') as csvinput:
        dirtyreader = csv.reader(csvinput)

        #print original header
        if (not headerless):
            print(','.join(next(dirtyreader, None) + ["Prediction"]))

        outputs = classify(cleanarr)

        for k, row in enumerate(dirtyreader):
            print(str(','.join(str(j) for j in (['"' + i + '"' if ',' in i else i for i in row]))) + ',' + str(get_key(int(outputs[k]), classmapping)))



#Main
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Predictor trained on '+TRAINFILE)
    parser.add_argument('csvfile', type=str, help='CSV file containing test set (unlabeled).')
    parser.add_argument('-validate', action='store_true', help='Validation mode. csvfile must be labeled. Output is classification statistics rather than predictions.')
    parser.add_argument('-cleanfile', action='store_true', help='Use this flag to save prediction time if the csvfile you are passing has already been preprocessed. Implies headerless.')
    parser.add_argument('-headerless', help='Do not treat the first line of csvfile as a header.', action='store_true')
    parser.add_argument('-json', action="store_true", default=False, help="report measurements as json")
    args = parser.parse_args()
    faulthandler.enable()

    #clean if not already clean
    if not args.cleanfile:
        cleanfile = tempfile.NamedTemporaryFile().name
        preprocessedfile = tempfile.NamedTemporaryFile().name
        preprocess(args.csvfile,preprocessedfile,args.headerless,(not args.validate))
        get_key, classmapping = clean(preprocessedfile, cleanfile, -1, args.headerless, (not args.validate))
    else:
        cleanfile=args.csvfile
        preprocessedfile=args.csvfile
        get_key = lambda x, y: x
        classmapping = {}

    #Predict or Validate?
    if not args.validate:
        Predict(cleanfile, get_key, args.headerless, preprocessedfile, classmapping)


    else:
        classifier_type = 'DT'
        if n_classes == 2:
            count, correct_count, num_TP, num_TN, num_FP, num_FN, num_class_1, num_class_0, preds, true_labels = Validate(cleanfile)
        else:
            count, correct_count, numeachclass, preds, true_labels = Validate(cleanfile)


        #validation report
        if args.json:
            import json
        if n_classes == 2:
            #Base metrics
            FN = float(num_FN) * 100.0 / float(count)
            FP = float(num_FP) * 100.0 / float(count)
            TN = float(num_TN) * 100.0 / float(count)
            TP = float(num_TP) * 100.0 / float(count)
            num_correct = correct_count
        
            #Calculated Metrics
            if int(num_TP + num_FN) != 0:
                TPR = num_TP / (num_TP + num_FN) # Sensitivity, Recall
            if int(num_TN + num_FP) != 0:
                TNR = num_TN / (num_TN + num_FP) # Specificity
            if int(num_TP + num_FP) != 0:
                PPV = num_TP / (num_TP + num_FP) # Recall
            if int(num_FN + num_TP) != 0:
                FNR = num_FN / (num_FN + num_TP) # Miss rate
            if int(2 * num_TP + num_FP + num_FN) != 0:
                FONE = 2 * num_TP / (2 * num_TP + num_FP + num_FN) # F1 Score
            if int(num_TP + num_FN + num_FP) != 0:
                TS = num_TP / (num_TP + num_FN + num_FP) # Critical Success Index
            #Best Guess Accuracy
            randguess = int(float(10000.0 * max(num_class_1, num_class_0)) / count) / 100.0
            #Model Accuracy
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            #Report
            if args.json:
                #                json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'n_classes':2, 'Number of False Negative Instances': num_FN, 'Number of False Positive Instances': num_FP, 'Number of True Positive Instances': num_TP, 'Number of True Negative Instances': num_TN,   'False Negatives': FN, 'False Positives': FP, 'True Negatives': TN, 'True Positives': TP, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0}
                json_dict = {'instance_count':                        count ,
                            'classifier_type':                        classifier_type ,
                            'n_classes':                            2 ,
                            'number_of_false_negative_instances':    num_FN ,
                            'number_of_false_positive_instances':    num_FP ,
                            'number_of_true_positive_instances':    num_TP ,
                            'number_of_true_negative_instances':    num_TN,
                            'false_negatives':                        FN ,
                            'false_positives':                        FP ,
                            'true_negatives':                        TN ,
                            'true_positives':                        TP ,
                            'number_correct':                        num_correct ,
                            'best_guess':                            randguess ,
                            'model_accuracy':                        modelacc ,
                            'model_capacity':                        model_cap ,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                             }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        Binary classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))
                print("System behavior")
                print("True Negatives:                     {:.2f}%".format(TN) + " (" + str(int(num_TN)) + "/" + str(count) + ")")
                print("True Positives:                     {:.2f}%".format(TP) + " (" + str(int(num_TP)) + "/" + str(count) + ")")
                print("False Negatives:                    {:.2f}%".format(FN) + " (" + str(int(num_FN)) + "/" + str(count) + ")")
                print("False Positives:                    {:.2f}%".format(FP) + " (" + str(int(num_FP)) + "/" + str(count) + ")")
                if int(num_TP + num_FN) != 0:
                    print("True Pos. Rate/Sensitivity/Recall:  {:.2f}".format(TPR))
                if int(num_TN + num_FP) != 0:
                    print("True Neg. Rate/Specificity:         {:.2f}".format(TNR))
                if int(num_TP + num_FP) != 0:
                    print("Precision:                          {:.2f}".format(PPV))
                if int(2 * num_TP + num_FP + num_FN) != 0:
                    print("F-1 Measure:                        {:.2f}".format(FONE))
                if int(num_TP + num_FN) != 0:
                    print("False Negative Rate/Miss Rate:      {:.2f}".format(FNR))
                if int(num_TP + num_FN + num_FP) != 0:
                    print("Critical Success Index:             {:.2f}".format(TS))
        
        #Multiclass
        else:
            num_correct = correct_count
            modelacc = int(float(num_correct * 10000) / count) / 100.0
            randguess = round(max(numeachclass.values()) / sum(numeachclass.values()) * 100, 2)
            if args.json:
        #        json_dict = {'Instance Count':count, 'classifier_type':classifier_type, 'Number Correct': num_correct, 'Best Guess': randguess, 'Model Accuracy': modelacc, 'Model Capacity': model_cap, 'Generalization Ratio': int(float(num_correct * 100) / model_cap) / 100.0, 'Model Efficiency': int(100 * (modelacc - randguess) / model_cap) / 100.0, 'n_classes': n_classes}
                json_dict = {'instance_count':                        count,
                            'classifier_type':                        classifier_type,
                            'n_classes':                            n_classes,
                            'number_correct':                        num_correct,
                            'best_guess':                            randguess,
                            'model_accuracy':                        modelacc,
                            'model_capacity':                        model_cap,
                            'generalization_ratio':                int(float(num_correct * 100) / model_cap) / 100.0,
                            'model_efficiency':                    int(100 * (modelacc - randguess) / model_cap) / 100.0
                            }
            else:
                if classifier_type == 'NN':
                    print("Classifier Type:                    Neural Network")
                else:
                    print("Classifier Type:                    Decision Tree")
                print("System Type:                        " + str(n_classes) + "-way classifier")
                print("Best-guess accuracy:                {:.2f}%".format(randguess))
                print("Model accuracy:                     {:.2f}%".format(modelacc) + " (" + str(int(num_correct)) + "/" + str(count) + " correct)")
                print("Improvement over best guess:        {:.2f}%".format(modelacc - randguess) + " (of possible " + str(round(100 - randguess, 2)) + "%)")
                print("Model capacity (MEC):               {:.0f} bits".format(model_cap))
                print("Generalization ratio:               {:.2f}".format(int(float(num_correct * 100) / model_cap) / 100.0) + " bits/bit")
                print("Model efficiency:                   {:.2f}%/parameter".format(int(100 * (modelacc - randguess) / model_cap) / 100.0))

        try:
            import numpy as np # For numpy see: http://numpy.org
            from numpy import array
        except:
            print("Note: If you install numpy (https://www.numpy.org) and scipy (https://www.scipy.org) this predictor generates a confusion matrix")

        def confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None):
            #check for numpy/scipy is imported
            try:
                from scipy.sparse import coo_matrix #required for multiclass metrics
            except:
                print("Note: If you install scipy (https://www.scipy.org) this predictor generates a confusion matrix")
                sys.exit()
            # Compute confusion matrix to evaluate the accuracy of a classification.
            # By definition a confusion matrix :math:C is such that :math:C_{i, j}
            # is equal to the number of observations known to be in group :math:i and
            # predicted to be in group :math:j.
            # Thus in binary classification, the count of true negatives is
            # :math:C_{0,0}, false negatives is :math:C_{1,0}, true positives is
            # :math:C_{1,1} and false positives is :math:C_{0,1}.
            # Read more in the :ref:User Guide <confusion_matrix>.
            # Parameters
            # ----------
            # y_true : array-like of shape (n_samples,)
            # Ground truth (correct) target values.
            # y_pred : array-like of shape (n_samples,)
            # Estimated targets as returned by a classifier.
            # labels : array-like of shape (n_classes), default=None
            # List of labels to index the matrix. This may be used to reorder
            # or select a subset of labels.
            # If None is given, those that appear at least once
            # in y_true or y_pred are used in sorted order.
            # sample_weight : array-like of shape (n_samples,), default=None
            # Sample weights.
            # normalize : {'true', 'pred', 'all'}, default=None
            # Normalizes confusion matrix over the true (rows), predicted (columns)
            # conditions or all the population. If None, confusion matrix will not be
            # normalized.
            # Returns
            # -------
            # C : ndarray of shape (n_classes, n_classes)
            # Confusion matrix.
            # References
            # ----------
            if labels is None:
                labels = np.array(list(set(list(y_true.astype('int')))))
            else:
                labels = np.asarray(labels)
                if np.all([l not in y_true for l in labels]):
                    raise ValueError("At least one label specified must be in y_true")


            if sample_weight is None:
                sample_weight = np.ones(y_true.shape[0], dtype=np.int64)
            else:
                sample_weight = np.asarray(sample_weight)
            if y_true.shape[0]!=y_pred.shape[0]:
                raise ValueError("y_true and y_pred must be of the same length")

            if normalize not in ['true', 'pred', 'all', None]:
                raise ValueError("normalize must be one of {'true', 'pred', 'all', None}")


            n_labels = labels.size
            label_to_ind = {y: x for x, y in enumerate(labels)}
            # convert yt, yp into index
            y_pred = np.array([label_to_ind.get(x, n_labels + 1) for x in y_pred])
            y_true = np.array([label_to_ind.get(x, n_labels + 1) for x in y_true])
            # intersect y_pred, y_true with labels, eliminate items not in labels
            ind = np.logical_and(y_pred < n_labels, y_true < n_labels)
            y_pred = y_pred[ind]
            y_true = y_true[ind]
            # also eliminate weights of eliminated items
            sample_weight = sample_weight[ind]
            # Choose the accumulator dtype to always have high precision
            if sample_weight.dtype.kind in {'i', 'u', 'b'}:
                dtype = np.int64
            else:
                dtype = np.float64
            cm = coo_matrix((sample_weight, (y_true, y_pred)), shape=(n_labels, n_labels), dtype=dtype,).toarray()


            with np.errstate(all='ignore'):
                if normalize == 'true':
                    cm = cm / cm.sum(axis=1, keepdims=True)
                elif normalize == 'pred':
                    cm = cm / cm.sum(axis=0, keepdims=True)
                elif normalize == 'all':
                    cm = cm / cm.sum()
                cm = np.nan_to_num(cm)
            return cm
        mtrx = confusion_matrix(np.array(true_labels).reshape(-1), np.array(preds).reshape(-1))
        if args.json:
            json_dict['confusion_matrix'] = mtrx.tolist()
            print(json.dumps(json_dict))
        else:
            mtrx = mtrx / np.sum(mtrx) * 100.0
            print("Confusion Matrix:")
            print(' ' + np.array2string(mtrx, formatter={'float': (lambda x: '{:.2f}%'.format(round(float(x), 2)))})[1:-1])


    #remove tempfile if created
    if not args.cleanfile: 
        os.remove(cleanfile)
        os.remove(preprocessedfile)



